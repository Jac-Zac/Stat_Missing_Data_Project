---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
    wrap: 72
---

```{r, echo=F, class.source = 'fold-show'}
library(here)

# Load utilities
source(here("src", "setup.R"))

# Define the child RMarkdown file path
child <- here("notebooks", "dataset_analysis", "synthetic_data_analysis.Rmd")
```

```{r, child = here("notebooks", "dataset_analysis", "synthetic_data_analysis.Rmd"), cache = TRUE}
# Run the file which does data analyses even if not knitted by runned manually
rmarkdown::render(child)
```
# Imputation Methods

The following imputation methods are tested on both the MCAR and MAR datasets. A regression model is fit on the imputed dataset, and the results are summarized.

## 1. List‑wise (Case) Deletion

List-wise deletion removes any case with missing values from analysis and is the default method in many statistical software tools. It works well when data is large and missing completely at random (MCAR). However, if these conditions are not met, it can lead to bias and loss of important information, making it a less suitable approach.

```{r}
listwise_deletion_data <- listwise_deletion(data.MAR.uni)
```
## 2. Pairwise Deletion

Pairwise deletion minimizes information loss compared to list-wise deletion by only removing data points when necessary to check if missing values are truly missing. It produces lower bias for data missing completely at random (MCAR) or missing at random (MAR).

```{r}
pairwise_deletion_data <- pairwise_deletion(data.MAR.uni)
```

## 3. Simple Imputation

Simple imputation replaces missing values using the mean, median, or mode of non-missing data, offering simplicity and ease of use. However, it can introduce bias or unrealistic results, especially in high-dimensional or large-scale data sets, making it unsuitable for handling big data effectively.

#### Mean Imputation

```{r}
# Evaluate imputation method using mean imputation
mean_imputation_data <- simple_imputation(data.MAR.uni, "mean")
mean_metrics <- compare_distributions(train_data, mean_imputation_data)
```

#### Median Imputation

```{r}
# Evaluate imputation method using median imputation
median_imputation_data <- simple_imputation(data.MAR.uni, "median")
median_metrics <- compare_distributions(train_data, median_imputation_data)
```
## 4. Regression Imputation
> NOTE: In the final version this is the idea (fit a model correctly here we simply used stepAIC) for each of the inputation techniqeus were this is needed

Regression imputation replaces missing values using predictions from a regression model built on complete data, preserving sample size and assuming data is missing at random (MAR). It includes single and multivariate regression methods, depending on the number of missing variables. 

```{r, class.source = 'fold-show'}
#' Regression Imputation with Stepwise Selection
#' @param data Data frame with missing values
#' @return Data frame with missing values imputed using stepwise regression
regression_imputation <- function(data) {
  for (col in names(data)) {
    if (any(is.na(data[[col]])) && is.numeric(data[[col]])) {
      # Get complete cases for this column
      complete_data <- data[complete.cases(data), ]
      incomplete_rows <- which(is.na(data[[col]]))
      predictors <- setdiff(names(data), col)
      
      # Fit initial model
      initial_model <- lm(as.formula(paste(col, "~ .")), data = complete_data)
      
      # Perform stepwise selection
      step_model <- stepAIC(initial_model, direction = "both", trace = FALSE)
      
      # Priting the summary of the models obtained
      print(summary(step_model))
      
      # Make predictions using the stepwise model
      predictions <- predict(step_model, newdata = data[incomplete_rows, predictors, drop = FALSE])
      
      # Impute the missing values
      data[[col]][incomplete_rows] <- predictions
    }
  }
  return(data)
}

# Perform imputation on MCAR and MAR datasets
regression_imputation_data <- regression_imputation(data.MAR.uni)
regression_metrics <- compare_distributions(train_data, regression_imputation_data)
```

```{r, echo = F}
# Build and evaluate models
model <- lm(target ~ ., data = regression_imputation_data)
predictions <- predict(model, test_data)
regression_imputation_rmse <- sqrt(mean((test_data$target - predictions)^2))
```

## 5. Hot-deck Imputation

Hot-deck imputation replaces missing values by selecting a donor from similar cases with complete data, either randomly or based on the closest match. It is widely used because it preserves data structure, reduces bias, and avoids model dependency. However, it lacks a robust theoretical foundation compared to other imputation techniques. 

```{r}
# Evaluate imputation method using hot-deck imputation
hotdeck_imputation_data <- hot_deck_imputation(data.MAR.uni)
hotdeck_metrics <- compare_distributions(train_data, hotdeck_imputation_data)
```

## 6. Expectation-Maximization (EM)

The expectation-maximization (EM) algorithm iteratively handles missing data using an "impute, estimate, and iterate" process until convergence. It alternates between the expectation step, estimating missing values based on observed data, and the maximization step, optimizing the likelihood of the complete data. It requires costly matrix computations.

```{r}
# Evaluate imputation method using EM imputation
em_imputation_data <- em_imputation(data.MAR.uni)
em_metrics <- compare_distributions(train_data, em_imputation_data)
```

## 7. Multiple Imputation

Multiple imputation addresses missing data by generating multiple complete datasets using observed data distributions to estimate missing values, reflecting uncertainty. Analysis is performed on each dataset, and results are combined into a single estimate. This technique overcomes single imputation limitations, reducing bias and providing more reliable results. 

## 8. GAM Imputation

GAM-based imputation leverages Generalized Additive Models to handle missing data, allowing for flexible modeling of relationships between variables. This technique is particularly effective when the relationship between predictors and the target variable is non-linear.

```{r}
# Evaluate imputation method using GAM-based imputation
gam_imputation_data <- gam_based_imputation(data.MAR.uni)
gam_metrics <- compare_distributions(train_data, gam_imputation_data)
```

## Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

### Decision tree / Random Forests

Decision tree imputation builds a predictive model for each variable with missing data by constructing a decision tree where branches represent decisions based on predictor variables, and leaves represent the predicted values. This method can handle both categorical and numerical data while capturing complex interactions and patterns in the data.

```{r}
# Evaluate imputation method using tree-based (Random Forest) imputation
forest_imputation_data <- tree_based_imputation(data.MAR.uni)
forest_metrics <- compare_distributions(train_data, forest_imputation_data)
```

## Ensemble methods

...

# Results and Discussion

## Performance metrics between Datasets
> Comparing the different datasets obtained via inputation

### The Kantorovich Problem

The Kantorovich problem calculates the distance between two probability measures.

**Definition:**
let \((X, \mathcal{X})\) and \((Y, \mathcal{Y})\) be two Polish spaces, that is, complete and separable metric spaces. 
Consider two probability measures \(P\) and \(Q\) belonging to the set of probability measures on \(X\) and \(Y\), 
respectively denoted by \(\mathcal{P}(X)\) and \(\mathcal{P}(Y)\). 

We define the set

$$
\Pi(P,Q) := \bigl\{\pi \in \mathcal{P}(X \times Y) : 
\pi(X \times \cdot) = P(\cdot),\ \pi(\cdot \times Y) = Q(\cdot)\bigr\},
$$

which is the class of probability measures on \((X \times Y,\, \mathcal{X} \times \mathcal{Y})\) 
whose marginals are \(P\) and \(Q\). The elements \(\pi\) of this class are called transport plans.

All elements of \(\Pi\) are probability measures that can be interpreted as ways to transfer the "mass" from \(P\) to \(Q\). 

However, moving this "mass" has a cost, defined by the cost function

$$
c(\cdot,\cdot) : X \times Y \to \mathbb{R}.
$$

Then, once a transport plan \(\pi \in \Pi\) is fixed, we define

$$
I_c(\pi) := \int_{X \times Y} c(x,y)\,\pi(dx\,dy),
$$

which represents the average cost of transferring \(P\) to \(Q\), associated with that particular transport plan. 

We can therefore formulate the Kantorovich problem as follows:

$$
K_c(P,Q) := \inf_{\pi \in \Pi} \int_{X \times Y} c(x,y)\,\pi(dx\,dy).
$$

Having defined the Kantorovich problem, we now assume that \(X = Y\) and let \(d\) be a distance on \(X\).  
We can then define the Wasserstein distance of order \(p\), denoted \(W_p\), as follows:

$$
W_p(P,Q) 
:= K_{d^p}(P,Q)^{1/p} 
= \biggl(\inf_{\pi \in \Pi(P,Q)} \int_{X^2} d^p(x,y)\,\pi(dx\,dy)\biggr)^{\!\!1/p}.
$$

The difference between the Kantorovich definition and the Wasserstein definition lies in the cost function, 
which is given by \(d(x,y)^p\), where \(d(\cdot,\cdot)\) is a distance on \(X\). It is precisely this definition 
of the cost function that guarantees, via the following proposition, that \(W_p\) is indeed a distance.

**Proposition**  
Assume \(X = Y\) and that for \(p > 1\), \(c(x,y) = d(x,y)^p\), where \(d\) is a distance on \(X\), i.e.:

1. \(d(x,y) = d(y,x) \geq 0\);
2. \(d(x,y) = 0\) if and only if \(x = y\);
3. \(\forall (x,y,z) \in X^3,\ d(x,z) \le d(x,y) + d(y,z)\).

Then \(W_p\) is a distance, namely it is symmetric, positive, satisfies \(W_p(P,Q) = 0\) 
if and only if \(P = Q\), and fulfills the triangle inequality:

$$
\forall (P,Q,Z) \in \mathcal{P}(X)^3 \quad 
W_p(P,Z) \le W_p(P,Q) + W_p(Q,Z).
$$

#### The Discrete Case

The Kantorovich problem can also be expressed in discrete form.

**Definition:** suppose \(X = \{ x_1, \dots, x_n \}\) and \(Y = \{ y_1, \dots, y_m \}\). 
In this case, \(P\) and \(Q\) can be identified with two probability vectors 
\(a \in \Sigma_n\) and \(b \in \Sigma_m\), where

$$
\Sigma_n 
= \bigl\{ a \in \mathbb{R}_+^n : \sum_{i=1}^n a_i = 1 \bigr\}.
$$

These probability vectors can be thought of as histograms, in which 
each \(a_i\) represents the probability associated with \(x_i\), and each \(b_j\) 
represents the probability associated with \(y_j\). 

We can define the discrete analogue of the set of probability measures 
\(\Pi(P,Q)\) as follows:

$$
U(a, b) := \bigl\{\, P \in \mathbb{R}_+^{n \times m} : 
P\,\mathbf{1}_m = a,\; P^\top \mathbf{1}_n = b \bigr\},
$$

where \(P\) is the transport matrix, the discrete analogue of 
the transport plan \(\pi\). In other words, 
\(P_{i,j}\) tells us how much mass to move from the point \(x_i\) to the point \(y_j\).

From bin \(i\) of \(a\) to bin \(j\) of \(b\). The constraints related to the set \(U(a,b)\) 
are similar to those of \(\Pi\), meaning that the sum of the rows or columns 
returns \(a\) or \(b\), so that \(P\,\mathbf{1}_m = a = \sum_{j=1}^m P_{i,j}\) 
for each \(i \in \{1,\dots,n\}\) and \(P^\top \mathbf{1}_n = \sum_{i=1}^n P_{i,j} = b_j\) 
for each \(j \in \{1,\dots,m\}\).

The cost function \(\mathbf{c}\) is represented by a matrix \(\mathbf{C} \in \mathbb{R}^{n \times m}\), 
where each \(C_{i,j} = c(x_i, y_j)\) explains the cost of going from \(a_i\) to \(b_j\). 
The average cost analogous to \(I_c\) in the discrete case is defined as

$$
\bar{I}_c 
= \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

We can then formulate the discrete Kantorovich problem as follows:

$$
L_C(a,b) 
:= \min_{P \in U(a,b)} \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

As in the continuous case, assuming \(X, Y \subset E\), with \(E\) a metric space 
equipped with a distance \(d\), we can also define the Wasserstein distance 
of order \(p\) in the discrete setting:

$$
W_p(a,b) 
:= \min_{P \in U(a,b)} 
\biggl(\sum_{i,j} D_{i,j}^p \, P_{i,j}\biggr)^{\!\!1/p}.
$$

where \(D_{i,j}^p = d^p(x_i, y_j)\) is the matrix of distances between the points \(x_i\) and \(y_j\).  
If \(X,Y \subset \mathbb{R}^n\), a standard example of distance between two points in the discrete case 
is \(D_{i,j} = \|x_i - y_j\|\), where \(\|\cdot\|\) is the Euclidean norm.

In the Wasserstein case, we consider the cost matrix \(\mathbf{C} = \mathbf{D}\), 
with \(\mathbf{D}\) being the matrix of Euclidean distances. 

### Jensen–Shannon Divergence

The Jensen–Shannon divergence (JSD) is a measure of similarity (or dissimilarity) between two probability distributions. 
**Definition:** let \(P\) and \(Q\) be two discrete probability distributions over the same domain \(\Omega\). Define

\[
M = \tfrac{1}{2}\,(P + Q),
\]

which is the average (or midpoint) distribution of \(P\) and \(Q\). Then the Jensen–Shannon divergence between \(P\) and \(Q\) is given by

\[
\mathrm{JSD}(P \,\|\, Q) 
= \tfrac{1}{2}\,\mathrm{KL}(P \,\|\, M)
+ \tfrac{1}{2}\,\mathrm{KL}(Q \,\|\, M),
\]

where \(\mathrm{KL}(\cdot \,\|\, \cdot)\) denotes the KL divergence, defined by:

\[
\mathrm{KL}(P \,\|\, Q) 
= \sum_{x \in \Omega} P(x) \,\log\!\Bigl(\tfrac{P(x)}{Q(x)}\Bigr).
\]

**Properties:**

1. **Symmetry**  
   \[
   \mathrm{JSD}(P \,\|\, Q) = \mathrm{JSD}(Q \,\|\, P).
   \]
   This follows because \(M\) is symmetric in \(P\) and \(Q\).

2. **Non-negativity**  
   \[
   \mathrm{JSD}(P \,\|\, Q) \ge 0,
   \]
   with equality if and only if \(P = Q\).

3. **Boundedness**  
   \[
   0 \,\le\, \mathrm{JSD}(P \,\|\, Q) \,\le\, \log(2).
   \]
   The maximum value \(\log(2)\) occurs when \(P\) and \(Q\) have disjoint supports.

4. **Metric Property**  
   Taking the square root gives a metric:
   \[
   d_{\mathrm{JSD}}(P, Q) := \sqrt{\mathrm{JSD}(P \,\|\, Q)}.
   \]
   This metric satisfies the triangle inequality.


```{r, echo = T, class.source = 'fold-show'}
# Create comparison results dataframe with safe extraction for MAE, RMSE, and Correlation
comparison_methods <- c("Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree")

# Create a data frame from the list of metrics
distribution_comparison <- data.frame(
  Method = rep(comparison_methods, each = 2),  # Repeat each method for two metrics
  Metric_Type = rep(c("Wasserstein", "JSD"), times = length(comparison_methods)),
  Value = c(mean_metrics$wasserstein, mean_metrics$jsd,
            median_metrics$wasserstein, median_metrics$jsd,
            regression_metrics$wasserstein, regression_metrics$jsd,
            hotdeck_metrics$wasserstein, hotdeck_metrics$jsd,
            em_metrics$wasserstein, em_metrics$jsd,
            gam_metrics$wasserstein, gam_metrics$jsd,
            forest_metrics$wasserstein, forest_metrics$jsd)
)
```

```{r, echo = F}
# Create the faceted distribution metrics plot with filtered data
p1 <- ggplot(distribution_comparison, 
             aes(x = reorder(Method, Value), 
                 y = Value)) +
  geom_bar(stat = "identity", 
           fill = "#5E81AC") +
  geom_text(aes(label = round(Value, 3)), 
            vjust = -0.5,
            size = 3) +
  facet_wrap(~Metric_Type, 
             scales = "free_y",  # Allow different y-axis scales for each metric
             ncol = 2) +        # Place plots side by side
  labs(title = "Distribution Metrics Comparison",
       x = "Method",
       y = "Metric Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 12, face = "bold"),
        strip.background = element_rect(fill = "#E5E9F0", color = NA),
        panel.spacing = unit(2, "lines"))  # Add some space between the facets

# Print the plot
print(p1)
```

## Performance metrics for the Prediction

- **Mean Absolute Error (MAE)**

    MAE measures the average difference between imputed values and true values defined as:
    
    $$
    MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
    $$

- **Mean Squared Error (MSE)**
    
    While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
    
    $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
    $$

- **Root Mean Square Error (RMSE)**

    RMSE computes the difference in imputed values and actual values as follows:
  
    $$
    RMSE = \sqrt{MSE}
    $$
    
- **Area under the curve (AUC)**

    AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
    
    $$
    TPR = \frac{TP}{TP + FN} \tag{21}
    $$
    
    $$
    FPR = \frac{FP}{FP + TN} \tag{22}
    $$
    
#### Function implementation for different performance metrics
> They are defined inside metrics
    
```{r}
# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```

### Test linear model for each Imputation method

```{r}
# List of precomputed imputed datasets
imputed_datasets <- list(
  "Original Dataset" = train_data, # Do it again for the original dataset
  "Listwise Deletion" = listwise_deletion_data,
  "Pairwise Deletion" = pairwise_deletion_data,
  "Mean Imputation" = mean_imputation_data,
  "Median Imputation" = median_imputation_data,
  "Regression Imputation" = regression_imputation_data,
  "Hot-deck Imputation" = hotdeck_imputation_data,
  "EM Imputation" = em_imputation_data,
  "GAM Imputation" = gam_imputation_data,
  "Random Forest Imputation" = forest_imputation_data
)

# Function to calculate RMSE and MAE for each imputed dataset
calculate_metrics <- function(imputed_data) {
  metrics <- evaluate_model_performance(imputed_data, test_data)
  return(c(metrics$rmse, metrics$mae))
}

# Apply the function to all imputed datasets and store results
metrics_results <- do.call(rbind, lapply(names(imputed_datasets), function(method) {
  metrics <- calculate_metrics(imputed_datasets[[method]])
  data.frame(
    Method = method,
    Metric = c("RMSE", "MAE"),
    Value = metrics,
    Highlight = ifelse(method == "Original Dataset", "Highlight", "Other"),
    stringsAsFactors = FALSE
  )
}))
```

#### Comparing Predictions (RMSE and MAE) of the linear model for each method
> The original dataset is highlighted in red.

Here we simply made a simple linear model with all of the covariate (knowing our synthetic dataset). To compare the RMSE and MAE when dealing with different inputated dataset.

```{r, echo = FALSE}
# Define the function to create the plot with horizontal bars
create_bar_plot <- function(metric_name) {
  # Get the maximum value to set appropriate limits
  max_value <- max(metrics_results[metrics_results$Metric == metric_name,]$Value)
  
  ggplot(metrics_results[metrics_results$Metric == metric_name,], 
         aes(x = reorder(Method, Value), y = Value, fill = Highlight)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = sprintf("%.3f", Value)), hjust = -0.5) +
    labs(
      title = paste(metric_name, "Across Imputation Methods"),
      x = "Imputation Method",
      y = paste(metric_name, "Value")
    ) +
    scale_fill_manual(
      values = c("Highlight" = red_nord, "Other" = blue_nord),
      guide = "none"
    ) +
    theme_minimal() +
    coord_flip(xlim = NULL, ylim = c(0, max_value * 1.2)) +  # Add 20% extra space on the right
    theme(
      plot.margin = unit(c(1, 1, 1, 1), "pt")  # top, right, bottom, left margins
    )
}

# Create and display the plots
mae_plot <- create_bar_plot("MAE")
rmse_plot <- create_bar_plot("RMSE")

# To display the plots
mae_plot
rmse_plot
```

## Computational Performance

.. Some small note on this

# Conclusions and Recommendations

> Perhaps some final plots and small discussions

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References
> Need to add references

```{r}
citation("pROC")
```

- ...

