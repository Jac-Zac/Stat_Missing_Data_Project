---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
    wrap: 72
---

```{r, echo=F, class.source = 'fold-show'}
library(here)

# Load utilities
source(here("src", "setup.R"))

# Define the child RMarkdown file path
child <- here("notebooks", "dataset_analysis", "synthetic_data_analysis.Rmd")
```

```{r, child = here("notebooks", "dataset_analysis", "synthetic_data_analysis.Rmd"), cache = TRUE}
# Run the file which does data analyses even if not knitted by runned manually
rmarkdown::render(child)
```
# Imputation Methods

The following imputation methods are tested on both the MCAR and MAR datasets. A regression model is fit on the imputed dataset, and the results are summarized.

## 1. Listâ€‘wise (Case) Deletion

List-wise deletion removes any case with missing values from analysis and is the default method in many statistical software tools. It works well when data is large and missing completely at random (MCAR). However, if these conditions are not met, it can lead to bias and loss of important information, making it a less suitable approach.

```{r}
listwise_deletion_data <- listwise_deletion(data.MAR.uni)
```
## 2. Pairwise Deletion

Pairwise deletion minimizes information loss compared to list-wise deletion by only removing data points when necessary to check if missing values are truly missing. It produces lower bias for data missing completely at random (MCAR) or missing at random (MAR).

```{r}
pairwise_deletion_data <- pairwise_deletion(data.MAR.uni)
```

## 3. Simple Imputation

Simple imputation replaces missing values using the mean, median, or mode of non-missing data, offering simplicity and ease of use. However, it can introduce bias or unrealistic results, especially in high-dimensional or large-scale data sets, making it unsuitable for handling big data effectively.

#### Mean Imputation

```{r}
# Evaluate imputation method using mean imputation
mean_imputation_data <- simple_imputation(data.MAR.uni, "mean")
mean_metrics <- compare_distributions(train_data, mean_imputation_data)
```

#### Median Imputation

```{r}
# Evaluate imputation method using median imputation
median_imputation_data <- simple_imputation(data.MAR.uni, "median")
median_metrics <- compare_distributions(train_data, median_imputation_data)
```
## 4. Regression Imputation
> NOTE: In the final version this is the idea (fit a model correctly here we simply used stepAIC) for each of the inputation techniqeus were this is needed

Regression imputation replaces missing values using predictions from a regression model built on complete data, preserving sample size and assuming data is missing at random (MAR). It includes single and multivariate regression methods, depending on the number of missing variables. 

```{r, class.source = 'fold-show'}
#' Regression Imputation with Stepwise Selection
#' @param data Data frame with missing values
#' @return Data frame with missing values imputed using stepwise regression
regression_imputation <- function(data) {
  for (col in names(data)) {
    if (any(is.na(data[[col]])) && is.numeric(data[[col]])) {
      # Get complete cases for this column
      complete_data <- data[complete.cases(data), ]
      incomplete_rows <- which(is.na(data[[col]]))
      predictors <- setdiff(names(data), col)
      
      # Fit initial model
      initial_model <- lm(as.formula(paste(col, "~ .")), data = complete_data)
      
      # Perform stepwise selection
      step_model <- stepAIC(initial_model, direction = "both", trace = FALSE)
      
      # Priting the summary of the models obtained
      print(summary(step_model))
      
      # Make predictions using the stepwise model
      predictions <- predict(step_model, newdata = data[incomplete_rows, predictors, drop = FALSE])
      
      # Impute the missing values
      data[[col]][incomplete_rows] <- predictions
    }
  }
  return(data)
}

# Perform imputation on MCAR and MAR datasets
regression_imputation_data <- regression_imputation(data.MAR.uni)
regression_metrics <- compare_distributions(train_data, regression_imputation_data)
```

```{r, echo = F}
# Build and evaluate models
model <- lm(target ~ ., data = regression_imputation_data)
predictions <- predict(model, test_data)
regression_imputation_rmse <- sqrt(mean((test_data$target - predictions)^2))
```

## 5. Hot-deck Imputation

Hot-deck imputation replaces missing values by selecting a donor from similar cases with complete data, either randomly or based on the closest match. It is widely used because it preserves data structure, reduces bias, and avoids model dependency. However, it lacks a robust theoretical foundation compared to other imputation techniques. 

```{r}
# Evaluate imputation method using hot-deck imputation
hotdeck_imputation_data <- hot_deck_imputation(data.MAR.uni)
hotdeck_metrics <- compare_distributions(train_data, hotdeck_imputation_data)
```

## 6. Expectation-Maximization (EM)

The expectation-maximization (EM) algorithm iteratively handles missing data using an "impute, estimate, and iterate" process until convergence. It alternates between the expectation step, estimating missing values based on observed data, and the maximization step, optimizing the likelihood of the complete data. It requires costly matrix computations.

```{r}
# Evaluate imputation method using EM imputation
em_imputation_data <- em_imputation(data.MAR.uni)
em_metrics <- compare_distributions(train_data, em_imputation_data)
```

## 7. Multiple Imputation

Multiple imputation addresses missing data by generating multiple complete datasets using observed data distributions to estimate missing values, reflecting uncertainty. Analysis is performed on each dataset, and results are combined into a single estimate. This technique overcomes single imputation limitations, reducing bias and providing more reliable results. 

## 8. GAM Imputation

GAM-based imputation leverages Generalized Additive Models to handle missing data, allowing for flexible modeling of relationships between variables. This technique is particularly effective when the relationship between predictors and the target variable is non-linear.

```{r}
# Evaluate imputation method using GAM-based imputation
gam_imputation_data <- gam_based_imputation(data.MAR.uni)
gam_metrics <- compare_distributions(train_data, gam_imputation_data)
```

## Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

### Decision tree / Random Forests

Decision tree imputation builds a predictive model for each variable with missing data by constructing a decision tree where branches represent decisions based on predictor variables, and leaves represent the predicted values. This method can handle both categorical and numerical data while capturing complex interactions and patterns in the data.

```{r}
# Evaluate imputation method using tree-based (Random Forest) imputation
forest_imputation_data <- tree_based_imputation(data.MAR.uni)
forest_metrics <- compare_distributions(train_data, forest_imputation_data)
```

## Ensemble methods

...

# Results and Discussion

## Performance metrics between Datasets
> Comparing the different datasets obtained via inputation

...

```{r, echo = T, class.source = 'fold-show'}
# Create comparison results dataframe with safe extraction for MAE, RMSE, and Correlation
comparison_methods <- c("Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree")

# Create a data frame from the list of metrics
distribution_comparison <- data.frame(
  Method = rep(comparison_methods, each = 2),  # Repeat each method for two metrics
  Metric_Type = rep(c("Wasserstein", "JSD"), times = length(comparison_methods)),
  Value = c(mean_metrics$wasserstein, mean_metrics$jsd,
            median_metrics$wasserstein, median_metrics$jsd,
            regression_metrics$wasserstein, regression_metrics$jsd,
            hotdeck_metrics$wasserstein, hotdeck_metrics$jsd,
            em_metrics$wasserstein, em_metrics$jsd,
            gam_metrics$wasserstein, gam_metrics$jsd,
            forest_metrics$wasserstein, forest_metrics$jsd)
)
```

```{r, echo = F}
# Create the faceted distribution metrics plot with filtered data
p1 <- ggplot(distribution_comparison, 
             aes(x = reorder(Method, Value), 
                 y = Value)) +
  geom_bar(stat = "identity", 
           fill = "#5E81AC") +
  geom_text(aes(label = round(Value, 3)), 
            vjust = -0.5,
            size = 3) +
  facet_wrap(~Metric_Type, 
             scales = "free_y",  # Allow different y-axis scales for each metric
             ncol = 2) +        # Place plots side by side
  labs(title = "Distribution Metrics Comparison",
       x = "Method",
       y = "Metric Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 12, face = "bold"),
        strip.background = element_rect(fill = "#E5E9F0", color = NA),
        panel.spacing = unit(2, "lines"))  # Add some space between the facets

# Print the plot
print(p1)
```

We need to add some description here ...

## Performance metrics for the Prediction

- **Mean Absolute Error (MAE)**

    MAE measures the average difference between imputed values and true values defined as:
    
    $$
    MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
    $$

- **Mean Squared Error (MSE)**
    
    While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
    
    $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
    $$

- **Root Mean Square Error (RMSE)**

    RMSE computes the difference in imputed values and actual values as follows:
  
    $$
    RMSE = \sqrt{MSE}
    $$
    
- **Area under the curve (AUC)**

    AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
    
    $$
    TPR = \frac{TP}{TP + FN} \tag{21}
    $$
    
    $$
    FPR = \frac{FP}{FP + TN} \tag{22}
    $$
    
#### Function implementation for different performance metrics
> They are defined inside metrics
    
```{r}
# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```

### Comparing Predictions

> Here I have to add back the predictions and the plots

## Computational Performance

.. Some small note on this

# Conclusions and Recommendations

> Perhaps some final plots and small discussions

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References
> Need to add references

```{r}
citation("pROC")
```

- ...

