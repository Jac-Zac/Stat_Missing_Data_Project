---
title: "Case Study"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true 
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
---

```{r librerie, echo=TRUE, results='hide', warning=FALSE, include = F}
# Load utilities
suppressMessages(library(here))

# Load utilities
source(here("src", "setup.R"))
```


```{r import data, echo=FALSE, results='hide', warning=FALSE}
# We work with the modified version, and we set the col names by looking at the names file
auto_mpg <- read.table("auto-mpg.data", header = FALSE, sep = "")

# Assign column names based on the attribute information provided
colnames(auto_mpg) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name")

# Define the color pallet
nord_colors= c("#5e81ac", "#bf616a", "#a3be8c", "#d08770", "#ebcb8b", "#b48ead")
```

# Second part
## Introduction

In this second part of our project, we address the challenge of missing data in real-world datasets. Unlike the previous section, our approach here begins with a complete dataset —specifically, a refined version of the well-known Auto MPG dataset. 
We will systematically introduce missing values into one variable, conditioned on two other variables, to simulate a Missing at Random mechanism. This will be done using functions from the VIM package in R.

Following the introduction of missing data, we will apply the various imputation techniques, previously discussed, to generate complete datasets. However, instead of simply comparing the distributions of these imputed datasets, our focus will shift to evaluating their performance in the context of linear regression models. Specifically, we will assess the impact of different imputation methods by comparing the estimated coefficients and Mean Squared Errors of the fitted models.

In the final segment of this section, we will extend our analysis by introducing missing values across multiple covariates, conditioned on the remaining variables, resulting in a dataset with a substantial proportion of missing data. To address this, we will employ multiple imputation techniques, particularly those implemented in the MICE package, which utilizes chained equations for imputing multiple variables. Subsequently, we will compare the performance of linear models fitted on these newly imputed datasets.

We have selected the Auto MPG dataset for this analysis due to its simplicity and recognition in the field. Our objective is to fit regression models starting from this dataset and systematically evaluate the effects of various imputation strategies on model performance.

Let's start by analyzing the data, exploring its features, and examining their relationships.

##Dataset exploration
The "auto-mpg" dataset tracks various technical information about car models manufactured in the 1970s and 1980s. The response variable will be mpg (miles per gallon).

First, we examine the variables and observations in the dataset:

```{r info, echo=FALSE}
str(auto_mpg)
summary(auto_mpg)
```

There are 9 variables with a total of 398 observations.

-mpg: A continuous variable indicating the gallons of fuel used to travel one mile.
- cylinders: A discrete quantitative variable indicating the number of engine cylinders (to be treated as a factor).
- displacement: A continuous variable indicating engine displacement.
- horsepower: A character variable indicating the horsepower of the car (to be treated as a continuous quantitative variable).
- weight: A continuous variable indicating the car's weight.
- acceleration: A continuous variable indicating the car's acceleration.
- model_year: An integer variable indicating the year the car model was produced.
- origin: A categorical variable indicating the continent of the car manufacturer (1: America, 2: Europe, 3: Asia).
- car_name: A qualitative variable indicating the manufacturer and model name of the car.

### Data Pre-Processing
Before building our model, we need to make the data usable. We observe anomalies in the dataset variables, so we clean the data when possible. We also assign the correct data types to variables and create two new variables: cut_model_year and car_brand.

- cut_model_year: A categorical variable dividing production years into the classes 70-73, 74-76, 77-79, and 80-82.
- car_brand: A categorical variable that tracks only the car's manufacturer.

```{r preprocessing, warning=FALSE, echo = F}

# Convert horsepower as numeric
auto_mpg$horsepower <- as.numeric(auto_mpg$horsepower)

# Remove rows with missing values
auto_mpg <- na.omit(auto_mpg)

# Convert 'cylinders' and 'origin' to factors
auto_mpg$cylinders <- factor(auto_mpg$cylinders)
auto_mpg$origin <- factor(auto_mpg$origin) 

# Can be interpreted as a categorical variable but it is rectilinear
# split the model_year variable into categories based on the breaks
auto_mpg$cut_model_year <- cut(auto_mpg$model_year, breaks = c(69, 73, 76, 79, 82), labels = c("70-73", "74-76", "77-79", "80-82"))
auto_mpg$cut_model_year <- factor(auto_mpg$cut_model_year)

# Split car names by manufacturer
auto_mpg$car_brand <- str_split(auto_mpg$car_name, pattern = " ", simplify = TRUE)[, 1]

# Convert car name column to a factor
auto_mpg$car_brand <- as.factor(auto_mpg$car_brand)

# read as characters
auto_mpg$car_brand <- as.character(auto_mpg$car_brand)

# Fix mispelled names
auto_mpg$car_brand[auto_mpg$car_brand == "chevroelt"] <- "chevrolet"
auto_mpg$car_brand[auto_mpg$car_brand == "chevy"] <- "chevrolet"
auto_mpg$car_brand[auto_mpg$car_brand == "hi"] <- NA
auto_mpg$car_brand[auto_mpg$car_brand == "maxda"] <- "mazda"
auto_mpg$car_brand[auto_mpg$car_brand == "mercedes-benz"] <- "mercedes"
auto_mpg$car_brand[auto_mpg$car_brand == "toyouta"] <- "toyota"
auto_mpg$car_brand[auto_mpg$car_brand == "vokswagen"] <- "volkswagen"

# I think vw is for volkswagen
auto_mpg$car_brand[auto_mpg$car_brand == "vw"] <- "volkswagen"

#capri è un modello di ford
auto_mpg$car_brand[auto_mpg$car_brand == "capri"] <- "ford"

# Convert car names to factor after that
auto_mpg$car_brand <- as.factor(auto_mpg$car_brand)

# Drop the car name
auto_mpg <- subset(auto_mpg, select = -car_name)
auto_mpg <- subset(auto_mpg, select = -model_year)

auto_mpg <- na.omit(auto_mpg)#aggiunto per "hi"<-NA

# Count missing values in each column [mi sa che lo toglierò ~D]
#missing_values <- colSums(is.na(auto_mpg))
#print("I valori mancanti per le relative colonne sono: ")
#print(missing_values)

```


```{r summury post preprocessing, echo=FALSE}
str(auto_mpg)
summary(auto_mpg)
```

After these steps, we obtain a new dataset that contains 9 variables and 391 observations:

- mpg: Continuous, indicating fuel consumption per mile.
- cylinders: Discrete, indicating the number of cylinders, categorized into 5 levels: 3, 4, 5, 6, 8.
- displacement: Continuous, indicating engine displacement.
- horsepower: Continuous, indicating car horsepower.
- weight: Continuous, indicating car weight.
- acceleration: Continuous, providing a parameter for the car's acceleration.
- origin: Categorical, indicating the continent of origin (1: America, 2: Europe, 3: Asia).
- cut_model_year: Categorical, dividing production years into the classes 70-73, 74-76, 77-79, and 80-82.
- car_brand: Categorical, tracking the car's manufacturer.

Some variables have differences in the summary statistics due to the removal of elements with null values.

### Exploratory Data Analysis

#### Continuous Variables

Let's start by visualizing the correlation matrix of the continuous variables:

```{r correlation matrix, echo=FALSE}

# Exclude factor variables
non_factor_variables = auto_mpg[, sapply(auto_mpg, function(x) !is.factor(x))]
cor_matrix <- cor(non_factor_variables)

# Plot the correlation matrix with different upper and lower
corrplot.mixed(cor_matrix, 
    lower="number", 
    upper ="ellipse",
    addgrid.col = "gray",
    tl.col = "black",
    tl.cex = 0.7,
    lower.col = colorRampPalette(c(nord_colors[2], "white", nord_colors[1]))(100),
    upper.col = colorRampPalette(c(nord_colors[2], "white", nord_colors[1]))(100)
)
```

Our analysis reveals both significant positive and negative correlations among the variables. Notably, 'acceleration' exhibits the weakest correlations with other variables, with coefficients ranging from -0.69 to 0.42. In contrast, variables such as 'weight', 'horsepower', and 'displacement' demonstrate strong positive intercorrelations. This observation aligns with expectations, as higher displacement typically results in increased horsepower, and heavier vehicles necessitate more powerful engines. Consequently, these more powerful engines tend to consume more fuel, which is consistent with anticipated outcomes.

##### Scatterplots with mpg as the Response Variable

```{r, echo = FALSE}
create_plot <- function(data, x_var, x_label, title, color) {
  ggplot(data = data, mapping = aes(!!sym(x_var), log(mpg))) +
    geom_point(color = color) +
    xlab(x_label) +
    ylab("Mpg") +
    theme_bw() +
    ggtitle(title)
}

variables <- list(
  list(var = "displacement", label = "Cilindrata", title = "Scatterplot Cilindrata-Mpg", color = nord_colors[1]),
  list(var = "horsepower", label = "Cavalli-potenza", title = "Scatterplot Cavalli-Mpg", color = nord_colors[2]),
  list(var = "weight", label = "Peso", title = "Scatterplot Peso-Mpg", color = nord_colors[3]),
  list(var = "acceleration", label = "Accelerazione", title = "Scatterplot Accelerazione-Mpg", color = nord_colors[4])
)

plots <- lapply(variables, function(x) create_plot(auto_mpg, x$var, x$label, x$title, x$color))

plot_grid(plotlist = plots)

```

Scatterplots show clear patterns for the first three variables. The positive correlation between acceleration and mpg is less obvious because it has a lower absolute value.

##### Distribution of Continuous Variables


```{r histograms continues, echo=FALSE}
create_histogram <- function(data, var, label, title, color) {
  ggplot(data, mapping = aes(!!sym(var), fill = !!sym(color))) +
    geom_histogram(aes(y = after_stat(density)), color = "black", linewidth = 0.3, alpha = 0.6, fill = color, bins = 25) +
    geom_density(color = "black", linewidth = 0.3, fill = color, alpha = 0.6) +
    labs(x = label, y = "Densità", title = title) +
    theme_bw()
}

variables <- list(
  list(var = "displacement", label = "Cilindrata", title = "Istogramma Cilindrata", color = nord_colors[1]),
  list(var = "horsepower", label = "Cavalli-potenza", title = "Istogramma Cavalli", color = nord_colors[2]),
  list(var = "weight", label = "Peso", title = "Istogramma Peso", color = nord_colors[3]),
  list(var = "acceleration", label = "Accelerazione", title = "Istogramma Accelerazione", color = nord_colors[4])
)

histograms <- lapply(variables, function(x) create_histogram(auto_mpg, x$var, x$label, x$title, x$color))

plot_grid(plotlist = histograms)
```

The histogram of displacement shows a positively skewed distribution with a peak around 100. The distributions of weight and horsepower are also positively skewed, with heavier right tails. The acceleration variable shows a perfectly symmetrical distribution with mean, mode, and median around 15.

#### Distribution of mpg

```{r, echo=FALSE}
create_histogram(auto_mpg, "mpg", "Mpg", "Istogramma Miglia per Gallone", nord_colors[6])
```

The histogram shows that mpg has a positively skewed distribution with a long right tail. This is more evident when a kernel density curve is added to the histogram, suggesting that transformations may be necessary.

#### Categorical Variables

We analyze categorical variables in relation to mpg using boxplots. For the high number of categories in car_brand, we use a bar plot instead.

```{r plots for categorical variables, echo=FALSE}
# Create a base plot with common code

plots <- list(
    ## Boxplots
    ggplot(auto_mpg, aes(x = cylinders, y = mpg)) + 
    geom_boxplot(fill = nord_colors[1], alpha = 0.3) +  
    labs(title = "Boxplot of mpg by cylinders",
        x = "Cylinders", y = "Mpg") +
        theme_bw(),

    ggplot(auto_mpg, aes(x = origin, y = mpg)) + 
    geom_boxplot(fill = nord_colors[2], alpha = 0.3) +  
    labs(title = "Boxplot of mpg by origin",
        x = "Origin", y = "Mpg") + 
        theme_bw(),
    
    ggplot(auto_mpg, aes(x = cut_model_year, y = mpg)) + 
    geom_boxplot(fill = nord_colors[3], alpha = 0.3) +  
    labs(title = "Boxplot of mpg by model_year",
        x = "Model Year categories", y = "Mpg") + 
        theme_bw(),
    
    ggplot(auto_mpg, aes(x = car_brand, y = mpg)) +
        stat_summary(fun = mean, geom = "col", fill = nord_colors[4]) +
        scale_fill_gradient2(low = nord_colors[1],
        high = nord_colors[2], mid = "white", midpoint = 0) +
        theme_bw() +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
        labs(title = "Barplot of mean mpg by car_brand",
             x = "Car brand", y = "Mean mpg")
)

grid.arrange(grobs = plots, ncol = 2)
```

The number of cylinders slightly influences mpg. We notice that 4-cylinder engines appear to be the most efficient (in fact, mid-to-high range compact cars generally use this type of engine, where low fuel consumption is a key requirement). Conversely, 8-cylinder engines are the least efficient in terms of fuel consumption (as 8-cylinder engines, typically V8s, are characteristic of supercars or American muscle cars).

The country where the car manufacturer is based also seems to impact fuel consumption. American manufacturers tend to produce cars with higher fuel consumption, while Asian manufacturers perform slightly better than European ones.

The year of production shows a positive trend regarding fuel efficiency as the years progress. This could be attributed to technological advancements enabling the production of more fuel-efficient cars or to the oil crises of the 1970s, which may have pushed manufacturers in this direction.

Finally, regarding the car manufacturer, it is difficult to identify very significant patterns. The only notable observation is that some brands, like Nissan, for example, have a much higher average miles-per-gallon compared to brands like Chrysler (noting that the former is an Asian brand, while the latter is American).

### Histograms and Density Curves

#### Histogram of mpg by Origin

```{r historgrams origin, echo=FALSE}
## Histogram
ggplot(auto_mpg, aes(x = mpg, fill=origin )) +
    # Plot the histogram with density instead of counts on the y-axis
    geom_histogram(aes(y = after_stat(density)),
    color = "black", linewidth=0.3, alpha = 0.5, bins=25) + 
    facet_wrap(~ origin) + # Create facets for each smoker category
    geom_density(aes(colour = factor(displacement)), lwd = 0.25, colour = "black") +
    scale_fill_manual(values = alpha(nord_colors,0.6)) + # Customize the fill color with transparency
    scale_colour_manual(values = alpha(nord_colors,1)) + # Customize the line color with full opacity
    labs(title = NULL , x = NULL, y = NULL) +  # Add legend titles and labels
    theme_bw() +
    theme(legend.position = "none", axis.text = element_text(size = 6)) # Hide the color labels
```

The graphs show unimodal curves with differing frequency peaks and averages based on the continent of origin:

- American Cars: Average 20 mpg, mode 13 mpg.
- European Cars: Average 27 mpg, mode 24 mpg.
- Asian Cars: Average 30 mpg, mode 32 mpg.

This confirms that American cars generally consume more fuel, followed by European cars, with Asian cars being the most fuel-efficient.

##### Histogram of mpg by Number of Cylinders

```{r historgrams cylinders, echo=FALSE}
## Histogram
ggplot(auto_mpg, aes(x = mpg, fill=cylinders )) +
    # Plot the histogram with density instead of counts on the y-axis
    geom_histogram(aes(y = after_stat(density)),
    color = "black", linewidth=0.3, alpha = 0.5, bins=15) + 
    facet_grid(~ cylinders) + # Create facets for each smoker category
    geom_density(aes(colour = factor(displacement)), lwd = 0.25, colour = "black") +
    scale_fill_manual(values = alpha(nord_colors,0.6)) + # Customize the fill color with transparency
    scale_colour_manual(values = alpha(nord_colors,1)) + # Customize the line color with full opacity
    labs(title = NULL, x = NULL, y = NULL) +  # Add legend titles and labels
    theme_bw() +
    theme(legend.position = "none", axis.text = element_text(size = 6)) # Hide the color labels
```

The distributions are similar across categories, with high peaks followed by heavier right tails. This suggests that the number of cylinders is not a major factor in explaining mpg. However, cars with more cylinders tend to achieve fewer miles per gallon. Data on 5-cylinder cars is limited, making conclusions difficult.

##### Histogram of mpg by Production Year

```{r historgrams model year, echo=FALSE}
## Histogram
ggplot(auto_mpg, aes(x = mpg, fill=cut_model_year )) +
    # Plot the histogram with density instead of counts on the y-axis
    geom_histogram(aes(y = after_stat(density)),
    color = "black", linewidth=0.3, alpha = 0.5, bins=20) + 
    facet_wrap(~ cut_model_year) + # Create facets for each smoker category
    geom_density(aes(colour = factor(displacement)), lwd = 0.25, colour = "black") +
    scale_fill_manual(values = alpha(nord_colors,0.6)) + # Customize the fill color with transparency
    scale_colour_manual(values = alpha(nord_colors,1)) + # Customize the line color with full opacity
    labs(title = NULL, x = NULL, y = NULL) +  # Add legend titles and labels
    theme_bw() +
    theme(legend.position = "none", axis.text = element_text(size = 6),
          plot.title = element_text(size = 14)) # Hide the color labels
```

Production year initially appears to have little influence, but there is a marked change in distribution patterns for cars from the 1980s, particularly compared to those from the early 1970s.


### Development and Evaluation of a Linear Regression Model on the Complete Auto MPG Dataset

In this subsection, we consider the construction and assessment of a linear regression model utilizing the complete Auto MPG dataset. Initially, we partition the dataset into training and testing subsets, employing an 80-20 split to ensure robust model validation. Subsequently, we fit multiple linear regression models to identify the optimal combination of predictor variables. Our primary model focuses on predicting the natural logarithm of mpg based on variables such as weight, horsepower, and categorized model year. We then evaluate the performance of this model, comparing it against alternative models with different predictor combinations, to determine the most effective predictors for fuel efficiency.

```{r}
#Train-test split
set.seed(42)
train_index <- createDataPartition(auto_mpg$mpg, p = 0.8, list = FALSE)  # 80-20 split
train_data <- auto_mpg[train_index, ]
test_data <- auto_mpg[-train_index, ]

test_data$mpg<-log(test_data$mpg)
```

```{r}
data_original<-train_data
```

Best fitted lmodel

```{r}
fit_multi1 <- lm(I(log(mpg))~weight+horsepower + cut_model_year,data = auto_mpg)#ottimo
#fit_multi2 <- lm(I(log(mpg))~weight + cut_model_year,data = auto_mpg)#ni
#fit_multi3 <- lm(I(log(mpg))~weight+displacement+cut_model_year ,data = auto_mpg)#bo
#fit_multi4 <- lm(I(log(mpg))~weight,data = auto_mpg)#bo

#summary(fit_multi1)
#summary(fit_multi2)
#summary(fit_multi3)
#summary(fit_multi4)

# plot(compare_performance(fit_multi1, fit_multi2, fit_multi3, fit_multi4))
```
From the assessment of the models we obtain that the model that uses as predictors weight, horsepower and cut_model_year is the best in fitting data.

### Generation and visualization of missing values using MAR 

As anticipated, we generate missing values in the dataset with MAR mechanism. We underline that we decided to remove values on the predictor weight with respect to acceleration and displacement, since its high correlation with other variables and importance role in predicting the target. 

```{r create mv}
missingMar<-delete_MAR_1_to_x(data_original, p = 0.2, cols_mis = "weight", cols_ctrl ="displacement", x =100)
missingMar<-delete_MAR_censoring(missingMar, p = 0.1, cols_mis = "weight", cols_ctrl = "acceleration")

#plot VIM
aggr_plot <- aggr(missingMar, numbers = TRUE,labels=names(data_original), prop = c(TRUE, FALSE) )


#plot 2
missing.rows = dim(missingMar)[1] -  dim(na.omit(missingMar))[1]
#sprintf("Dim dataset: [%s]", toString(dim(missingMar)))
sprintf("Missing rows: %s (%s%%)", missing.rows, round((missing.rows*100)/dim(missingMar)[1], 2))

missings_df <- data.frame(type=c("missing", "non-missing") ,count = c(missing.rows,  dim(na.omit(missingMar))[1]))

ggplot(missings_df, aes(fill=type, y="", x=count)) + 
    geom_bar(position="stack", stat="identity")+
    ggtitle("%missing data") +
    xlab("Obs") + ylab("") +
    theme(text = element_text(size = 18))+
    scale_fill_manual(values = c(nord_colors[2], nord_colors[3]))+
    theme_bw()
``` 

## Plot missingvalues of weight against displacement and acceleration

```{r plot mv, warning=FALSE}
# Calculate global ranges for both x and y variables
dis_range <- range(c(data_original$displacement, missingMar$displacement), na.rm = TRUE)
weight_range <- range(c(data_original$weight, missingMar$weight), na.rm = TRUE)
accel_range <- range(c(data_original$acceleration, missingMar$acceleration), na.rm = TRUE)

# Add padding to ranges
dis_pad <- diff(dis_range) * 0.05
weight_pad <- diff(weight_range) * 0.05
accel_pad <- diff(accel_range) * 0.05

# First plot: DIS vs. Weight
plot_data1 <- rbind(
  data.frame(
    X = data_original$displacement,
    Y = data_original$weight,
    missing = "Present"
  ),
  data.frame(
    X = missingMar$displacement,
    Y = missingMar$weight,
    missing = "Missing"
  )
)

scatter_plot1 <- ggplot(plot_data1, aes(x = X, y = Y, color = missing)) +
  geom_point(size = 2) +  # Increased point size
  scale_color_manual(values = c("Missing" = "black", "Present" = red_nord)) +  # Corrected color mapping
  scale_x_continuous(limits = c(dis_range[1] - dis_pad, dis_range[2] + dis_pad)) +
  scale_y_continuous(limits = c(weight_range[1] - weight_pad, weight_range[2] + weight_pad)) +
  theme_minimal(base_size = 12) +  # Increased base font size
  labs(color = "Data Status", x = "Displacement", y = "Weight") +
  theme(legend.position = "none")

final_plot1 <- ggMarginal(scatter_plot1, 
                          margins = "y", 
                          groupColour = TRUE, 
                          groupFill = TRUE,
                          type = "boxplot", 
                          size = 5,  
                          width = 0.2,  
                          outlier.size = 1)

# Second plot: Acceleration vs. Weight
plot_data2 <- rbind(
  data.frame(
    X = data_original$acceleration,
    Y = data_original$weight,
    missing = "Present"
  ),
  data.frame(
    X = missingMar$acceleration,
    Y = missingMar$weight,
    missing = "Missing"
  )
)

scatter_plot2 <- ggplot(plot_data2, aes(x = X, y = Y, color = missing)) +
  geom_point(size = 2) +  
  scale_color_manual(values = c("Missing" = "black", "Present" = red_nord)) +  
  scale_x_continuous(limits = c(accel_range[1] - accel_pad, accel_range[2] + accel_pad)) +
  scale_y_continuous(limits = c(weight_range[1] - weight_pad, weight_range[2] + weight_pad)) +
  theme_minimal(base_size = 12) +  
  labs(color = "Data Status", x = "Acceleration", y = "Weight") +
  theme(legend.position = "none")

final_plot2 <- ggMarginal(scatter_plot2, 
                          margins = "y", 
                          groupColour = TRUE, 
                          groupFill = TRUE,
                          type = "boxplot", 
                          size = 5,  
                          width = 0.2,  
                          outlier.size = 1)

# Combine the two plots using gridExtra::grid.arrange()
grid.arrange(final_plot1, final_plot2, ncol=1)
```

#### 3D Plot to showcase the relation better
In this section, we employ a 3D scatter plot to elucidate the relationships among 'displacement', 'acceleration', and 'weight' within our dataset. This visualization not only highlights the interplay between these variables but also distinctly marks the data points where 'weight' values are missing.

```{r, fig.width=9, fig.height=9}

# Assuming `data_original` is your original dataset and `missingMar` is the modified one
plotMar_data <- data_original %>%
  mutate(missing = ifelse(is.na(missingMar$weight), "Missing", "Present"))  # Flag missing/present

# Create the 3D plot based on `plotMar_data`

plot_ly(data = plotMar_data, 
        x = ~displacement, 
        y = ~acceleration, 
        z = ~weight,  # Use original weight for z-axis
        color = ~missing, 
        colors = c("Present" = "black", "Missing" = red_nord),  # Ensure 'red' is defined
        type = 'scatter3d', 
        mode = 'markers', 
        marker = list(size = 4)) %>%  # Adjust point size here
  plotly::layout(
    title = '3D Plot of Missing vs Present Data',
    scene = list(
      xaxis = list(title = 'Displacement'),
      yaxis = list(title = 'Acceleration'),
      zaxis = list(title = 'Weight')
    )
  )

```

### Imputation Techniques and Linear Model Fitting

In this section, we address the challenge of missing data within our dataset by applying various imputation methods. The goal is to assess how different imputation strategies influence the performance and outcomes of linear regression models.

## mean, median, listwise deletion, regression, random forest, gam

```{r imp statistic}
imp_ds_mean<-impute_mean(missingMar)
imp_ds_median<-impute_median(missingMar)
imp_del_list<-listwise_deletion(missingMar)
imp_ds_reg <- regression_imputation(missingMar[, -which(names(missingMar) == "car_brand")], noise = TRUE)
imp_ds_rf <- tree_based_imputation(missingMar[, -which(names(missingMar) == "car_brand")], noise = TRUE)

#sub dataset for Gam model
prova <- missingMar[, !names(missingMar) %in% c("car_brand", "origin", "cylinders", "cut_model_year")]
imp_ds_gam <- gam_based_imputation(prova, noise = TRUE, max_predictors = 4)
imp_ds_gam <- cbind(imp_ds_gam, missingMar[, c("car_brand", "origin", "cylinders", "cut_model_year")])

```

Post-imputation, we fit a linear regression model to each imputed dataset. The models predict the logarithm of miles per gallon (log(mpg)) using predictors such as weight, horsepower, and cut_model_year. This approach enables us to evaluate and compare the impact of each imputation method on the regression coefficients and overall model performance.

By systematically applying these imputation techniques and fitting corresponding linear models, we aim to identify the most effective strategies for handling missing data in our analysis.


```{r imp statistic model}
fit_mean<-lm(I(log(mpg))~weight+horsepower + cut_model_year, data = imp_ds_mean)
fit_median<-lm(I(log(mpg))~weight+horsepower + cut_model_year, data = imp_ds_median)
fit_list<-lm(I(log(mpg))~weight+horsepower + cut_model_year, data = imp_del_list)
fit_reg<-lm(I(log(mpg))~weight+horsepower + cut_model_year, data = imp_ds_reg)
fit_rf<-lm(I(log(mpg))~weight+horsepower + cut_model_year, data = imp_ds_rf)
fit_gam<-lm(I(log(mpg))~weight+horsepower + cut_model_year, data = imp_ds_gam)
```

#### Summary and Analysis of coefficients

In this section, we present a comprehensive summary and analysis of the coefficients derived from linear models fitted to datasets subjected to various imputation techniques. The imputation methods employed include mean imputation, median imputation, listwise deletion, regression imputation, random forest imputation, and GAM based imputation. For each imputed dataset, a linear model was fitted with the logarithm of miles per gallon as the response variable and weight, horsepower, and cat_model_year as predictor variables.

We begin by summarizing the fitted models, highlighting key statistics such as coefficient estimates, standard errors, and significance levels. Subsequently, we conduct a detailed coefficient analysis, focusing on the estimates and their confidence intervals for each predictor across the different imputation methods. This analysis is further visualized through plots that compare the coefficients and their confidence intervals, providing insights into the variability and stability of the estimates resulting from the different imputation strategies.

By examining these comparisons, we aim to assess the impact of various imputation methods on the model coefficients and to identify which methods yield the most reliable and consistent estimates.

Summary

```{r imp summary fit}
summary(fit_mean)
summary(fit_median)
summary(fit_list)
summary(fit_reg)
summary(fit_rf)
summary(fit_gam)
```

Coefficient analysis

```{r}
# Extract coefficients and confidence intervals
fit_models <- list(mean = fit_mean, median = fit_median, list = fit_list, reg = fit_reg, rf = fit_rf, gam = fit_gam, or = fit_multi1)

# Create a data frame to store coefficients and their intervals
coef_df <- do.call(rbind, lapply(names(fit_models), function(model_name) {
  coefs <- summary(fit_models[[model_name]])$coefficients
  ci <- confint(fit_models[[model_name]])
  data.frame(
    Model = model_name,
    Term = rownames(coefs),
    Estimate = coefs[, "Estimate"],
    Lower_CI = ci[, 1],
    Upper_CI = ci[, 2]
  )
}))

# View the coefficient data frame
#print(coef_df)

# Plotting
#ggplot(coef_df, aes(x = Term, y = Estimate, color = Model)) +
#  geom_point(position = position_dodge(width = 0.5)) +
#  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, position = position_dodge(width = 0.5)) +
#  labs(title = "Comparison of Coefficients with Confidence Intervals",
#       x = "Terms",
#       y = "Coefficient Estimate") +
#  theme_minimal() +
#  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

##### Intercept
```{r}
# Extract specific coefficients and their confidence intervals
coef_names <-"(Intercept)" # Specify the coefficients you want to compare

# Create a data frame to store selected coefficients and their intervals
selected_coef_df <- do.call(rbind, lapply(names(fit_models), function(model_name) {
  coefs <- summary(fit_models[[model_name]])$coefficients
  ci <- confint(fit_models[[model_name]])
  
  # Filter for selected coefficients
  selected_coefs <- coefs[coef_names, , drop = FALSE]
  selected_ci <- ci[coef_names, , drop = FALSE]
  
  data.frame(
    Model = model_name,
    Term = rownames(selected_coefs),
    Estimate = selected_coefs[, "Estimate"],
    Lower_CI = selected_ci[, 1],
    Upper_CI = selected_ci[, 2]
  )
}))

# View the selected coefficient data frame
print(selected_coef_df)


# Plotting selected coefficients with specified y-axis limits
ggplot(selected_coef_df, aes(x = Term, y = Estimate, color = Model)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of Selected Coefficients with Confidence Intervals",
       x = "Coefficient Terms",
       y = "Coefficient Estimate") +
  scale_y_continuous(limits = c(3.65, 4.05)) +  # Set y-axis limits
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


##### Weight

```{r}
# Extract specific coefficients and their confidence intervals
coef_names <-"weight" # Specify the coefficients you want to compare

# Create a data frame to store selected coefficients and their intervals
selected_coef_df <- do.call(rbind, lapply(names(fit_models), function(model_name) {
  coefs <- summary(fit_models[[model_name]])$coefficients
  ci <- confint(fit_models[[model_name]])
  
  # Filter for selected coefficients
  selected_coefs <- coefs[coef_names, , drop = FALSE]
  selected_ci <- ci[coef_names, , drop = FALSE]
  
  data.frame(
    Model = model_name,
    Term = rownames(selected_coefs),
    Estimate = selected_coefs[, "Estimate"],
    Lower_CI = selected_ci[, 1],
    Upper_CI = selected_ci[, 2]
  )
}))

# View the selected coefficient data frame
print(selected_coef_df)


# Plotting selected coefficients with specified y-axis limits
ggplot(selected_coef_df, aes(x = Term, y = Estimate, color = Model)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of Selected Coefficients with Confidence Intervals",
       x = "Coefficient Terms",
       y = "Coefficient Estimate") +
  #scale_y_continuous(limits = c(3.65, 4.05)) +  # Set y-axis limits
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

##### Horsepower

```{r}
# Extract specific coefficients and their confidence intervals
coef_names <-"horsepower" # Specify the coefficients you want to compare

# Create a data frame to store selected coefficients and their intervals
selected_coef_df <- do.call(rbind, lapply(names(fit_models), function(model_name) {
  coefs <- summary(fit_models[[model_name]])$coefficients
  ci <- confint(fit_models[[model_name]])
  
  # Filter for selected coefficients
  selected_coefs <- coefs[coef_names, , drop = FALSE]
  selected_ci <- ci[coef_names, , drop = FALSE]
  
  data.frame(
    Model = model_name,
    Term = rownames(selected_coefs),
    Estimate = selected_coefs[, "Estimate"],
    Lower_CI = selected_ci[, 1],
    Upper_CI = selected_ci[, 2]
  )
}))

# View the selected coefficient data frame
print(selected_coef_df)


# Plotting selected coefficients with specified y-axis limits
ggplot(selected_coef_df, aes(x = Term, y = Estimate, color = Model)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of Selected Coefficients with Confidence Intervals",
       x = "Coefficient Terms",
       y = "Coefficient Estimate") +
  #scale_y_continuous(limits = c(0.9*Lower_CI, 1.1*Upper_CI)) +  # Set y-axis limits
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

##### Cut_model_year

```{r}
# Extract specific coefficients and their confidence intervals
coef_names <-c("cut_model_year74-76","cut_model_year77-79","cut_model_year80-82") # Specify the coefficients you want to compare

# Create a data frame to store selected coefficients and their intervals
selected_coef_df <- do.call(rbind, lapply(names(fit_models), function(model_name) {
  coefs <- summary(fit_models[[model_name]])$coefficients
  ci <- confint(fit_models[[model_name]])
  
  # Filter for selected coefficients
  selected_coefs <- coefs[coef_names, , drop = FALSE]
  selected_ci <- ci[coef_names, , drop = FALSE]
  
  data.frame(
    Model = model_name,
    Term = rownames(selected_coefs),
    Estimate = selected_coefs[, "Estimate"],
    Lower_CI = selected_ci[, 1],
    Upper_CI = selected_ci[, 2]
  )
}))

# View the selected coefficient data frame
#print(selected_coef_df)


# Plotting selected coefficients with specified y-axis limits
ggplot(selected_coef_df, aes(x = Term, y = Estimate, color = Model)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of Selected Coefficients with Confidence Intervals",
       x = "Coefficient Terms",
       y = "Coefficient Estimate") +
  #scale_y_continuous(limits = c(3.65, 4.05)) +  # Set y-axis limits
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


#### Predictions based on RMSE

```{r}
#Predict on test data
pred_mean <- predict(fit_mean, newdata = test_data)
pred_median <- predict(fit_median, newdata = test_data)
pred_list <- predict(fit_list, newdata = test_data)
pred_reg <- predict(fit_reg, newdata = test_data)
pred_rf <- predict(fit_rf, newdata = test_data)
pred_gam <- predict(fit_gam, newdata = test_data)

pred_origin <- predict(fit_multi1, newdata = test_data)


rmse_value <- RMSE(pred_mean, test_data$mpg)
cat("mean (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_median, test_data$mpg)
cat("median (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_list, test_data$mpg)
cat("list del (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_reg, test_data$mpg)
cat("reg (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_rf, test_data$mpg)
cat("rf (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_gam, test_data$mpg)
cat("gam (RMSE):", rmse_value, "\n")


rmse_value <- RMSE(pred_origin, test_data$mpg)
cat("true (RMSE):", rmse_value, "\n")
```
The analysis revealed that advanced imputation methods, particularly random forest and GAM, yielded superior model performance, as evidenced by more accurate coefficient estimates and narrower confidence intervals. These methods were followed by regression imputation, while simpler approaches like mean imputation and listwise deletion demonstrated comparatively lower performance. This outcome aligns with expectations, as sophisticated imputation techniques are better equipped to capture complex data patterns, leading to more reliable and robust predictive models.



# -------------------------------------------
# TODO
# -------------------------------------------

```{r}
missingMar<-delete_MAR_1_to_x(data_original, p = c(0.2,0.2, 0.2), cols_mis = c("weight", "horsepower", "mpg"), cols_ctrl =c("displacement", "acceleration", "displacement"), x =10)

idx_miss <- which(!complete.cases(missingMar))
df_no_miss <- missingMar[complete.cases(missingMar), ]

tmp<-delete_MAR_1_to_x(df_no_miss, p = c(0.1,0.3), cols_mis = c("displacement", "cut_model_year"), cols_ctrl =c("horsepower", "weight"), x =100)

missingComplex <- missingMar
missingComplex[complete.cases(missingMar), ] <- tmp

# Convert categorical variables to factors before imputation
# missingComplex$cut_model_year <- as.factor(missingComplex$cut_model_year)

#plot VIM
aggr_plot <- aggr(missingComplex, numbers = TRUE,labels=names(missingComplex), prop = c(TRUE, FALSE) )

missing.rows = dim(missingComplex)[1] -  dim(na.omit(missingComplex))[1]
#sprintf("Dim dataset: [%s]", toString(dim(missingMar)))
sprintf("Missing rows: %s (%s%%)", missing.rows, round((missing.rows*100)/dim(missingComplex)[1], 2))

missings_df <- data.frame(type=c("missing", "non-missing") ,count = c(missing.rows,  dim(na.omit(missingComplex))[1]))

ggplot(missings_df, aes(fill=type, y="", x=count)) + 
    geom_bar(position="stack", stat="identity")+
    ggtitle("%missing data") +
    xlab("Obs") + ylab("") +
    theme(text = element_text(size = 18))+
    scale_fill_manual(values = c(nord_colors[2], nord_colors[3]))+
    theme_bw()

```

```{r}
str(missingComplex)
# Method 1: Using dplyr
missingComplex_no_year <- missingComplex %>% select(-cut_model_year)

# Method 2: Base R
missingComplex_no_year <- missingComplex[, -which(names(missingComplex) == "cut_model_year")]

# Method 3: Base R alternative
missingComplex_no_year <- subset(missingComplex, select = -cut_model_year)
```

### Tree imputation

```{r}
str(missingMar)
```

```{r}
complex_ds_mean<-impute_mean(missingComplex)
complex_ds_del<-listwise_deletion(missingComplex)

# Assuming your data frame is named 'df'
no_categorical_dataset <- Filter(function(x) is.numeric(x), missingComplex)

#complex_mice
# Define custom mice imputation function
mice.impute.forest.noise  <- mice_impute_forest 
# Specify the imputation method for each variable

# Run MICE imputation
# After running your MICE imputation
complex_ds_forest <- mice(no_categorical_dataset, method = "forest.noise", m = 5)

# To get the first imputed dataset
imputed_dataset <- complete(complex_ds_forest, 1)
# imputed_dataset_pool <- pool(complex_ds_forest)
```

#### Gam Imputation

```{r}
mice.impute.gam.noise  <- mice_impute_gam
# Specify the imputation method for each variable

# Run MICE imputation
# After running your MICE imputation
complex_ds_gam <- mice(no_categorical_dataset, method = "gam.noise", m = 5)
```

```{r}
fit_complex_mean <-lm(I(log(mpg))~weight+horsepower, data = complex_ds_mean)
fit_complex_del <-lm(I(log(mpg))~weight+horsepower, data = complex_ds_del)
fit_complex_forest <-lm(I(log(mpg))~weight+horsepower, data = imputed_dataset)
```

```{r}
summary(fit_complex_mean)
summary(fit_complex_del)
summary(fit_complex_forest)
```

```{r}
# Extract the imputed datasets
imputed_datasets <- complete(complex_ds_forest, "all")

# Generate predictions for each imputed dataset
predictions_forest_pooled <- lapply(imputed_datasets, function(data) {
  model <- lm(I(log(mpg)) ~ weight + horsepower, data = data)
  predict(model, newdata = test_data)
})

# Combine predictions (e.g., average across imputations)
final_predictions_tree <- Reduce("+", predictions_forest_pooled) / length(predictions_forest_pooled)
```

```{r}
# Extract the imputed datasets
imputed_datasets <- complete(complex_ds_gam, "all")

# Generate predictions for each imputed dataset
predictions_gam_pooled <- lapply(imputed_datasets, function(data) {
  model <- lm(I(log(mpg)) ~ weight + horsepower, data = data)
  predict(model, newdata = test_data)
})

# Combine predictions (e.g., average across imputations)
final_predictions_gam <- Reduce("+", predictions_gam_pooled) / length(predictions_gam_pooled)
```

```{r}
pred_cmean <- predict(fit_complex_mean, newdata = test_data)
pred_cdel <- predict(fit_complex_del, newdata = test_data)
pred_forest <- predict(fit_complex_forest, newdata = test_data)

# Modifing model
fit_original_bad <- lm(I(log(mpg))~weight+horsepower,data = auto_mpg)#ottimo

pred_origin <- predict(fit_original_bad, newdata = test_data)

rmse_value <- RMSE(pred_cmean, test_data$mpg)
cat("Mean mean (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_cdel, test_data$mpg)
cat("Del mean (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(pred_forest, test_data$mpg)
cat("Forest mean (RMSE):", rmse_value, "\n")
rmse_value <- RMSE(final_predictions_tree, test_data$mpg)
cat("Pooled Forest mean (RMSE)", rmse_value, "\n")
rmse_value <- RMSE(final_predictions_gam, test_data$mpg)
cat("Pooled GAM mean (RMSE)", rmse_value, "\n")

rmse_value <- RMSE(pred_origin, test_data$mpg)
cat("true (RMSE):", rmse_value, "\n")
```
Regression

```{r}
reg_mice<-mice(missingMar, method = "norm.predict")

reg_model <- with(reg_mice, lm(I(log(mpg))~weight+horsepower))
pooled_reg <- pool(reg_model)
summary(pooled_reg)

reg_model <- with(reg_mice, lm(I(log(mpg)) ~ weight + horsepower + cut_model_year))

# Extract the imputed datasets
imputed_datasets <- complete(reg_mice, "all")

# Generate predictions for each imputed dataset
predictions_list <- lapply(imputed_datasets, function(data) {
  model <- lm(I(log(mpg)) ~ weight + horsepower, data = data)
  predict(model, newdata = test_data)
})

# Combine predictions (e.g., average across imputations)
final_predictions <- Reduce("+", predictions_list) / length(predictions_list)

rmse_value <- RMSE(final_predictions, test_data$mpg)
cat("mice reg (RMSE):", rmse_value, "\n")
```

## *fare kcross-val*

## Cannone
## plot fit

## guardare dens var ricostruite?
## scatterplot con var ricostruite?
