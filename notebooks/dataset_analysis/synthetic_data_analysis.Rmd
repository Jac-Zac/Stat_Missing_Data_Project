---
title: "Synthetic Data Analysis"
author: "Jacopo Zacchigna"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true 
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
---

# Environment Setup
> Everything is defined inside `setup.R`

```{r, echo = T}
# Load utilities
library(here)

# Load utilities
source(here("src", "setup.R"))
```


```{r, echo = F}
# Display all available palettes
# Show each palette individually
# nord_show_palette("polarnight")
# nord_show_palette("snowstorm")
# nord_show_palette("frost")
# nord_show_palette("aurora")
# Define colors using a Nord palette
# Select Nord palettes
frost_palette <- nord("frost", 4)
aurora_palette <- nord("aurora", 4)

# Choose specific colors
blue_nord <- frost_palette[[length(frost_palette)]]
red_nord <- aurora_palette[[1]]
green_nord <- aurora_palette[[3]]

# Defind a color pallet for red and blue
nord_contrast = c(blue_nord, red_nord)
```

# Data Generation and Exploration

## Synthetic Dataset Creation

We begin by analyzing simpler datasets to explore various imputation methods. Initially, we generate datasets without missing values. 
In the subsequent step, we introduce missing values according to two mechanisms: Missing Completely at Random (MCAR) and Missing at Random (MAR).

Our analysis starts with the creation of a synthetic dataset designed to exhibit specific correlations among covariates. We define a target variable that has a different types of relationships with these covariates.

First Dataset: This simpler dataset assumes a linear relationship among covariates, with the target variable linearly dependent on them.
By structuring our study in this manner, we can systematically assess how different imputation methods perform under varying conditions of data missingness and correlation structures.

```{r, echo=T, class.source = 'fold-show'}
# Generate synthetic dataset with 5 covariates, a linear relationship between the variable and predictor
# And also a linear relationship with the covariates.
synthetic_data <- synthetic_dataset_gen(n_samples = 1000, 
                                        n_covariates = 5, 
                                        correlation = "linear", 
                                        target_type = "linear", 
                                        noise_level = 0.5)

# Summary of the dataset
summary(synthetic_data)
```
The summary is not very informative considering this is just a synthetic dataset. 
We also tried to generate other more complex datasets (with non-linear and polynomials relationships among variables) but we decided to report here only a simpler example, by way of illustration. 

## Data Visualization

This section should be expanded to explore the dataset more in depth.

The first step in our analyses is to visualize the correlation between the continuous covariates in our dataset.

```{r, echo = F}
cor_matrix <- cor(synthetic_data)

# Plot the correlation matrix
corrplot.mixed(cor_matrix, 
               lower = "number", 
               upper = "ellipse",
               addgrid.col = "gray",
               tl.col = "black",
               tl.cex = 0.7,
               lower.col = colorRampPalette(c(blue_nord, "white", red_nord))(100),
               upper.col = colorRampPalette(c(blue_nord, "white", red_nord))(100))
```

Since for other datasets we considered non-linear relationships among variables the correlation matrix should be not informative in these two cases. Indeed traditional correlation coefficients like Pearson's may not adequately capture the strength or nature of variable's associations.

For this reason we can use scatterplots to visually assess relationships between variables. Trhough these, non-linear patterns may become evident, guiding the selection of appropriate analytical methods.

We considered a scatterplot for each pair of variables in the datasets.

```{r, echo = F}
# Scatter plot matrix linear covariates
ggpairs(synthetic_data)
```

We can also plot histograms for each covariate to showcase their distributions.

```{r, echo =F}
# Define Nord color palette
nord_colors <- nord("frost", 4)  # Choose a Nord palette and the number of colors you want

# Plot histogram for each continuous variable
data_continuous <- synthetic_data[, sapply(synthetic_data, is.numeric)]

continuous_melted <- reshape2::melt(data_continuous)

ggplot(continuous_melted, aes(x = value)) +
  geom_histogram(bins = 30, 
                 fill = nord_colors[1],     # Use a color from the Nord palette
                 color = "black",
                 linewidth = 0.2,                 # Adjust this value for thinner borders
                 alpha = 0.7) +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Distribution of Continuous Variables", x = "Value", y = "Frequency")
```

### Regression Model on Original Dataset (Baseline)

In the third step of our analysis on the complete dataset we want to fit regression models that we will later use for baseline comparison.
*We should instead use a subset of the covariate (that is relevant) for this linear model.*

```{r}

# Calculate number of samples for training set
num_samples <- nrow(synthetic_data) * 0.8

# Randomly sample indices for training set
train_index <- sample(seq_len(nrow(synthetic_data)), size = num_samples)

# Split the dataset in training and testing
train_data <- synthetic_data[train_index, ]
test_data <- synthetic_data[-train_index, ]

# Fit baseline model with all of the covariates
baseline_model <- lm(target ~ ., data = train_data)
baseline_predictions <- predict(baseline_model, test_data)
baseline_rmse <- sqrt(mean((baseline_predictions - test_data$target)^2))

summary(baseline_model)
paste("RMSE on the test set: ", baseline_rmse)
```
We can fit also different models, that we expect to perform not better than the linear model when the relations are linear, but better in other cases. 

```{r}
# Generalized Additive Model
# Initially with all of the variables for semplicity
gam_model <- gam(target ~ s(X1) + s(X2) + s(X3) + s(X4)+ s(X5), data = train_data)

# Decision Tree, with all of the variables for semplicity
rpart_model <- rpart(target ~ ., data = train_data)
```

# Missing Data Analysis

This study focuses on Missing at Random (MAR) data due to its practical relevance and analytical tractability compared to Missing Completely at Random (MCAR) and Missing Not at Random (MNAR).

We exclude Missing Completely at Random (MCAR) data because its randomness preserves the dataset’s underlying distribution, making missingness equivalent to reduced sample size without introducing bias. Handling MNAR (Missing Not at Random) data is beyond this study’s scope because identifying and modeling its dependence on unobserved variables is often ambiguous and requires unverifiable assumptions.

In statistical analysis, understanding the patterns of missing data is crucial for selecting appropriate handling methods. The primary patterns are:

**Univariate Missing Data**: Occurs when only a single variable has missing values. This pattern is uncommon across most disciplines and typically arises in experimental studies. 

**Monotone Missing Data**: Characterized by a situation where the missingness of one variable implies the missingness of all subsequent variables. This pattern is often observed in longitudinal studies where participants drop out and do not return. The monotone pattern is generally easier to manage, as the missingness follows a clear, observable sequence. 

**Non-Monotone Missing Data**: Occurs when the missingness of one variable does not predict the missingness of other variables. This pattern is more complex and requires careful analysis to handle appropriately. 

Recognizing these patterns helps in choosing suitable methods for handling missing data, thereby improving the robustness and validity of statistical analyses.
Let's start from our synthetic datasets. Obviously no missing data are present there. Let's check it.

### Missing data in the original dataset
> Obviously, no missing data is present in our dataset; it is synthetically generated.

```{r}
original_missing <- as.data.frame(sapply(train_data, is.na))
original_missing$index <- 1:nrow(original_missing)
original_missing_melted <- melt(original_missing, id.vars = "index")

ggplot(original_missing_melted, aes(x = index, y = variable, fill = value)) +
  geom_tile() +
  scale_fill_manual(values = c("white", red_nord)) +
  labs(title = "Missing Data Pattern - Original", x = "Index", y = "Variable")
```

# Missing Data Theory

Rubin's missing data theory introduces a dataset $Y$ that is partitioned into observed values ($Y_o$) and missing values ($Y_m$). The presence or absence of data is tracked by an indicator matrix $R$ defined for each element of $Y$ as:

$$
R = \begin{cases}
0 & \text{if } Y \text{ is observed} \\
1 & \text{if } Y \text{ is missing}
\end{cases}
$$

Missing Completely at Random (MCAR) defines a mechanism where the probability of missingness $P(R|q)$ has no relationship with any values in the dataset. Implementation can be univariate, where missing values are inserted through random selection or Bernoulli trials with probability $p$, or multivariate, where missing values are distributed either uniformly or randomly across the entire dataset.

Missing At Random (MAR/CAR) occurs when missingness probability $P(R|Y_o, q)$ depends on observed data patterns but not on missing values. For univariate cases, missing values are determined by another observed feature using rank-based probability ($P(x_{i,miss} = \text{missing}) = \frac{r_{i,obs}}{\sum r_{i,obs}}$) or percentile thresholds. In multivariate cases, missingness is generated through related feature pairs or correlation patterns,

## Introducing Missing Data

### Showcase missing data functions

Showcase Introduce MAR function (example to show how to use print_function)

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(introduce_mar)
```

### Introducing missing data with different mechanisms

## MCAR mechanism

We start by delating the 10% of values from the first covariate of our dataset in order to perform univariate deletion. 

```{r}
data.MCAR.uni <- introduce_mcar(train_data, prop_missing = 0.1, missing_cols = "X1")
aggr(data.MCAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(train_data), col = nord_contrast)
```
This is the easiest case to deal with, but not very common in real world.

The following code demonstrates how to simulate missing data under the MCAR assumption in specific columns of a dataset and visualize the resulting missing data patterns.  The resulting data.MCAR.multi dataset is a modified version of the original, with 10% of the entries in columns 'X1', 'X2', and 'X3' replaced with NA values. 

```{r}
data.MCAR.multi <- introduce_mcar(train_data, prop_missing = 0.1, missing_cols =  c('X1','X2','X3'))
aggr(data.MCAR.multi, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(train_data), col = nord_contrast)
```
## MAR mechanism

The following function introduces Missing At Random (MAR) values into the dataset.
We want that 10% of the data in the columns listed in target_cols (X1) should be missing and the missingness in X1 is determined by the values in X2. For example, missingness might be more likely when X2 is below or above a threshold.
This creates a modified dataset (data.MAR.uni) where values in X1 are missing based on the values in X2.


```{r}
# data_MAR.uni <- introduce_mar(train_data, prop_missing = 0.1, predictor_cols = c("X2"), target_cols = c("X1"))

data.MAR.uni <- delete_MAR_censoring(train_data, p = 0.1, cols_mis = c('X1'), cols_ctrl = c('X2'))
aggr(data.MAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(train_data), col = nord_contrast)

# Create a new dataset indicating missing status
intersection_dataset <- train_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")

# Plot the data using the new dataset
ggplot(intersection_dataset, aes(x = X1, y = X2, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X2"
  )
```

In general, the `delete_MAR_1_to_x()` function in R creates **Missing At Random (MAR)** values by controlling missingness in one column (`cols_mis`) based on the values of another column (`cols_ctrl`). It splits the data into two groups using a cutoff value (e.g., median) in `cols_ctrl`. Group 1 contains rows below the cutoff, and group 2 contains rows above. The `x` parameter sets the odds ratio of missing data: for `x = 3`, group 2 is **3 times** more likely to have missing values than group 1.


The delete_MAR_1_to_x Function creates MAR values in multiple columns (cols_mis) based on other columns (cols_ctrl).
In our example we introduced missing values in X1 and X2 and missingness in X1 and X2 is controlled by X3 and X4. For example, missingness may increase when X3 or X4 exceeds a threshold.
x = 10: The odds ratio for missingness between two groups (e.g., above and below the median of cols_ctrl) is 10:1.

```{r}
data.MAR.multi <- delete_MAR_1_to_x(train_data ,p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 10)
# data.MAR.multi <- delete_MAR_1_to_x(train_data , p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 2)
aggr(data.MAR.multi, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(train_data), col = nord_contrast)
```

```{r}
# Plot for X1 vs X3
# Create a new dataset indicating missing status
intersection_dataset <- train_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1) | is.na(data.MAR.uni$X3), "Missing", "Present")

# Plot the data using the new dataset
ggplot(intersection_dataset, aes(x = X1, y = X3, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X3"
  )

# Plot for X2 vs X4
intersection_dataset <- train_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1) | is.na(data.MAR.uni$X4), "Missing", "Present")

ggplot(intersection_dataset, aes(x = X1, y = X4, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X4"
  )
```

The MAR mechanism ensures that missingness in one or more columns is correlated with values in other columns, making the data realistic.
The visualizations using aggr and scatter plots help understand how missing values are distributed and how they relate to control variables (cols_ctrl).

```{r, echo =F}
# Ensuring methods work as expected
sum(is.na(data.MAR.uni)) / nrow(data.MAR.uni)  # Should be approximately 10%
sum(is.na(data.MAR.multi)) / nrow(data.MAR.multi) # Should be approximately  n_vars * 10%

# Analyze missing patterns and summarize
#For MCAR
# print(summarize_missing(data.MCAR.uni))
# print(summary(data.MCAR.uni))
# print(summarize_missing(data.MCAR.multi))
# print(summary(data.MCAR.multi))
#For MAR
# print(summarize_missing(data.MAR.uni))
# print(summary(data.MAR.uni))
# print(summarize_missing(data.MAR.multi))
# print(summary(data.MAR.multi))
```

### Plot missing data for the different mechanisms

#### Custom missing data visualization

We can use the following tools to verify how missing data are distributed in our dataset. These will be very useful in the analysis of our definitive dataset, since it can help us in finding the best strategies to deal with missing values.

```{r, class.source = 'fold-show'}
# Create individual plots
p1 <- plot_missing_data(data.MCAR.uni, "MCAR uni", c("white", red_nord))
p2 <- plot_missing_data(data.MCAR.multi, "MCAR multi", c("white", red_nord))
p3 <- plot_missing_data(data.MAR.uni, "MAR uni", c("white", red_nord))
p4 <- plot_missing_data(data.MAR.multi, "MAR multi", c("white", red_nord))

# Combine plots into one
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

In the final dataset we will create different datasets using these functions and we will analyse them in order to perform the imputation or delation technique that is more suitable for the type of dataset. 