---
title: "Synthetic Data Analysis"
author: "Jacopo Zacchigna"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true 
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
---

# Environment Setup
> Everything is defined inside `setup.R`

```{r, echo = T}
# Load utilities
library(here)

# Load utilities
source(here("src", "setup.R"))
```


```{r, echo = F}
# Display all available palettes
# Show each palette individually
# nord_show_palette("polarnight")
# nord_show_palette("snowstorm")
# nord_show_palette("frost")
# nord_show_palette("aurora")
# Define colors using a Nord palette
# Select Nord palettes
frost_palette <- nord("frost", 4)
aurora_palette <- nord("aurora", 4)

# Choose specific colors
blue_nord <- frost_palette[[length(frost_palette)]]
red_nord <- aurora_palette[[1]]
green_nord <- aurora_palette[[3]]

# Defind a color pallet for red and blue
nord_contrast = c(blue_nord, red_nord)
```


# Synthetic data study

## Missing value patterns

In statistical analysis, understanding the patterns of missing data is crucial for selecting appropriate handling methods. The primary patterns are:

**Univariate Missing Data**: Occurs when only a single variable has missing values. This pattern is uncommon across most disciplines and typically arises in experimental studies. 

**Monotone Missing Data**: Characterized by a situation where the missingness of one variable implies the missingness of all subsequent variables. This pattern is often observed in longitudinal studies where participants drop out and do not return. The monotone pattern is generally easier to manage, as the missingness follows a clear, observable sequence. 

**Non-Monotone Missing Data**: Occurs when the missingness of one variable does not predict the missingness of other variables. This pattern is more complex and requires careful analysis to handle appropriately. 

Recognizing these patterns helps in choosing suitable methods for handling missing data, thereby improving the robustness and validity of statistical analyses.


##  Missing value generation mechanisms

Rubin's missing data theory introduces a dataset $Y$ that is partitioned into observed values ($Y_o$) and missing values ($Y_m$). The presence or absence of data is tracked by an indicator matrix $R$ defined for each element of $Y$ as:

$$
R = \begin{cases}
0 & \text{if } Y \text{ is observed} \\
1 & \text{if } Y \text{ is missing}
\end{cases}
$$

**Missing Completely at Random (MCAR)** defines a mechanism where the probability of missingness $P(R|q)$ has no relationship with any values in the dataset. Implementation can be univariate, where missing values are inserted through random selection or Bernoulli trials with probability $p$, or multivariate, where missing values are distributed either uniformly or randomly across the entire dataset.

**Missing At Random (MAR/CAR)** occurs when missingness probability $P(R|Y_o, q)$ depends on observed data patterns but not on missing values. For univariate cases, missing values are determined by another observed feature using rank-based probability ($P(x_{i,miss} = \text{missing}) = \frac{r_{i,obs}}{\sum r_{i,obs}}$) or percentile thresholds. In multivariate cases, missingness is generated through related feature pairs or correlation patterns.

This study focuses on Missing at Random (MAR) data due to its practical relevance and analytical tractability compared to Missing Completely at Random (MCAR) and Missing Not at Random (MNAR).

We exclude Missing Completely at Random (MCAR) data because its randomness preserves the dataset’s underlying distribution, making missingness equivalent to reduced sample size without introducing bias. Handling MNAR (Missing Not at Random) data is beyond this study’s scope because identifying and modeling its dependence on unobserved variables is often ambiguous and requires unverifiable assumptions.

### Implementation and missing data exploration

Showcase Introduce MAR function (example to show how to use print_function)

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(introduce_mar)
```


### Synthetic Dataset Creation

We begin by analyzing simpler datasets to explore various imputation methods. Initially, we generate datasets without missing values. 
In the subsequent step, we introduce missing values according to two mechanisms: Missing Completely at Random (MCAR) and Missing at Random (MAR).

Our analysis starts with the creation of a synthetic dataset designed to exhibit specific correlations among covariates. We define a target variable that has a different types of relationships with these covariates.

First Dataset: This simpler dataset assumes a linear relationship among covariates, with the target variable linearly dependent on them.
By structuring our study in this manner, we can systematically assess how different imputation methods perform under varying conditions of data missingness and correlation structures.

```{r, echo=T, class.source = 'fold-show'}
# Generate synthetic dataset with 5 covariates, a linear relationship between the variable and predictor
# And also a linear relationship with the covariates.
synthetic_data <- synthetic_dataset_gen(n_samples = 1000, 
                                        n_covariates = 5, 
                                        correlation = "linear", 
                                        target_type = "linear", 
                                        noise_level = 0.5)

# Summary of the dataset
summary(synthetic_data)
```
The summary is not very informative considering this is just a synthetic dataset. 
We also tried to generate other more complex datasets (with non-linear and polynomials relationships among variables) but we decided to report here only a simpler example, by way of illustration. 


### MCAR mechanism

We start by deleting the 10% of values from the first covariate of our dataset in order to perform univariate deletion. 

```{r}
data.MCAR.uni <- introduce_mcar(synthetic_data, prop_missing = 0.1, missing_cols = "X1")
aggr(data.MCAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)
```
This is the easiest case to deal with, but not very common in real world.

The following code demonstrates how to simulate missing data under the MCAR assumption in specific columns of a dataset and visualize the resulting missing data patterns.  The resulting data.MCAR.multi dataset is a modified version of the original, with 10% of the entries in columns 'X1', 'X2', and 'X3' replaced with NA values. 

```{r}
data.MCAR.multi <- introduce_mcar(synthetic_data, prop_missing = 0.1, missing_cols =  c('X1','X2','X3'))
aggr(data.MCAR.multi, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)
```

### MAR mechanism

The following function introduces Missing At Random (MAR) values into the dataset.
We want that 10% of the data in the columns listed in target_cols (X1) should be missing and the missingness in X1 is determined by the values in X2. For example, missingness might be more likely when X2 is below or above a threshold.
This creates a modified dataset (data.MAR.uni) where values in X1 are missing based on the values in X2.


```{r}
# data_MAR.uni <- introduce_mar(synthetic_data, prop_missing = 0.1, predictor_cols = c("X2"), target_cols = c("X1"))

data.MAR.uni <- delete_MAR_censoring(synthetic_data, p = 0.1, cols_mis = c('X1'), cols_ctrl = c('X2'))
aggr(data.MAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)

# Create a new dataset indicating missing status
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")

# Plot the data using the new dataset
ggplot(intersection_dataset, aes(x = X1, y = X2, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X2"
  )
```

In general, the `delete_MAR_1_to_x()` function in R creates **Missing At Random (MAR)** values by controlling missingness in one column (`cols_mis`) based on the values of another column (`cols_ctrl`). It splits the data into two groups using a cutoff value (e.g., median) in `cols_ctrl`. Group 1 contains rows below the cutoff, and group 2 contains rows above. The `x` parameter sets the odds ratio of missing data: for `x = 3`, group 2 is **3 times** more likely to have missing values than group 1.


The delete_MAR_1_to_x Function creates MAR values in multiple columns (cols_mis) based on other columns (cols_ctrl).
In our example we introduced missing values in X1 and X2 and missingness in X1 and X2 is controlled by X3 and X4. For example, missingness may increase when X3 or X4 exceeds a threshold.
x = 10: The odds ratio for missingness between two groups (e.g., above and below the median of cols_ctrl) is 10:1.

```{r}
data.MAR.multi <- delete_MAR_1_to_x(synthetic_data ,p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 10)
# data.MAR.multi <- delete_MAR_1_to_x(synthetic_data , p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 2)
aggr(data.MAR.multi, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)
```

```{r}
# Plot for X1 vs X3
# Create a new dataset indicating missing status
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1) | is.na(data.MAR.uni$X3), "Missing", "Present")

# Plot the data using the new dataset
ggplot(intersection_dataset, aes(x = X1, y = X3, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X3"
  )

# Plot for X2 vs X4
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1) | is.na(data.MAR.uni$X4), "Missing", "Present")

ggplot(intersection_dataset, aes(x = X1, y = X4, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X4"
  )
```

The MAR mechanism ensures that missingness in one or more columns is correlated with values in other columns, making the data realistic.
The visualizations using aggr and scatter plots help understand how missing values are distributed and how they relate to control variables (cols_ctrl).

```{r, echo =F}
# Ensuring methods work as expected
sum(is.na(data.MAR.uni)) / nrow(data.MAR.uni)  # Should be approximately 10%
sum(is.na(data.MAR.multi)) / nrow(data.MAR.multi) # Should be approximately  n_vars * 10%

# Analyze missing patterns and summarize
#For MCAR
# print(summarize_missing(data.MCAR.uni))
# print(summary(data.MCAR.uni))
# print(summarize_missing(data.MCAR.multi))
# print(summary(data.MCAR.multi))
#For MAR
# print(summarize_missing(data.MAR.uni))
# print(summary(data.MAR.uni))
# print(summarize_missing(data.MAR.multi))
# print(summary(data.MAR.multi))
```

### Plot missing data for the different mechanisms

#### Custom missing data visualization

We can use the following tools to verify how missing data are distributed in our dataset. These will be very useful in the analysis of our definitive dataset, since it can help us in finding the best strategies to deal with missing values.

```{r, class.source = 'fold-show'}
# Create individual plots
p1 <- plot_missing_data(data.MCAR.uni, "MCAR uni", c("white", red_nord))
p2 <- plot_missing_data(data.MCAR.multi, "MCAR multi", c("white", red_nord))
p3 <- plot_missing_data(data.MAR.uni, "MAR uni", c("white", red_nord))
p4 <- plot_missing_data(data.MAR.multi, "MAR multi", c("white", red_nord))

# Combine plots into one
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

In the final dataset we will create different datasets using these functions and we will analyse them in order to perform the imputation or delation technique that is more suitable for the type of dataset. 

## Imputation techiques

This section reviews imputation techniques for handling missing data, from simple deletion methods to advanced machine learning approaches, highlighting their uses and impact on data reliability.

| **Imputation Technique**         | **Description**                                                                                                                                                                                                                                                                                                                                                                           | **Strengths**                                                                                                                                                                     | **Limitations**                                                                                                                                                                                   |
|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **List-wise (Case) Deletion**     | Removes any case with missing values from analysis.                                                                                                                                                                                                                                                                                                                                       | Simple and default in many tools; effective for large data if missingness is completely at random (MCAR).                                                                         | Can introduce bias and cause significant data loss if MCAR assumption is violated.                                                                                                                |
| **Pairwise Deletion**             | Excludes only the necessary data points when assessing missing values.                                                                                                                                                                                                                                                                                                                     | Retains more data compared to list-wise deletion; less bias for MCAR or MAR data.                                                                                                 | May produce inconsistent sample sizes and biased estimates if assumptions are unmet.                                                                                                               |
| **Simple Imputation**             | Replaces missing values with mean, median, or mode of observed data.                                                                                                                                                                                                                                                                                                                       | Easy to use and computationally efficient.                                                                                                                                        | Can distort data variability and introduce bias, especially in complex or large datasets.                                                                                                         |
| **Regression Imputation**         | Predicts missing values using a regression model built on complete data.                                                                                                                                                                                                                                                                                                                   | Preserves sample size and captures linear relationships; suitable for MAR data.                                                                                                  | Assumes MAR and linearity; less effective for non-linear patterns or complex dependencies.                                                                                                         |
| **Hot-deck Imputation**           | Fills missing values using a donor case with similar observed values.                                                                                                                                                                                                                                                                                                                      | Maintains data structure and avoids model assumptions.                                                                                                                            | Lacks strong theoretical support; less reliable for small datasets or complex matching.                                                                                                           |
| **Expectation-Maximization (EM)** | Iteratively estimates missing values by alternating between expectation and maximization steps to optimize data likelihood.                                                                                                                                                                                                                                                                | Produces accurate imputations by modeling uncertainty; handles MAR data well.                                                                                                    | Computationally intensive with complex matrix calculations; requires convergence.                                                                                                                  |
| **Multiple Imputation**           | Generates multiple complete datasets to estimate missing values, reflecting uncertainty. Results are combined into a single estimate.                                                                                                                                                                                                                                                       | Reduces bias; reflects uncertainty; provides more robust and reliable results.                                                                                                   | Time-consuming; requires careful model selection and combining results.                                                                                                                            |
| **GAM Imputation**                | Uses Generalized Additive Models to impute data, allowing non-linear relationships between variables.                                                                                                                                                                                                                                                                                       | Flexible and effective for complex, non-linear relationships.                                                                                                                     | More complex modeling and computational cost compared to simpler methods.                                                                                                                          |
| **Decision Tree / Random Forest** | Builds predictive models for missing data with decision tree algorithms.                                                                                                                                                                                                                                                                                                                    | Captures complex patterns; handles both numerical and categorical data.                                                                                                           | Computationally intensive for large datasets; prone to overfitting without proper tuning.                                                                                                         |
| **K-Nearest Neighbors (KNN)**     | Imputes missing values by averaging or taking the mode of "k" nearest neighbors based on a distance metric.                                                                                                                                                                                                                                                                                 | Captures intricate patterns and relationships; flexible for different types of data.                                                                                              | High computational cost for large datasets due to distance calculations.                                                                                                                           |



## Performance metrics between Datasets
> Comparing the different datasets obtained via inputation

### The Kantorovich Problem

The Kantorovich problem calculates the distance between two probability measures.

**Definition:**
let \((X, \mathcal{X})\) and \((Y, \mathcal{Y})\) be two Polish spaces, that is, complete and separable metric spaces. 
Consider two probability measures \(P\) and \(Q\) belonging to the set of probability measures on \(X\) and \(Y\), 
respectively denoted by \(\mathcal{P}(X)\) and \(\mathcal{P}(Y)\). 

We define the set

$$
\Pi(P,Q) := \bigl\{\pi \in \mathcal{P}(X \times Y) : 
\pi(X \times \cdot) = P(\cdot),\ \pi(\cdot \times Y) = Q(\cdot)\bigr\},
$$

which is the class of probability measures on \((X \times Y,\, \mathcal{X} \times \mathcal{Y})\) 
whose marginals are \(P\) and \(Q\). The elements \(\pi\) of this class are called transport plans.

All elements of \(\Pi\) are probability measures that can be interpreted as ways to transfer the "mass" from \(P\) to \(Q\). 

However, moving this "mass" has a cost, defined by the cost function

$$
c(\cdot,\cdot) : X \times Y \to \mathbb{R}.
$$

Then, once a transport plan \(\pi \in \Pi\) is fixed, we define

$$
I_c(\pi) := \int_{X \times Y} c(x,y)\,\pi(dx\,dy),
$$

which represents the average cost of transferring \(P\) to \(Q\), associated with that particular transport plan. 

We can therefore formulate the Kantorovich problem as follows:

$$
K_c(P,Q) := \inf_{\pi \in \Pi} \int_{X \times Y} c(x,y)\,\pi(dx\,dy).
$$

Having defined the Kantorovich problem, we now assume that \(X = Y\) and let \(d\) be a distance on \(X\).  
We can then define the Wasserstein distance of order \(p\), denoted \(W_p\), as follows:

$$
W_p(P,Q) 
:= K_{d^p}(P,Q)^{1/p} 
= \biggl(\inf_{\pi \in \Pi(P,Q)} \int_{X^2} d^p(x,y)\,\pi(dx\,dy)\biggr)^{\!\!1/p}.
$$

The difference between the Kantorovich definition and the Wasserstein definition lies in the cost function, 
which is given by \(d(x,y)^p\), where \(d(\cdot,\cdot)\) is a distance on \(X\). It is precisely this definition 
of the cost function that guarantees, via the following proposition, that \(W_p\) is indeed a distance.

**Proposition**  
Assume \(X = Y\) and that for \(p > 1\), \(c(x,y) = d(x,y)^p\), where \(d\) is a distance on \(X\), i.e.:

1. \(d(x,y) = d(y,x) \geq 0\);
2. \(d(x,y) = 0\) if and only if \(x = y\);
3. \(\forall (x,y,z) \in X^3,\ d(x,z) \le d(x,y) + d(y,z)\).

Then \(W_p\) is a distance, namely it is symmetric, positive, satisfies \(W_p(P,Q) = 0\) 
if and only if \(P = Q\), and fulfills the triangle inequality:

$$
\forall (P,Q,Z) \in \mathcal{P}(X)^3 \quad 
W_p(P,Z) \le W_p(P,Q) + W_p(Q,Z).
$$

#### The Discrete Case

The Kantorovich problem can also be expressed in discrete form.

**Definition:** suppose \(X = \{ x_1, \dots, x_n \}\) and \(Y = \{ y_1, \dots, y_m \}\). 
In this case, \(P\) and \(Q\) can be identified with two probability vectors 
\(a \in \Sigma_n\) and \(b \in \Sigma_m\), where

$$
\Sigma_n 
= \bigl\{ a \in \mathbb{R}_+^n : \sum_{i=1}^n a_i = 1 \bigr\}.
$$

These probability vectors can be thought of as histograms, in which 
each \(a_i\) represents the probability associated with \(x_i\), and each \(b_j\) 
represents the probability associated with \(y_j\). 

We can define the discrete analogue of the set of probability measures 
\(\Pi(P,Q)\) as follows:

$$
U(a, b) := \bigl\{\, P \in \mathbb{R}_+^{n \times m} : 
P\,\mathbf{1}_m = a,\; P^\top \mathbf{1}_n = b \bigr\},
$$

where \(P\) is the transport matrix, the discrete analogue of 
the transport plan \(\pi\). In other words, 
\(P_{i,j}\) tells us how much mass to move from the point \(x_i\) to the point \(y_j\).

From bin \(i\) of \(a\) to bin \(j\) of \(b\). The constraints related to the set \(U(a,b)\) 
are similar to those of \(\Pi\), meaning that the sum of the rows or columns 
returns \(a\) or \(b\), so that \(P\,\mathbf{1}_m = a = \sum_{j=1}^m P_{i,j}\) 
for each \(i \in \{1,\dots,n\}\) and \(P^\top \mathbf{1}_n = \sum_{i=1}^n P_{i,j} = b_j\) 
for each \(j \in \{1,\dots,m\}\).

The cost function \(\mathbf{c}\) is represented by a matrix \(\mathbf{C} \in \mathbb{R}^{n \times m}\), 
where each \(C_{i,j} = c(x_i, y_j)\) explains the cost of going from \(a_i\) to \(b_j\). 
The average cost analogous to \(I_c\) in the discrete case is defined as

$$
\bar{I}_c 
= \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

We can then formulate the discrete Kantorovich problem as follows:

$$
L_C(a,b) 
:= \min_{P \in U(a,b)} \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

As in the continuous case, assuming \(X, Y \subset E\), with \(E\) a metric space 
equipped with a distance \(d\), we can also define the Wasserstein distance 
of order \(p\) in the discrete setting:

$$
W_p(a,b) 
:= \min_{P \in U(a,b)} 
\biggl(\sum_{i,j} D_{i,j}^p \, P_{i,j}\biggr)^{\!\!1/p}.
$$

where \(D_{i,j}^p = d^p(x_i, y_j)\) is the matrix of distances between the points \(x_i\) and \(y_j\).  
If \(X,Y \subset \mathbb{R}^n\), a standard example of distance between two points in the discrete case 
is \(D_{i,j} = \|x_i - y_j\|\), where \(\|\cdot\|\) is the Euclidean norm.

In the Wasserstein case, we consider the cost matrix \(\mathbf{C} = \mathbf{D}\), 
with \(\mathbf{D}\) being the matrix of Euclidean distances. 

### Jensen–Shannon Divergence

The Jensen–Shannon divergence (JSD) is a measure of similarity (or dissimilarity) between two probability distributions. 
**Definition:** let \(P\) and \(Q\) be two discrete probability distributions over the same domain \(\Omega\). Define

\[
M = \tfrac{1}{2}\,(P + Q),
\]

which is the average (or midpoint) distribution of \(P\) and \(Q\). Then the Jensen–Shannon divergence between \(P\) and \(Q\) is given by

\[
\mathrm{JSD}(P \,\|\, Q) 
= \tfrac{1}{2}\,\mathrm{KL}(P \,\|\, M)
+ \tfrac{1}{2}\,\mathrm{KL}(Q \,\|\, M),
\]

where \(\mathrm{KL}(\cdot \,\|\, \cdot)\) denotes the KL divergence, defined by:

\[
\mathrm{KL}(P \,\|\, Q) 
= \sum_{x \in \Omega} P(x) \,\log\!\Bigl(\tfrac{P(x)}{Q(x)}\Bigr).
\]

**Properties:**

1. **Symmetry**  
   \[
   \mathrm{JSD}(P \,\|\, Q) = \mathrm{JSD}(Q \,\|\, P).
   \]
   This follows because \(M\) is symmetric in \(P\) and \(Q\).

2. **Non-negativity**  
   \[
   \mathrm{JSD}(P \,\|\, Q) \ge 0,
   \]
   with equality if and only if \(P = Q\).

3. **Boundedness**  
   \[
   0 \,\le\, \mathrm{JSD}(P \,\|\, Q) \,\le\, \log(2).
   \]
   The maximum value \(\log(2)\) occurs when \(P\) and \(Q\) have disjoint supports.

4. **Metric Property**  
   Taking the square root gives a metric:
   \[
   d_{\mathrm{JSD}}(P, Q) := \sqrt{\mathrm{JSD}(P \,\|\, Q)}.
   \]
   This metric satisfies the triangle inequality.


## Visualization

The following paragraph presents visualizations of various imputation techniques applied to different types of datasets to observe their impact. The reconstructed datasets are then evaluated and compared using two distribution-based metrics.

```{r,  class.source = 'fold-show'}
# Define methods with their display names and corresponding functions
imputation_methods <- list(
  "Mean Imputation" = function(data) { simple_imputation(data, "mean") },
  # Return only necessary collums
  "KNN Imputation" = function(data) {
  # Perform the kNN imputation
  imputed_data <- kNN(data, variable = "x2")
  
  # Remove the column with the '_imp' suffix (e.g., x2_imp)
  imputed_data <- imputed_data[, !grepl("_imp$", colnames(imputed_data))]
  
  # Return the imputed dataset without the '_imp' column
  return(imputed_data)
  },
  "Regression" = function(data) { regression_imputation(data) },
  "Regression with Noise" = function(data) { regression_imputation(data, noise = TRUE) },
  "Hot Deck" = function(data) { hot_deck_imputation(data) },
  "EM" = function(data) { impute_EM(data) },
  "GAM" = function(data) { gam_based_imputation(data) },
  "GAM with Noise" = function(data) { gam_based_imputation(data, noise = TRUE) },
  "Random Forest" = function(data) { tree_based_imputation(data) },
  "Random Forest with Noise" = function(data) { tree_based_imputation(data, noise=TRUE) }
)
```


### Linear Relationship

We begin by testing the imputation techniques on data characterized by a linear relationship and heteroscedasticity. Methods such as regression imputation are expected to perform well in this scenario, as they are designed to capture and model linear patterns within the data.

```{r, fig.width=18, fig.height=14}
# Generate and create missing data
n <- 200
p <- 0.3
linear_data <- generate_data(n, "linear",x_range = c(0, 1),  noise_sd = 0.5, 
                             homoscedasticity = F, min_sd_noise = 0.1)
linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

# Use the new combined analysis function
linear_result <- plot_imputations_and_metrics(original_data = linear_data, missing_data = linear_missing, imputation_methods = imputation_methods)

# Access the list of individual plots
imputation_plots <- linear_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r}
# Access the metrics plot
metrics_plot <- linear_result$metrics_plot
print(metrics_plot)
```



### Quadratic Relationship

The second dataset is constructed with a quadratic relationship and an unbalanced number of data points. In cases involving quadratic relationships, we anticipate that more flexible imputation methods, such as Generalized Additive Models (GAM) and Random Forest, will outperform simple linear regression due to their ability to capture complex, non-linear patterns in the data.

```{r, fig.width=18, fig.height=14}
# Generate and create missing data
quad_data <- generate_data(n, "quadratic",x_range = c(-2, 2), noise_sd = 1, balanced = F, alpha = 1, beta = 2)
quad_missing <- delete_MAR_1_to_x(quad_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

quad_result <- plot_imputations_and_metrics(quad_data, quad_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- quad_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r}
# Access the metrics plot
metrics_plot <- quad_result$metrics_plot
print(metrics_plot)
```

### Piecewise

The third dataset is designed with a piecewise relationship, where distinct segments of the data follow different patterns. For this type of distribution, we expect imputation methods capable of handling discontinuities and local variations, such as Random Forest and k-Nearest Neighbors, to perform better than global modeling approaches like linear regression, which may struggle to accurately capture the segmented structure of the data.

```{r, fig.width=18, fig.height=14}
# Generate and create missing data
piecewise_data <- generate_data(n, "piecewise", noise_sd = 2)
piecewise_missing <- delete_MAR_1_to_x(piecewise_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

piecewise_result <- plot_imputations_and_metrics(piecewise_data, piecewise_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- piecewise_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r}
# Access the metrics plot
metrics_plot <- piecewise_result$metrics_plot
print(metrics_plot)
```

### Logarithmic Relationship

The final dataset follows a logarithmic relationship, providing insight into how different imputation methods handle non-linear but monotonic patterns.

```{r, fig.width=18, fig.height=14}
# Generate and create missing data
log_data <- generate_data(n, "log", x_range = c(1, 100), noise_sd = 1)
log_missing <- delete_MAR_1_to_x(log_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

log_result <- plot_imputations_and_metrics(log_data, log_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- log_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r}
# Access the metrics plot
metrics_plot <- log_result$metrics_plot
print(metrics_plot)
```

### Conclusions

The visualizations demonstrate that:

1. Simple mean imputation tends to perform poorly as it ignores the relationship between variables
2. kNN and hot deck methods can work well when there are similar cases nearby
3. Regression imputation works best for linear relationships
4. GAM and Random Forest methods are more flexible and can handle non-linear relationships better
5. The choice of imputation method should depend on the underlying relationship between variables
