---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
    wrap: 72
---

# Missing data study

### Environment Setup
> Everything is defined inside `setup.R`

```{r, echo = T}
# Load utilities
suppressMessages(library(here))

# Load utilities
source(here("src", "setup.R"))
```

## Missing value patterns

In statistical analysis, understanding the patterns of missing data is crucial for selecting appropriate handling methods. The primary patterns are:

**Univariate Missing Data**: Occurs when only a single variable has missing values. This pattern is uncommon across most disciplines and typically arises in experimental studies. 

**Monotone Missing Data**: Characterized by a situation where the missingness of one variable implies the missingness of all subsequent variables. This pattern is often observed in longitudinal studies where participants drop out and do not return. The monotone pattern is generally easier to manage, as the missingness follows a clear, observable sequence. 

**Non-Monotone Missing Data**: Occurs when the missingness of one variable does not predict the missingness of other variables. This pattern is more complex and requires careful analysis to handle appropriately. 

Recognizing these patterns helps in choosing suitable methods for handling missing data, thereby improving the robustness and validity of statistical analyses.


##  Missing value generation mechanisms

Rubin's missing data theory introduces a dataset $Y$ that is partitioned into observed values ($Y_o$) and missing values ($Y_m$). The presence or absence of data is tracked by an indicator matrix $R$ defined for each element of $Y$ as:

$$
R = \begin{cases}
0 & \text{if } Y \text{ is observed} \\
1 & \text{if } Y \text{ is missing}
\end{cases}
$$

**Missing Completely at Random (MCAR)** defines a mechanism where the probability of missingness $P(R|q)$ has no relationship with any values in the dataset. Implementation can be univariate, where missing values are inserted through random selection or Bernoulli trials with probability $p$, or multivariate, where missing values are distributed either uniformly or randomly across the entire dataset.

**Missing At Random (MAR/CAR)** occurs when missingness probability $P(R|Y_o, q)$ depends on observed data patterns but not on missing values. For univariate cases, missing values are determined by another observed feature using rank-based probability ($P(x_{i,miss} = \text{missing}) = \frac{r_{i,obs}}{\sum r_{i,obs}}$) or percentile thresholds. In multivariate cases, missingness is generated through related feature pairs or correlation patterns.

This study focuses on Missing at Random (MAR) data due to its practical relevance and analytical tractability compared to Missing Completely at Random (MCAR) and Missing Not at Random (MNAR).

We exclude Missing Completely at Random (MCAR) data because its randomness preserves the dataset’s underlying distribution, making missingness equivalent to reduced sample size without introducing bias. Handling MNAR (Missing Not at Random) data is beyond this study’s scope because identifying and modeling its dependence on unobserved variables is often ambiguous and requires unverifiable assumptions.

### Implementation and missing data exploration

Showcase Introduce MAR function (example to show how to use print_function)

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(introduce_mar)
```


### Synthetic Dataset Creation

We begin by analyzing simpler datasets to explore various imputation methods. Initially, we generate datasets without missing values. 
In the subsequent step, we introduce missing values according to two mechanisms: Missing Completely at Random (MCAR) and Missing at Random (MAR).

Our analysis starts with the creation of a synthetic dataset designed to exhibit specific correlations among covariates. We define a target variable that has a different types of relationships with these covariates.

First Dataset: This simpler dataset assumes a linear relationship among covariates, with the target variable linearly dependent on them.
By structuring our study in this manner, we can systematically assess how different imputation methods perform under varying conditions of data missingness and correlation structures.

```{r, echo=T}
# Generate synthetic dataset with 5 covariates, a linear relationship between the variable and predictor
# And also a linear relationship with the covariates.
synthetic_data <- synthetic_dataset_gen(n_samples = 1000, 
                                        n_covariates = 5, 
                                        correlation = "linear", 
                                        target_type = "linear", 
                                        noise_level = 0.5)

# Summary of the dataset
summary(synthetic_data)
```
The summary is not very informative considering this is just a synthetic dataset. 
We also tried to generate other more complex datasets (with non-linear and polynomials relationships among variables) but we decided to report here only a simpler example, by way of illustration. 

### MCAR mechanism

We start by deleting the 10% of values from the first covariate of our dataset in order to perform univariate deletion. 

```{r}
data.MCAR.uni <- introduce_mcar(synthetic_data, prop_missing = 0.1, missing_cols = "X1")
aggr(data.MCAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)
```
This is the easiest case to deal with, but not very common in real world.

The following code demonstrates how to simulate missing data under the MCAR assumption in specific columns of a dataset and visualize the resulting missing data patterns.  The resulting data.MCAR.multi dataset is a modified version of the original, with 10% of the entries in columns 'X1', 'X2', and 'X3' replaced with NA values. 

```{r}
data.MCAR.multi <- introduce_mcar(synthetic_data, prop_missing = 0.1, missing_cols =  c('X1','X2','X3'))
aggr(data.MCAR.multi, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)
```

### MAR mechanism

The following function introduces Missing At Random (MAR) values into the dataset.
We want that 10% of the data in the columns listed in target_cols (X1) should be missing and the missingness in X1 is determined by the values in X2. For example, missingness might be more likely when X2 is below or above a threshold.
This creates a modified dataset (data.MAR.uni) where values in X1 are missing based on the values in X2.


```{r}
# data_MAR.uni <- introduce_mar(synthetic_data, prop_missing = 0.1, predictor_cols = c("X2"), target_cols = c("X1"))

data.MAR.uni <- delete_MAR_censoring(synthetic_data, p = 0.1, cols_mis = c('X1'), cols_ctrl = c('X2'))
aggr(data.MAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)

# Create a new dataset indicating missing status
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")

# Plot the data using the new dataset
ggplot(intersection_dataset, aes(x = X1, y = X2, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X2"
  )
```

In general, the `delete_MAR_1_to_x()` function in R creates **Missing At Random (MAR)** values by controlling missingness in one column (`cols_mis`) based on the values of another column (`cols_ctrl`). It splits the data into two groups using a cutoff value (e.g., median) in `cols_ctrl`. Group 1 contains rows below the cutoff, and group 2 contains rows above. The `x` parameter sets the odds ratio of missing data: for `x = 3`, group 2 is **3 times** more likely to have missing values than group 1.


The delete_MAR_1_to_x Function creates MAR values in multiple columns (cols_mis) based on other columns (cols_ctrl).
In our example we introduced missing values in X1 and X2 and missingness in X1 and X2 is controlled by X3 and X4. For example, missingness may increase when X3 or X4 exceeds a threshold.
x = 10: The odds ratio for missingness between two groups (e.g., above and below the median of cols_ctrl) is 10:1.

```{r}
data.MAR.multi <- delete_MAR_1_to_x(synthetic_data ,p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 10)
# data.MAR.multi <- delete_MAR_1_to_x(synthetic_data , p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 2)
aggr(data.MAR.multi, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)
```

```{r}
# Plot for X1 vs X3
# Create a new dataset indicating missing status
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1) | is.na(data.MAR.uni$X3), "Missing", "Present")

# Plot the data using the new dataset
ggplot(intersection_dataset, aes(x = X1, y = X3, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X3"
  )

# Plot for X2 vs X4
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1) | is.na(data.MAR.uni$X4), "Missing", "Present")

ggplot(intersection_dataset, aes(x = X1, y = X4, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  theme_minimal() +
  labs(
    color = "Data Status",
    title = "Highlighting Missing Data in a New Dataset",
    x = "X1",
    y = "X4"
  )
```

The MAR mechanism ensures that missingness in one or more columns is correlated with values in other columns, making the data realistic.
The visualizations using aggr and scatter plots help understand how missing values are distributed and how they relate to control variables (cols_ctrl).

```{r, echo =F}
# Ensuring methods work as expected
sum(is.na(data.MAR.uni)) / nrow(data.MAR.uni)  # Should be approximately 10%
sum(is.na(data.MAR.multi)) / nrow(data.MAR.multi) # Should be approximately  n_vars * 10%

# Analyze missing patterns and summarize
#For MCAR
# print(summarize_missing(data.MCAR.uni))
# print(summary(data.MCAR.uni))
# print(summarize_missing(data.MCAR.multi))
# print(summary(data.MCAR.multi))
#For MAR
# print(summarize_missing(data.MAR.uni))
# print(summary(data.MAR.uni))
# print(summarize_missing(data.MAR.multi))
# print(summary(data.MAR.multi))
```

### Plot missing data for the different mechanisms

#### Custom missing data visualization

We can use the following tools to verify how missing data are distributed in our dataset. These will be very useful in the analysis of our definitive dataset, since it can help us in finding the best strategies to deal with missing values.

```{r, class.source = 'fold-show'}
# Create individual plots
p1 <- plot_missing_data(data.MCAR.uni, "MCAR uni", c("white", red_nord))
p2 <- plot_missing_data(data.MCAR.multi, "MCAR multi", c("white", red_nord))
p3 <- plot_missing_data(data.MAR.uni, "MAR uni", c("white", red_nord))
p4 <- plot_missing_data(data.MAR.multi, "MAR multi", c("white", red_nord))

# Combine plots into one
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

In the final dataset we will create different datasets using these functions and we will analyse them in order to perform the imputation or delation technique that is more suitable for the type of dataset. 

## Performance metrics between Datasets
> Since we possess the **original dataset prior to the introduction of missing values**, we will introduce _two metrics_ to compare _the distributions of two datasets_: the **original dataset** without missing values and the dataset in which missing values have been **imputed** using various techniques

We indeed care about comparing the distribution of the two datasets more than comparing pointwise the actual reconstruction of the dataset.

### The Kantorovich Problem

The Kantorovich problem calculates the distance between two probability measures.

**Definition:**
let \((X, \mathcal{X})\) and \((Y, \mathcal{Y})\) be two Polish spaces, that is, complete and separable metric spaces. 
Consider two probability measures \(P\) and \(Q\) belonging to the set of probability measures on \(X\) and \(Y\), 
respectively denoted by \(\mathcal{P}(X)\) and \(\mathcal{P}(Y)\). 

We define the set

$$
\Pi(P,Q) := \bigl\{\pi \in \mathcal{P}(X \times Y) : 
\pi(X \times \cdot) = P(\cdot),\ \pi(\cdot \times Y) = Q(\cdot)\bigr\},
$$

which is the class of probability measures on \((X \times Y,\, \mathcal{X} \times \mathcal{Y})\) 
whose marginals are \(P\) and \(Q\). The elements \(\pi\) of this class are called transport plans.

All elements of \(\Pi\) are probability measures that can be interpreted as ways to transfer the "mass" from \(P\) to \(Q\). 

However, moving this "mass" has a cost, defined by the cost function

$$
c(\cdot,\cdot) : X \times Y \to \mathbb{R}.
$$

Then, once a transport plan \(\pi \in \Pi\) is fixed, we define

$$
I_c(\pi) := \int_{X \times Y} c(x,y)\,\pi(dx\,dy),
$$

which represents the average cost of transferring \(P\) to \(Q\), associated with that particular transport plan. 

We can therefore formulate the Kantorovich problem as follows:

$$
K_c(P,Q) := \inf_{\pi \in \Pi} \int_{X \times Y} c(x,y)\,\pi(dx\,dy).
$$

Having defined the Kantorovich problem, we now assume that \(X = Y\) and let \(d\) be a distance on \(X\).  
We can then define the Wasserstein distance of order \(p\), denoted \(W_p\), as follows:

$$
W_p(P,Q) 
:= K_{d^p}(P,Q)^{1/p} 
= \biggl(\inf_{\pi \in \Pi(P,Q)} \int_{X^2} d^p(x,y)\,\pi(dx\,dy)\biggr)^{\!\!1/p}.
$$

The difference between the Kantorovich definition and the Wasserstein definition lies in the cost function, 
which is given by \(d(x,y)^p\), where \(d(\cdot,\cdot)\) is a distance on \(X\). It is precisely this definition 
of the cost function that guarantees, via the following proposition, that \(W_p\) is indeed a distance.

**Proposition**  
Assume \(X = Y\) and that for \(p > 1\), \(c(x,y) = d(x,y)^p\), where \(d\) is a distance on \(X\), i.e.:

1. \(d(x,y) = d(y,x) \geq 0\);
2. \(d(x,y) = 0\) if and only if \(x = y\);
3. \(\forall (x,y,z) \in X^3,\ d(x,z) \le d(x,y) + d(y,z)\).

Then \(W_p\) is a distance, namely it is symmetric, positive, satisfies \(W_p(P,Q) = 0\) 
if and only if \(P = Q\), and fulfills the triangle inequality:

$$
\forall (P,Q,Z) \in \mathcal{P}(X)^3 \quad 
W_p(P,Z) \le W_p(P,Q) + W_p(Q,Z).
$$

#### The Discrete Case

The Kantorovich problem can also be expressed in discrete form.

**Definition:** suppose \(X = \{ x_1, \dots, x_n \}\) and \(Y = \{ y_1, \dots, y_m \}\). 
In this case, \(P\) and \(Q\) can be identified with two probability vectors 
\(a \in \Sigma_n\) and \(b \in \Sigma_m\), where

$$
\Sigma_n 
= \bigl\{ a \in \mathbb{R}_+^n : \sum_{i=1}^n a_i = 1 \bigr\}.
$$

These probability vectors can be thought of as histograms, in which 
each \(a_i\) represents the probability associated with \(x_i\), and each \(b_j\) 
represents the probability associated with \(y_j\). 

We can define the discrete analogue of the set of probability measures 
\(\Pi(P,Q)\) as follows:

$$
U(a, b) := \bigl\{\, P \in \mathbb{R}_+^{n \times m} : 
P\,\mathbf{1}_m = a,\; P^\top \mathbf{1}_n = b \bigr\},
$$

where \(P\) is the transport matrix, the discrete analogue of 
the transport plan \(\pi\). In other words, 
\(P_{i,j}\) tells us how much mass to move from the point \(x_i\) to the point \(y_j\).

From bin \(i\) of \(a\) to bin \(j\) of \(b\). The constraints related to the set \(U(a,b)\) 
are similar to those of \(\Pi\), meaning that the sum of the rows or columns 
returns \(a\) or \(b\), so that \(P\,\mathbf{1}_m = a = \sum_{j=1}^m P_{i,j}\) 
for each \(i \in \{1,\dots,n\}\) and \(P^\top \mathbf{1}_n = \sum_{i=1}^n P_{i,j} = b_j\) 
for each \(j \in \{1,\dots,m\}\).

The cost function \(\mathbf{c}\) is represented by a matrix \(\mathbf{C} \in \mathbb{R}^{n \times m}\), 
where each \(C_{i,j} = c(x_i, y_j)\) explains the cost of going from \(a_i\) to \(b_j\). 
The average cost analogous to \(I_c\) in the discrete case is defined as

$$
\bar{I}_c 
= \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

We can then formulate the discrete Kantorovich problem as follows:

$$
L_C(a,b) 
:= \min_{P \in U(a,b)} \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

As in the continuous case, assuming \(X, Y \subset E\), with \(E\) a metric space 
equipped with a distance \(d\), we can also define the Wasserstein distance 
of order \(p\) in the discrete setting:

$$
W_p(a,b) 
:= \min_{P \in U(a,b)} 
\biggl(\sum_{i,j} D_{i,j}^p \, P_{i,j}\biggr)^{\!\!1/p}.
$$

where \(D_{i,j}^p = d^p(x_i, y_j)\) is the matrix of distances between the points \(x_i\) and \(y_j\).  
If \(X,Y \subset \mathbb{R}^n\), a standard example of distance between two points in the discrete case 
is \(D_{i,j} = \|x_i - y_j\|\), where \(\|\cdot\|\) is the Euclidean norm.

In the Wasserstein case, we consider the cost matrix \(\mathbf{C} = \mathbf{D}\), 
with \(\mathbf{D}\) being the matrix of Euclidean distances. 

### Jensen–Shannon Divergence

The Jensen–Shannon divergence (JSD) is a measure of similarity (or dissimilarity) between two probability distributions. 
**Definition:** let \(P\) and \(Q\) be two discrete probability distributions over the same domain \(\Omega\). Define

\[
M = \tfrac{1}{2}\,(P + Q),
\]

which is the average (or midpoint) distribution of \(P\) and \(Q\). Then the Jensen–Shannon divergence between \(P\) and \(Q\) is given by

\[
\mathrm{JSD}(P \,\|\, Q) 
= \tfrac{1}{2}\,\mathrm{KL}(P \,\|\, M)
+ \tfrac{1}{2}\,\mathrm{KL}(Q \,\|\, M),
\]

where \(\mathrm{KL}(\cdot \,\|\, \cdot)\) denotes the KL divergence, defined by:

\[
\mathrm{KL}(P \,\|\, Q) 
= \sum_{x \in \Omega} P(x) \,\log\!\Bigl(\tfrac{P(x)}{Q(x)}\Bigr).
\]

**Properties:**

1. **Symmetry**  
   \[
   \mathrm{JSD}(P \,\|\, Q) = \mathrm{JSD}(Q \,\|\, P).
   \]
   This follows because \(M\) is symmetric in \(P\) and \(Q\).

2. **Non-negativity**  
   \[
   \mathrm{JSD}(P \,\|\, Q) \ge 0,
   \]
   with equality if and only if \(P = Q\).

3. **Boundedness**  
   \[
   0 \,\le\, \mathrm{JSD}(P \,\|\, Q) \,\le\, \log(2).
   \]
   The maximum value \(\log(2)\) occurs when \(P\) and \(Q\) have disjoint supports.

4. **Metric Property**  
   Taking the square root gives a metric:
   \[
   d_{\mathrm{JSD}}(P, Q) := \sqrt{\mathrm{JSD}(P \,\|\, Q)}.
   \]
   This metric satisfies the triangle inequality.


```{r}
# Define the child RMarkdown file path
part_1 <- here("notebooks", "partial_analyses", "part_1.Rmd")
```

```{r, child = here("notebooks", "partial_analyses", "part_1.Rmd"), cache = TRUE}
# Run the file which does data analyses even if not knitted by runned manually
rmarkdown::render(part_1)
```

# Temporary stop the knit process here
# The second part will be integrated here

```{r}
knitr::knit_exit()
```

```{r}
# Define the child RMarkdown file path
part_2 <- here("notebooks", "partial_analyses", "part_2.Rmd")
```

```{r, child = here("notebooks", "partial_analyses", "part_2.Rmd"), cache = TRUE}
rmarkdown::render(part_2)
```

# Conclusions and Recommendations

> Perhaps some final plots and small discussions

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References
> Need to add references

```{r}
citation("pROC")
```

- ...
