---
title: "Synthetic Data Analysis"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true 
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
---

```{r, echo = F}
# Load utilities
suppressMessages(library(here))

# Load utilities
source(here("src", "setup.R"))
```

# Imputation techniques

This section reviews imputation techniques for handling missing data, from simple deletion methods to advanced machine learning approaches, highlighting their uses and impact on data reliability.

| **Imputation Technique** | **Description** | **Strengths** | **Limitations** |
|-----------------|-------------------|------------------|-------------------|
| **List-wise Deletion** | Removes cases with missing values. | Simple; works for MCAR data. | Causes data loss and bias if MCAR is violated. |
| **Pairwise Deletion** | Removes data points only when necessary. | Retains more data; less bias for MCAR/MAR. | Inconsistent sample sizes; potential bias. |
| **Simple Imputation** | Replaces missing values with mean, median, or mode. | Easy and efficient. | Can distort data and introduce bias. |
| **Regression Imputation** | Predicts missing values using regression. | Preserves sample size; good for MAR. | Assumes linearity; less effective for non-linear data. |
| **Hot-deck Imputation** | Uses similar cases to fill missing values. | Avoids model assumptions. | Less reliable for small datasets. |
| **Expectation-Maximization (EM)** | Iteratively estimates missing values. | Accurate imputations; handles MAR well. | Computationally intensive. |
| **Multiple Imputation** | Generates multiple datasets to reflect uncertainty. | Reduces bias; more robust results. | Time-consuming; complex to combine results. |
| **GAM Imputation** | Uses Generalized Additive Models for non-linear data. | Flexible for non-linear data. | More complex and computationally expensive. |
| **Decision Tree / Random Forest** | Builds models using decision trees. | Captures complex patterns; handles various data types. | Computationally heavy; prone to overfitting. |
| **K-Nearest Neighbors (KNN)** | Imputes based on nearest neighbors. | Flexible and captures patterns. | High computational cost for large datasets. |

## Performance metrics between Datasets

Since we possess the **original dataset prior to the introduction of missing values**, we will introduce *two metrics* to compare *the distributions of two datasets*: the **original dataset** without missing values and the dataset in which missing values have been **imputed** using various techniques

We indeed care about comparing the distribution of the two datasets more than comparing pointwise the actual reconstruction of the dataset.

-   The **Jensen–Shannon Divergence (JSD)** quantifies the similarity or divergence between two probability distributions $P$ and $Q$. It is based on the midpoint distribution $M = \frac{1}{2}(P + Q)$ and is calculated as the average of the Kullback-Leibler (KL) divergences of $P$ and $Q$ relative to $M$. JSD is symmetric and always non-negative, with values ranging between $0$ (identical distributions) and $\log(2)$ (distributions with disjoint supports).

-   The **Wasserstein distance** provides a way to measure the cost of transforming one probability distribution into another. It can be viewed as the minimum “effort” required to transport all the mass of one distribution to match the other, where the cost depends on both the amount of mass moved and the distance it travels. Unlike divergence-based measures (e.g. KL or JS divergence), Wasserstein distance naturally accounts for the geometry of the sample space, making it particularly useful when the shapes or locations of the distributions differ.

## Visualization of the different techiques

The following paragraph presents visualizations of various imputation techniques applied to different types of datasets to observe their impact. The reconstructed datasets are then evaluated and compared using two distribution-based metrics.

```{r,  class.source = 'fold-show'}
# Define methods with their display names and corresponding functions
imputation_methods <- list(
  # "Original Dataset" = function(data) { data },
  "Mean Imputation" = function(data) { simple_imputation(data, "mean") },
  "Hot Deck" = function(data) { hot_deck_imputation(data) },
  "KNN Imputation" = function(data) { imputed_data <- kNN(data, imp_var = FALSE)},
  "Regression" = function(data) { regression_imputation(data) },
  "Regression with Noise" = function(data) { regression_imputation(data, noise = TRUE) },
  "GAM" = function(data) { gam_based_imputation(data) },
  "GAM with Noise" = function(data) { gam_based_imputation(data, noise = TRUE) },
  "Random Forest" = function(data) { tree_based_imputation(data) },
  "Random Forest with Noise" = function(data) { tree_based_imputation(data, noise=TRUE) }
)
```

### Linear Relationship

We begin by testing the imputation techniques on data characterized by a linear relationship and heteroscedasticity. Methods such as regression imputation are expected to perform well in this scenario, as they are designed to capture and model linear patterns within the data.

#### Analyses of noise in regression imputation

We begin by demonstrating how various noise strategies can be incorporated into the imputation methods, as this technique will be applicable in future scenarios as well.

```{r, fig.width=18, fig.height=10}
# Generate and create missing data
n <- 200
p <- 0.3
linear_data <- generate_data(n, "linear",x_range = c(0, 1),  noise_sd = 0.5, 
                             homoscedasticity = F, min_sd_noise = 0.1)

# linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 100)
# linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)
linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

noise_imputation_methods <- list(
  "Regression" = function(data) { regression_imputation(data) },
  "Regression with Noise" = function(data) { regression_imputation(data, noise = TRUE) }
)

# Use the new combined analysis function
noise_result <- plot_imputations_and_metrics(original_data = linear_data, missing_data = linear_missing, imputation_methods = noise_imputation_methods)

# Access the list of individual plots
noise_plots <- noise_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = noise_plots, ncol = 2)
```

#### Comparison of the different techniques for the simple dataset

```{r, fig.width=18, fig.height=20}
# Use the new combined analysis function
linear_result <- plot_imputations_and_metrics(original_data = linear_data, missing_data = linear_missing, imputation_methods = imputation_methods)

# Access the list of individual plots
imputation_plots <- linear_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- linear_result$metrics_plot
print(metrics_plot)
```

### Quadratic Relationship

The second dataset is constructed with a quadratic relationship and an unbalanced number of data points. In cases involving quadratic relationships, we anticipate that more flexible imputation methods, such as Generalized Additive Models (GAM) and Random Forest, will outperform simple linear regression due to their ability to capture complex, non-linear patterns in the data. The relationship is generated via: $x^2 + \epsilon$.

```{r, fig.width=18, fig.height=20}
# Generate and create missing data
quad_data <- generate_data(n, "quadratic",x_range = c(-2, 2), noise_sd = 1, balanced = F, alpha = 1, beta = 2)
quad_missing <- delete_MAR_1_to_x(quad_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

quad_result <- plot_imputations_and_metrics(quad_data, quad_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- quad_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- quad_result$metrics_plot
print(metrics_plot)
```

### Piecewise

The third dataset is designed with a piecewise relationship, where distinct segments of the data follow different patterns. For this type of distribution, we expect imputation methods capable of handling discontinuities and local variations, such as Random Forest and k-Nearest Neighbors, to perform better than global modeling approaches like linear regression, which may struggle to accurately capture the segmented structure of the data. To avoid missingness in only one of the pieces, we lowered the odds to 3 instead of the usual 10.

The dependent variable $x_2$ is generated using the following piecewise relationship:

$$
x_2 =
\begin{cases} 
2 + 2x_1 + \epsilon, & \text{if } x_1 < \text{pivot} \\
15 + 5x_1 + \epsilon, & \text{if } x_1 \geq \text{pivot}
\end{cases}
$$

Here, $\text{pivot} = \frac{x_{\text{range}[1]} + x_{\text{range}[2]}}{2}$, and $\epsilon$ is a random noise term added to introduce variability.

```{r, fig.width=18, fig.height=20}
# Generate and create missing data
piecewise_data <- generate_data(n, "piecewise", noise_sd = 2)
piecewise_missing <- delete_MAR_1_to_x(piecewise_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 3)

piecewise_result <- plot_imputations_and_metrics(piecewise_data, piecewise_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- piecewise_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- piecewise_result$metrics_plot
print(metrics_plot)
```

### Logarithmic Relationship

The final dataset follows a logarithmic relationship, providing insight into how different imputation methods handle non-linear but monotonic patterns. The dataset is generated by: $3 \log(x) + \epsilon$

```{r, fig.width=18, fig.height=20}
# Generate and create missing data
log_data <- generate_data(n, "log", x_range = c(1, 100), noise_sd = 1)
log_missing <- delete_MAR_1_to_x(log_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

log_result <- plot_imputations_and_metrics(log_data, log_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- log_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

\`\`\`{r, fig.width=18, fig.height=12} \# Access the metrics plot metrics_plot \<- log_result\$metrics_plot print(metrics_plot)
