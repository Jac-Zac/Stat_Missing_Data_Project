---
title: "Synthetic_data_experiment_prova"
author: "Ludovica"
date: "2025-01-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

---
  title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  toc: true
toc_depth: 3
toc_float: true
# toc_float: false
code_fold: hide # Hidden by default unless specified otherwise
df_print: paged
highlight: tango
theme: cosmo
number_sections: true
toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
  wrap: 72
---
  
  ```{r, echo=F, class.source = 'fold-show'}
# Load utilities
source("setup.R") 



# Load the initial analysis file
child = here::here("dataset_analysis", "synthetic_data_analysis.Rmd")
```

```{r, child = "dataset_analysis/synthetic_data_analysis.Rmd", cache = TRUE}
```

```{r}
install.packages("ggplot2")
install.packages("GGally")
install.packages("reshape2")
install.packages("corrplot")
install.packages("here")
install.packages("pROC")
install.packages("randomForest")
install.packages("mgcv")
install.packages("nord")
#install ...

#Inputation methods
install.packages("mice")
# Other alternative
install.packages("VIM") # (Visualization and Imputation of Missing Values) 


## Load requirements 
suppressMessages(library(ggplot2))
suppressMessages(library(here))
suppressMessages(library(GGally))
suppressMessages(library(pROC))
suppressMessages(library(corrplot))
suppressMessages(library(reshape2))
suppressMessages(library(mice))
suppressMessages(library(VIM))
suppressMessages(library(RColorBrewer))
suppressMessages(library(nord))

# Load utilities using here()
#source(here("src", "synthetic_data.R"))
#source(here("src", "missing_data.R"))
#source(here("src", "inputation_methods.R"))
#source(here("src", "metrics.R"))
#source(here("src", "utils.R"))

# Fixes the seed for reproducibility
set.seed(42)


```

In this second part we will deal with the analysis of datasets containing missing data and tecnhiques employed to deal with this issue.
In the field of missing data analysis, various techniques have been developed to address incomplete datasets. These methods are commonly categorized into three groups:

**Deletion Methods**: These approaches remove data entries with missing values. The two primary forms are:

  -*Listwise Deletion*: Eliminates entire rows that have any missing values across variables.   While straightforward, this method can lead to significant data loss and potential bias if the missing data are not missing completely at random (MCAR). 
  
 -*Pairwise Deletion*: Excludes only the specific variables with missing data from analyses, allowing other variables to remain in the analysis. This method utilizes all available data for each analysis but may result in inconsistencies across analyses due to varying sample sizes. 

**Toleration Methods**: These techniques adjust analyses to accommodate missing data without removing it. For example, some statistical models can handle missing data internally by estimating the missing values during analysis. This approach maintains the dataset's size but may introduce additional complexity and assumptions. We will investigate this approach through tree-based learning techniques.

**Imputation Methods**: These methods estimate and replace missing values with plausible data  points based on existing information. Common imputation techniques include:

*Simple Imputation*: Replacing the missing values with the mean or median or mode of the observed values for that variable. This method is simple but can reduce variability and may not be suitable for all data types (for example high-dimensionality datasets).
*Regression Imputation and Multiple Imputation*: Generating multiple datasets with different imputed values and then combining the results to account for the uncertainty of the missing data. This approach provides more robust estimates but is computationally intensive. 
*Hot-deck Imputation*
*Decision Tree-based Imputation*.

### Missing Data Ignoring Techniques

Missing data ignoring techniques delete cases with missing data. While simple, these methods are suitable only when missing data is minimal. Furthermore, they are very effective for MCAR missing values. Two common methods include:

- **Listwise Deletion (LD)**: Also known as case deletion or complete case analysis. It removes cases with any missing data, using only complete cases. While easy and fast, it can lead to significant data loss and bias unless data are missing completely at random (MCAR) and the data sample is large enough.
It is the default choice in most statistical software packages. 

```{r}
# Evaluate imputation method using listwise deletion
listwise_results <- evaluate_imputation_method(listwise_deletion, data_mcar, data_mar, train_index, synthetic_data)
```


- **Pairwise Deletion (PD)**: Known as the available case method.  This method 
considers each variable separately. For each variable, all recorded values in each 
observation are considered and missing data are ignored. This means that different calculations will use different cases and will have different sample sizes.It results in larger sample sizes than the previous method but may cause inconsistent sample sizes across analyses. However it offers better estimates than listwise delection but is unbiased only if the data are MCAR.

```{r}
# Evaluate imputation method using pairwise deletion
pairwise_results <- evaluate_imputation_method(pairwise_deletion, data_mcar, data_mar, train_index, synthetic_data)
```

### Missing Data Toleration Techniques

Missing data toleration techniques handle datasets with missing values directly, making them ideal when the goal is not to predict the missing values. Predicting missing values can introduce bias, leading to unreliable results. These techniques work better when no prediction of missing data is needed.

Some key techniques include:

- **CART **: CART tackles missing data in decision trees by using surrogate splits to assign cases with missing values to branches based on the best available information.

However, for analyses that require complete datasets, missing values must be either filled in or removed beforehand. In cases where missing data is substantial or the mechanism behind it is non-random, **imputation techniques** are often more effective than ignoring techniques.


# Imputation Methods

#### NOTES:

For each of the inputation methods, we can test the performances of different approaches.
Perhaps doing a function to test them all, and we can test this with different simulated datasets.
Then we can test the best performing one on a real dataset and discuss.

Perhaps we can also showcase how missing data can lead to inaccurate conclusions when handled poorly.

**BTW I would ignore timeseries data**
  
  **Test also datasets with outliers perhaps to see if this is problematic**
  
Should we do some testing also on the fact that the type of missing data mechanism might not be known in a real-world scenario ? Sensitivity analyses ? I think this is a bit beyond the scope of what we are doing.

  
  #### Testing the different performance metric:
  
  Explain a bit the difference between the metric for imputation. And perhaps make plot to show the

**Pareto front ?**
  
  > I would suggest providing a summary for each of the imputation methods.

## Imputation Methods

The following imputation methods are tested on both the MCAR and MAR datasets. A regression model is fit on the imputed dataset, and the results are summarized.

#### 3. Single Imputation 

Single imputation involves replacing missing values with a single estimated value. It is a flexible technique that allows complete data analysis methods to be applied by filling in missing data, ensuring consistency across repeated analyses.

However, while it addresses the missing data problem, single imputation does not account for the uncertainty in the estimates of missing values. This can result in overestimated sample sizes, underestimated variance and standard errors, and narrower confidence intervals, leading to higher Type I error rates.

Common simple imputation techniques include:

- **Basic Techniques**:
  - Mean/Median/Mean Imputation
  - Regression Imputation
  - Hot-Deck Imputation

- **Advanced Techniques**:
  - Expectation Maximization (EM)
  
We will concentrate our attention on the first two techniques.

The first two functions are based on mean and median imputations.
The underlying idea is to replace missing values with a quantitative/qualitative (in the case of categorical variables that we won't tackle) attribute of all the non-missing values.
```{r}
# Evaluate imputation method using mean imputation
mean_results <- evaluate_imputation_method(simple_imputation, data_mcar, data_mar, train_index, synthetic_data)
```



```{r}
# Evaluate imputation method using median imputation
median_results <- evaluate_imputation_method(simple_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

#### 4. Regression Imputation

Regression imputation, also called conditional mean imputation, replaces each missing value with a predicted value based on a regression model if data are missing under MAR. 
A general regression imputation is in two phases: first, a regression model is fitted using all the available complete observations, and then missing values are estimated based on the built regression model. 
Little (1992) divided the regression imputation into two categories: independent variables based and both independent and dependent variables based methods.  
The former just uses information in the reported independent variables in a case to impute the missing independent variables, using Weighted Least Squres regression or Generalised Least Squares. GLS uses both dependent variable and reported independent variables to impute missing values if the partial correlation of dependent variable and a missing independent variable given the reported independent variable is high. 
While it preserves the sample size by retaining cases with missing data, this method can lead to inflated correlations and covariances due to the deterministic nature of the imputed values. Additionally, regression imputation may require large sample sizes to produce stable estimates and can underestimate variance, potentially affecting the reliability of statistical analyses.

```{r}
# Evaluate imputation method using regression imputation
regression_results <- evaluate_imputation_method(regression_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

#### 5. Hot-deck Imputation

Traditional hot-deck imputation involves stratifying the dataset into classes based on auxiliary variables. Complete cases within these classes are retained, and the most similar case is selected to impute missing data.
There are two primary selection techniques for identifying similar cases:
Random (Stochastic) Method: This approach randomly selects observed values from donor cases that match the missing data based on predetermined auxiliary variables. If a matched class lacks observed values, it combines with other classes to perform imputation. 
Deterministic Methods: These methods identify the most similar case without missing values and copy its value(s) to fill in the missing data.
Hot-deck imputation preserves the population distribution by substituting different observed values for each missing value and maintains the proper measurement level of variables (categorical variables remain categorical, and continuous variables remain continuous). For this reason it is widely used and very popular.
Hot-deck imputation does not explicitly account for the possibility that the probability of missing data may differ between respondent and non-respondent subpopulations, potentially distorting correlations and covariances. Additionally, the donor dataset must be sufficiently large to identify appropriate donor cases.

We consider only the first type of selection.

```{r}
# Evaluate imputation method using hot-deck imputation
hotdeck_results <- evaluate_imputation_method(hot_deck_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

#### 6. Expectation-Maximization (EM)

It is an iterative method based on the basic idea to first predict the missing values based on assumed values for the parameters, then use these predictions to update the parameters, and  
repeat until the sequence of parameters converges to maximum likelihood estimates.

This is a robust method for handling missing data, often outperforming ad hoc techniques like listwise deletion, pairwise deletion, and mean imputation. This superiority arises because EM assumes that data are missing at random (MAR) rather than missing completely at random (MCAR), allowing for more accurate parameter estimates. 

However, EM has certain limitations. It does not incorporate uncertainty into the imputed values, leading to underestimated standard errors and overly narrow confidence intervals. Additionally, the convergence rate of EM can be slow, especially when a significant portion of the data is missing. This slow convergence is due to the algorithm's iterative nature and the complexity of the underlying data structure. 

Moreover, EM performance can vary depending on the data's characteristics and the underlying assumptions. 
To address these limitations, modifications and extensions to the EM algorithm have been developed. These adaptations aim to improve convergence rates, incorporate uncertainty into imputations, and enhance the algorithm's robustness across various data types and missing data patterns. 

```{r}
# Evaluate imputation method using EM imputation
em_results <- evaluate_imputation_method(em_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

### 7. Multiple Imputation

Multiple Imputation (MI) is a widely accepted method for handling missing data under the assumption of Missing at Random (MAR) and multivariate normality. It retains all the advantages of single imputation while overcoming its key drawbacks. MI improves on single imputation by introducing statistical uncertainty around the true values, which emulates the variability seen in a complete dataset by imputing missing data m times.It is very effective even with small sample sizes.
However it is time-consuming, as it requires the imputation of 5-10 datasets and involves different methods to combine the results.

The procedure offers several desirable features:
- **Introduction of Random Error**: MI adds appropriate random error into the imputation process, ensuring unbiased parameter estimates.
- **Versatility**: MI is applicable to any type of data and analysis.
- **Efficiency**: MI is efficient, even for small values of m. Typically, just 3-5 imputations are sufficient to yield excellent results.

The MI procedure consists of three main steps:
1. **Imputation**
2. **Analysis**
3. **Combination**
These steps are repeated m times to generate the final imputed dataset, ensuring a more robust analysis.


...

### 8. GAM Imputation

```{r}
# Evaluate imputation method using GAM-based imputation
gam_results <- evaluate_imputation_method(gam_based_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

# Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

## Decision tree / Random Forests

```{r}
# Evaluate imputation method using tree-based (Random Forest) imputation
tree_results <- evaluate_imputation_method(tree_based_imputation, data_mcar, data_mar, train_index, synthetic_data)
```


## Ensemble methods


## Bayesian-based methods 
We don't know if to investigate also this approach, that, even though it is a very effective approach (one of the most promising).

# Results and Discussion

## Performance metrics


- **Mean Absolute Error (MAE)**
  
  MAE measures the average difference between imputed values and true values defined as:
  
  $$
  MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
  $$
  
  - **Mean Squared Error (MSE)**
  
  While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
  
  $$
  MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
$$
  
  - **Root Mean Square Error (RMSE)**
  
  RMSE computes the difference in imputed values and actual values as follows:
  
  $$
  RMSE = \sqrt{MSE}
$$
  
  - **Area under the curve (AUC)**
  
  AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
  
  $$
  TPR = \frac{TP}{TP + FN} \tag{21}
$$
  
  $$
  FPR = \frac{FP}{FP + TN} \tag{22}
$$
  
  #### Function implementation for different performance metrics
  > They are defined inside metrics

```{r}
# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```


### Comparison of Results

Create a summary table and plots to compare RMSE, mean absolute differences across all methods 

```{r, echo = T}
# Create results dataframe with RMSE
results <- data.frame(
  Method = c("Listwise", "Pairwise", "Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree"),
  RMSE_Prediction = c(
    listwise_results$rmse_mcar, 
    pairwise_results$rmse_mcar, 
    mean_results$rmse_mcar, 
    median_results$rmse_mcar, 
    regression_results$rmse_mcar, 
    hotdeck_results$rmse_mcar, 
    em_results$rmse_mcar, 
    gam_results$rmse_mcar, 
    tree_results$rmse_mcar
  )
)

# Create comparison results dataframe with safe extraction for MAE, RMSE, and Correlation
comparison_methods <- c("Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree")

# Ensure you include the corresponding differences for Tree-based methods as well
comparison_diffs <- list(mean_results$diff_mcar, 
                         median_results$diff_mcar, 
                         regression_results$diff_mcar, 
                         hotdeck_results$diff_mcar, 
                         em_results$diff_mcar, 
                         gam_results$diff_mcar, 
                         tree_results$diff_mcar)

# Custom function to safely extract values from the list of metrics
safe_extract <- function(metric_list, metric_name) {
  if (!is.null(metric_list) && metric_name %in% names(metric_list)) {
    if (length(metric_list[[metric_name]]) > 0) {
      return(metric_list[[metric_name]][1])  # Extract the first value
    } else {
      return(NA)  # If the metric is an empty vector, return NA
    }
  } else {
    message(paste("Warning: Metric", metric_name, "not found in metric_list. Returning NA."))
    return(NA)
  }
}

# Extract metrics for comparison
comparison_results <- data.frame(
  Method = comparison_methods,
  MAE = sapply(comparison_diffs, safe_extract, "mae"),
  RMSE = sapply(comparison_diffs, safe_extract, "rmse"),
  Correlation = sapply(comparison_diffs, safe_extract, "correlation")
)

# Remove any rows where all metrics are NA
comparison_results <- comparison_results[rowSums(is.na(comparison_results[,-1])) < ncol(comparison_results)-1, ]
```

### Plot the difference metrics between datasets

```{r}
# Plot 1: RMSE of Prediction Models
p1 <- create_bar_plot(results, "reorder(Method, RMSE_Prediction)", "RMSE_Prediction", "#88C0D0", 
                      "Prediction Model Performance")
print(p1)

# Plot 2: MAE
p2 <- create_bar_plot(comparison_results, "reorder(Method, MAE)", "MAE", "#A3BE8C", 
                      "Mean Absolute Error")
print(p2)

# Plot 3: RMSE Comparison
p3 <- create_bar_plot(comparison_results, "reorder(Method, RMSE)", "RMSE", "#EBCB8B", 
                      "Root Mean Square Error")
print(p3)

# Plot 4: Correlation
p4 <- create_bar_plot(comparison_results, "reorder(Method, Correlation)", "Correlation", "#B48EAD", 
                      "Correlation with Original Data")
print(p4)
```

## Imputation Performance Comparison

## Computational Performance

# Conclusions and Recommendations

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References

```{r}
citation("pROC")
@article{article,
author = {Song, Qinbao and Shepperd, Martin},
year = {2007},
month = {10},
pages = {261-291},
title = {Missing Data Imputation Techniques},
volume = {2},
journal = {IJBIDM},
doi = {10.1504/IJBIDM.2007.015485}
}
```

- ...
