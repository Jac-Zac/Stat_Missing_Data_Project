---
title: "Synthetic Data Analysis"
author: "Jacopo Zacchigna"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true 
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
---

# Environment Setup
> Everything is defined inside `setup.R`

```{r, echo = T}
# Load utilities
source("../setup.R")
```


```{r, echo = F}
# Display all available palettes
# Show each palette individually
# nord_show_palette("polarnight")
# nord_show_palette("snowstorm")
# nord_show_palette("frost")
# nord_show_palette("aurora")
```

# Data Generation and Exploration

## Synthetic Dataset Creation

We begin by analyzing simpler datasets to explore various imputation methods. Initially, we generate datasets without missing values. 
In the subsequent step, we introduce missing values according to two mechanisms: Missing Completely at Random (MCAR) and Missing at Random (MAR).

Our analysis starts with the creation of a synthetic dataset designed to exhibit specific correlations among covariates. We define a target variable that has a different types of relationships with these covariates.

First Dataset: This simpler dataset assumes a linear relationship among covariates, with the target variable linearly dependent on them.
By structuring our study in this manner, we can systematically assess how different imputation methods perform under varying conditions of data missingness and correlation structures.

```{r, echo=T, class.source = 'fold-show'}
# Generate synthetic dataset with 5 covariates, a linear relationship between the variable and predictor
# And also a linear relationship with the covariates.
synthetic_data <- synthetic_dataset_gen(n_samples = 1000, 
                                        n_covariates = 5, 
                                        correlation = "linear", 
                                        target_type = "linear", 
                                        noise_level = 0.5)

# Summary of the dataset
summary(synthetic_data)
```
The summary is not very informative considering this is just a synthetic dataset. 
We also tried to generate other more complex datasets (with non-linear and polynomials relationships among variables) but we decided to report here only a simpler example, by way of illustration. 

## Data Visualization

This section should be expanded to explore the dataset more in depth.

The first step in our analyses is to visualize the correlation between the continuous covariates in our dataset.

```{r, echo = F}
cor_matrix <- cor(synthetic_data)

# Define colors using a Nord palette
blue_nord <- nord("frost", 4)[[length(nord("frost", 4))]]  # Choose a Nord palette and number of colors
red_nord <- nord("aurora", 1)

# Plot the correlation matrix
corrplot.mixed(cor_matrix, 
               lower = "number", 
               upper = "ellipse",
               addgrid.col = "gray",
               tl.col = "black",
               tl.cex = 0.7,
               lower.col = colorRampPalette(c(blue_nord, "white", red_nord))(100),
               upper.col = colorRampPalette(c(blue_nord, "white", red_nord))(100))
```

Since for other datasets we considered non-linear relationships among variables the correlation matrix should be not informative in these two cases. Indeed traditional correlation coefficients like Pearson's may not adequately capture the strength or nature of variable's associations.

For this reason we can use scatterplots to visually assess relationships between variables. Trhough these, non-linear patterns may become evident, guiding the selection of appropriate analytical methods.

We considered a scatterplot for each pair of variables in the datasets.

```{r, echo = F}
# Scatter plot matrix linear covariates
ggpairs(synthetic_data)
```

We can also plot histograms for each covariate to showcase their distributions.

```{r, echo =F}
# Define Nord color palette
nord_colors <- nord("frost", 4)  # Choose a Nord palette and the number of colors you want

# Plot histogram for each continuous variable
data_continuous <- synthetic_data[, sapply(synthetic_data, is.numeric)]

continuous_melted <- reshape2::melt(data_continuous)

ggplot(continuous_melted, aes(x = value)) +
  geom_histogram(bins = 30, 
                 fill = nord_colors[1],     # Use a color from the Nord palette
                 color = "black",
                 linewidth = 0.2,                 # Adjust this value for thinner borders
                 alpha = 0.7) +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Distribution of Continuous Variables", x = "Value", y = "Frequency")
```

### Regression Model on Original Dataset (Baseline)

In the third step of our analysis on the complete dataset we want to fit regression models that we will later use for baseline comparison.
*We should instead use a subset of the covariate (that is relevant) for this linear model.*

```{r}

# Calculate number of samples for training set
num_samples <- nrow(synthetic_data) * 0.8

# Randomly sample indices for training set
train_index <- sample(seq_len(nrow(synthetic_data)), size = num_samples)

# Split the dataset in training and testing
train_data <- synthetic_data[train_index, ]
test_data <- synthetic_data[-train_index, ]

# Fit baseline model with all of the covariates
baseline_model <- lm(target ~ ., data = train_data)
baseline_predictions <- predict(baseline_model, test_data)
baseline_rmse <- sqrt(mean((baseline_predictions - test_data$target)^2))

summary(baseline_model)
paste("RMSE on the test set: ", baseline_rmse)
```
We can fit also different models, that we expect to perform not better than the linear model when the relations are linear, but better in other cases. 

```{r}
# Generalized Additive Model
# Initially with all of the variables for semplicity
gam_model <- gam(target ~ s(X1) + s(X2) + s(X3) + s(X4)+ s(X5), data = synthetic_data)

# Decision Tree, with all of the variables for semplicity
rpart_model <- rpart(target ~ ., data = synthetic_data)
```
# Missing Data Analysis

In statistical analysis, understanding the patterns of missing data is crucial for selecting appropriate handling methods. The primary patterns are:

**Univariate Missing Data**: Occurs when only a single variable has missing values. This pattern is uncommon across most disciplines and typically arises in experimental studies. 

**Monotone Missing Data**: Characterized by a situation where the missingness of one variable implies the missingness of all subsequent variables. This pattern is often observed in longitudinal studies where participants drop out and do not return. The monotone pattern is generally easier to manage, as the missingness follows a clear, observable sequence. 

**Non-Monotone Missing Data**: Occurs when the missingness of one variable does not predict the missingness of other variables. This pattern is more complex and requires careful analysis to handle appropriately. 

Recognizing these patterns helps in choosing suitable methods for handling missing data, thereby improving the robustness and validity of statistical analyses.
Let's start from our synthetic datasets. Obviously no missing data are present there. Let's check it.

### Missing data in the original dataset
> Obviously, no missing data is present in our dataset; it is synthetically generated.

```{r}
# Plot missing data patterns in the original dataset

# original_missing <- as.data.frame(sapply(synthetic_data_complex_poly, is.na))
# original_missing$index <- 1:nrow(original_missing)
# original_missing_melted <- melt(original_missing, id.vars = "index")

# original_missing <- as.data.frame(sapply(synthetic_data_poly_spline, is.na))
# original_missing$index <- 1:nrow(original_missing)
# original_missing_melted <- melt(original_missing, id.vars = "index")

original_missing <- as.data.frame(sapply(synthetic_data, is.na))
original_missing$index <- 1:nrow(original_missing)
original_missing_melted <- melt(original_missing, id.vars = "index")

ggplot(original_missing_melted, aes(x = index, y = variable, fill = value)) +
  geom_tile() +
  scale_fill_manual(values = c("white", red_nord)) +
  labs(title = "Missing Data Pattern - Original", x = "Index", y = "Variable")
```

The missing data theory, established by Rubin, categorizes missingness into three main mechanisms based on the observed and missing data. The data set, denoted as $Y$, is divided into observed $Y_o$) and missing $Y_m$ data. A missing value matrix, $R$$, is defined to indicate the presence or absence of data: 

$$
R = \begin{cases}
0 & \text{if } Y \text{ is observed} \\
1 & \text{if } Y \text{ is missing}
\end{cases}
$$

The relationship between missingness in $R$ and the data set $Y$ is represented by a vector $q$. These mechanisms help guide the handling of missing data in various methods.

Here is a summary focusing on Missing Completely at Random (MCAR) and Conditionally at Random (CAR, often referred to as MAR – Missing At Random) mechanisms:

## Missing Completely at Random (MCAR)

**Definition**: In the MCAR mechanism, the missingness is entirely independent of both observed and unobserved data. The probability that data is missing does not depend on any feature values. The probability of MCAR is defined as:
$$P(R|q)$$

Missing values are inserted randomly across the dataset, either using Bernoulli trials or random selection methods. 
MCAR mechanisms are straightforward since missingness is independent of any feature values. Various methods can be used to generate MCAR data:

**Univariate MCAR Implementation**: Only one feature in the dataset has missing values:

  - **Random Position Selection**: Select a feature $x_{miss}$ and delete values at random positions using a random number generator or permutation.  
  - **Bernoulli Trials**: Use Bernoulli trials to determine missingness. A success probability $p$ corresponds to the desired missing rate (MR). Each data point has a probability $p$ of being set as missing. For small datasets, Bernoulli trials may lead to variation in the actual missing rate due to random variation from small sample sizes.

**Multivariate MCAR Implementation**: Missing values are distributed across multiple features:

  - **Uniform Distribution**: Generate missing values equally across all features. Each feature has the same missing rate.  
  - **Random Distribution**: Choose a total number of missing values based on the desired MR and randomly select cells across the dataset.  

## Missing At Random (MAR or CAR)

**Definition**: In the CAR (MAR) mechanism, the probability of missing data depends on the values of observed data but not on the unobserved (missing) values themselves. It assumes a dependency between missingness and observed features.

The probability for MAR can be defined as:

$$P(R|Y_o, q)$$

E.g. Younger participants in a smoking survey might be more likely to skip reporting their cigarette consumption, but the missingness is independent of the actual number of cigarettes consumed.

Several approaches exist, often involving a determining feature. For instance, missing values in one feature (like cigarette consumption) may depend on values of another observed feature (like age). Configurations can involve ranks, percentiles, or division into groups based on a feature’s value.

Each mechanism carries implications for analysis and imputation, as assumptions about the data's structure influence how accurately the missing values can be predicted or imputed.

In MAR mechanisms, missingness depends on observed values of other features but not on the missing values themselves. This adds complexity to the generation process:

**Univariate MAR Implementation**: Missing values in one feature $x_{miss}$ are determined by the values of another feature $x_{obs}$ (also called the determining feature):

  - **Rank-Based Selection**: Compute the ranks of $x_{obs}$ and assign higher or lower probabilities for missingness based on these ranks.
  Probability of missingness for each pattern: $P(x_{i,miss} = \text{missing}) = \frac{r_{i,obs}}{\sum r_{i,obs}}$ where $r_{i,obs}$ is the rank of the $i^{th}$ observation of $x_{obs}$.  
  - **Percentile or Cut-off Method**: Sort the values of $x_{obs}$ and choose missing positions in $x_{miss}$ corresponding to lower values in $x_{obs}$. The cutoff may be determined by a percentile.  

**Multivariate MAR Implementation**: MAR for multiple features requires defining relationships among multiple pairs of features:
  
  - **Pairs of Features**:  
    Select pairs $\{x_{obs}, x_{miss}\}$ for each combination of features. Missing values in $x_{miss}$ are inserted based on observed values in $x_{obs}$.  

  - **Correlated Feature Sets**:  
    When the dataset contains correlated features, missingness can be simulated in dependent features. MAR implementations often simulate this by ordering features by correlation or mutual information with the class label and generating missing values progressively.


The MAR implementations can be sensitive to the choice of the determining feature. Consistent experimental design is crucial when evaluating imputation methods using synthetic MAR data. Generating accurate MAR patterns requires careful tuning of dependency relationships, especially when working with nominal or categorical features, as ranking or ordering becomes problematic.

## Introducing Missing Data

### Showcase missing data functions

Showcase Introduce Mar function (example to show how to use print_function)

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(introduce_mcar)
```

### Introducing missing data with different mechanisms

## MCAR mechanism

We start by delating the 10% of values from the first covariate of our dataset in order to perform univariate deletion. 

```{r}
data.MCAR.uni <- delete_MCAR(synthetic_data, p = 0.1, cols_mis = c('X1'), n_mis_stochastic = F)
aggr(data.MCAR.uni, numbers=TRUE, sortVars=TRUE, labels=names(data))

```
This is the easiest case to deal with, but not very common in real world.

The following code demonstrates how to simulate missing data under the MCAR assumption in specific columns of a dataset and visualize the resulting missing data patterns.  The resulting data.MCAR.multi dataset is a modified version of the original, with 10% of the entries in columns 'X1', 'X2', and 'X3' replaced with NA values. 

```{r}
data.MCAR.multi <- delete_MCAR(synthetic_data, p = 0.1, cols_mis = c('X1','X2','X3'), n_mis_stochastic = F)
aggr(data.MCAR.multi, numbers=TRUE, labels=names(synthetic_data), prop = c(TRUE, FALSE))
```

The `delete_MAR_censoring()` function in R creates Missing At Random (MAR) values by censoring data in one column (`cols_mis`) based on values in another column (`cols_ctrl`). It divides the data into groups using either sorting or quantiles. Missing values are created:

- where = "lower": in rows with the smallest values in cols_ctrl.
- where = "upper": in rows with the largest values.
- where = "both": in both extremes, splitting missingness between the smallest and largest values.

The p parameter determines the proportion of missing values, and sorting controls whether to use sorting or quantiles for determining cutoffs.

## MAR mechanism

The following function introduces Missing At Random (MAR) values into the dataset.
We want that 10% of the data in the columns listed in cols_mis (X1) should be missing and the missingness in X1 is determined by the values in X2. For example, missingness might be more likely when X2 is below or above a threshold.
This creates a modified dataset (data.MAR.uni) where values in X1 are missing based on the values in X2.


```{r}
data.MAR.uni <- delete_MAR_censoring(synthetic_data,p = 0.1, cols_mis = c('X1'), cols_ctrl = c('X2'))
aggr(data.MAR.uni, numbers=TRUE, labels=names(synthetic_data), prop = c(TRUE, FALSE))
plot(synthetic_data$X1, synthetic_data$X2, col = 'red')
points(data.MAR.uni$X1, data.MAR.uni$X2)
```

In general, the `delete_MAR_1_to_x()` function in R creates **Missing At Random (MAR)** values by controlling missingness in one column (`cols_mis`) based on the values of another column (`cols_ctrl`). It splits the data into two groups using a cutoff value (e.g., median) in `cols_ctrl`. Group 1 contains rows below the cutoff, and group 2 contains rows above. The `x` parameter sets the odds ratio of missing data: for `x = 3`, group 2 is **3 times** more likely to have missing values than group 1.


The delete_MAR_1_to_x Function creates MAR values in multiple columns (cols_mis) based on other columns (cols_ctrl).
In our example we introduced missing values in X1 and X2 and missingness in X1 and X2 is controlled by X3 and X4. For example, missingness may increase when X3 or X4 exceeds a threshold.
x = 10: The odds ratio for missingness between two groups (e.g., above and below the median of cols_ctrl) is 10:1.

```{r}
data.MAR.multi <- delete_MAR_1_to_x(synthetic_data,p = 0.1, cols_mis = c('X1','X2'), cols_ctrl = c('X3','X4'), x = 10)
```
```{r}
aggr(data.MAR.multi, numbers=TRUE, labels=names(data), prop = c(TRUE, FALSE))
```

```{r}
plot(synthetic_data$X1, synthetic_data$X3, col = 'red')
points(data.MAR.multi$X1,data.MAR.multi$X3)

plot(synthetic_data$X2,synthetic_data$X4, col = 'red')
points(data.MAR.multi$X2,data.MAR.multi$X4)
```

The MAR mechanism ensures that missingness in one or more columns is correlated with values in other columns, making the data realistic.
The visualizations using aggr and scatter plots help understand how missing values are distributed and how they relate to control variables (cols_ctrl).

```{r}
# Ensuring methods work as expected
sum(is.na(data.MAR.uni)) / nrow(data.MAR.uni)  # Should be approximately 10%
sum(is.na(data.MAR.multi)) / nrow(data.MAR.multi) # Should be approximately 10%
```

```{r, echo =F}
# Analyze missing patterns and summarize
#For MCAR
 print(summarize_missing(data.MCAR.uni))
 print(summary(data.MCAR.uni))
 print(summarize_missing(data.MCAR.multi))
 print(summary(data.MCAR.multi))
#For MAR
 print(summarize_missing(data.MAR.uni))
 print(summary(data.MAR.uni))
 print(summarize_missing(data.MAR.multi))
 print(summary(data.MAR.multi))
```

### Plot missing data for the different mechanisms

#### Custom missing data visualization

We can use the following tools to verify how missing data are distributed in our dataset. These will be very useful in the analysis of our definitive dataset, since it can help us in finding the best strategies to deal with missing values.

```{r, class.source = 'fold-show'}
## Plot for MCAR
plot_missing_data(data.MCAR.uni, "MCAR uni", c("white", red_nord))
plot_missing_data(data.MCAR.multi, "MCAR multi", c("white", red_nord))

# Plot for MAR
plot_missing_data(data.MAR.uni, "MAR uni", c("white", red_nord))
plot_missing_data(data.MAR.multi, "MAR multi", c("white", red_nord))
```

In the final dataset we will create different datasets using these functions and we will analyse them in order to perform the imputation or delation technique that is more suitable for the type of dataset. 
#### VIM package missing data visualization

```{r, class.source = 'fold-show'}
nord_contrast = c(blue_nord, red_nord)

# For data mcar
aggr(data.MCAR.multi, plot = TRUE, numbers = TRUE, prop = FALSE, col = nord_contrast)

# For data_mar
aggr(data.MAR.multi, plot = TRUE, numbers = TRUE, prop = FALSE, col = nord_contrast)
```

```{r}
data_mcar <- introduce_mcar(synthetic_data, prop_missing = 0.1, missing_cols = "X3")
data_mar <- introduce_mar(synthetic_data, prop_missing = 0.1, predictor_cols = c("X1", "X2"), target_cols = c("X3"))
```
