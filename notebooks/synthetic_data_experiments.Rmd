---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
    wrap: 72
---

```{r, echo=F, class.source = 'fold-show'}
library(here)

# Load utilities
source(here("src", "setup.R"))

# Define the child RMarkdown file path
child <- here("notebooks", "dataset_analysis", "synthetic_data_analysis.Rmd")
```

```{r, child = here("notebooks", "dataset_analysis", "synthetic_data_analysis.Rmd"), cache = TRUE}
# Run the file which does data analyses even if not knitted by runned manually
rmarkdown::render(child)
```
# Imputation Methods

The following imputation methods are tested on both the MCAR and MAR datasets. A regression model is fit on the imputed dataset, and the results are summarized.

## 1. Listâ€‘wise (Case) Deletion

List-wise deletion removes any case with missing values from analysis and is the default method in many statistical software tools. It works well when data is large and missing completely at random (MCAR). However, if these conditions are not met, it can lead to bias and loss of important information, making it a less suitable approach.

```{r}
# Evaluate imputation method using listwise deletion
listwise_results <- evaluate_imputation_method(listwise_deletion, data_mcar, data_mar, train_index, synthetic_data)
```


## 2. Pairwise Deletion

Pairwise deletion minimizes information loss compared to list-wise deletion by only removing data points when necessary to check if missing values are truly missing. It produces lower bias for data missing completely at random (MCAR) or missing at random (MAR).

```{r}
# Evaluate imputation method using pairwise deletion
pairwise_results <- evaluate_imputation_method(pairwise_deletion, data_mcar, data_mar, train_index, synthetic_data)
```

## 3. Simple Imputation

Simple imputation replaces missing values using the mean, median, or mode of non-missing data, offering simplicity and ease of use. However, it can introduce bias or unrealistic results, especially in high-dimensional or large-scale data sets, making it unsuitable for handling big data effectively.

#### Mean Imputation

```{r}
# Evaluate imputation method using mean imputation
mean_results <- evaluate_imputation_method(simple_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

#### Median Imputation

```{r}
# Evaluate imputation method using median imputation
median_results <- evaluate_imputation_method(simple_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

## 4. Regression Imputation
> NOTE: In the final version this is the idea (fit a model correctly here we simply used stepAIC) for each of the inputation techniqeus were this is needed

Regression imputation replaces missing values using predictions from a regression model built on complete data, preserving sample size and assuming data is missing at random (MAR). It includes single and multivariate regression methods, depending on the number of missing variables. 

```{r, class.source = 'fold-show'}
library(MASS)  # For stepAIC

#' Regression Imputation with Stepwise Selection
#' @param data Data frame with missing values
#' @return Data frame with missing values imputed using stepwise regression
regression_imputation <- function(data) {
  for (col in names(data)) {
    if (any(is.na(data[[col]])) && is.numeric(data[[col]])) {
      # Get complete cases for this column
      complete_data <- data[complete.cases(data), ]
      incomplete_rows <- which(is.na(data[[col]]))
      predictors <- setdiff(names(data), col)
      
      # Fit initial model
      initial_model <- lm(as.formula(paste(col, "~ .")), data = complete_data)
      
      # Perform stepwise selection
      step_model <- stepAIC(initial_model, direction = "both", trace = FALSE)
      
      # Priting the summary of the models obtained
      print(summary(step_model))
      
      # Make predictions using the stepwise model
      predictions <- predict(step_model, newdata = data[incomplete_rows, predictors, drop = FALSE])
      
      # Impute the missing values
      data[[col]][incomplete_rows] <- predictions
    }
  }
  return(data)
}

# Perform imputation on MCAR and MAR datasets
imputed_result_mcar <- regression_imputation(data_mcar)
imputed_result_mar <- regression_imputation(data_mar)

# Split the imputed datasets into training and testing sets
train_mcar <- imputed_result_mcar[train_index, ]
test_mcar <- imputed_result_mcar[-train_index, ]

train_mar <- imputed_result_mar[train_index, ]
test_mar <- imputed_result_mar[-train_index, ]

# Build and evaluate models
model_mcar <- lm(target ~ ., data = train_mcar)
predictions_mcar <- predict(model_mcar, test_mcar)
rmse_mcar <- sqrt(mean((test_mcar$target - predictions_mcar)^2))

model_mar <- lm(target ~ ., data = train_mar)
predictions_mar <- predict(model_mar, test_mar)
rmse_mar <- sqrt(mean((test_mar$target - predictions_mar)^2))

# Define dist_metrics
dist_metrics = c("wasserstein", "jsd")

# Calculate distribution metrics for MCAR
dist_metrics_mcar <- compare_distributions(
  original_data = synthetic_data,
  imputed_data = imputed_result_mcar,
  metrics = dist_metrics
)

# Calculate distribution metrics for MAR
dist_metrics_mar <- compare_distributions(
  original_data = synthetic_data,
  imputed_data = imputed_result_mar,
  metrics = dist_metrics
)

# Calculate traditional differences (if needed)
diff_mcar <- compare_imputed_to_original(synthetic_data, imputed_result_mcar)
diff_mar <- compare_imputed_to_original(synthetic_data, imputed_result_mar)


# Store and print results
regression_results <- list(
    rmse_mcar = rmse_mcar,
    rmse_mar = rmse_mar,
    diff_mcar = diff_mcar,
    diff_mar = diff_mar,
    distribution_metrics = list(
      mcar = dist_metrics_mcar,
      mar = dist_metrics_mar
    )
)

# Evaluate imputation method using regression imputation
# regression_results <- evaluate_imputation_method(regression_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

## 5. Hot-deck Imputation

Hot-deck imputation replaces missing values by selecting a donor from similar cases with complete data, either randomly or based on the closest match. It is widely used because it preserves data structure, reduces bias, and avoids model dependency. However, it lacks a robust theoretical foundation compared to other imputation techniques. 

```{r}
# Evaluate imputation method using hot-deck imputation
hotdeck_results <- evaluate_imputation_method(hot_deck_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

## 6. Expectation-Maximization (EM)

The expectation-maximization (EM) algorithm iteratively handles missing data using an "impute, estimate, and iterate" process until convergence. It alternates between the expectation step, estimating missing values based on observed data, and the maximization step, optimizing the likelihood of the complete data. It requires costly matrix computations.

```{r}
# Evaluate imputation method using EM imputation
em_results <- evaluate_imputation_method(em_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

## 7. Multiple Imputation

Multiple imputation addresses missing data by generating multiple complete datasets using observed data distributions to estimate missing values, reflecting uncertainty. Analysis is performed on each dataset, and results are combined into a single estimate. This technique overcomes single imputation limitations, reducing bias and providing more reliable results. 

## 8. GAM Imputation

GAM-based imputation leverages Generalized Additive Models to handle missing data, allowing for flexible modeling of relationships between variables. This technique is particularly effective when the relationship between predictors and the target variable is non-linear.

```{r}
# Evaluate imputation method using GAM-based imputation
gam_results <- evaluate_imputation_method(gam_based_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

# Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

## Decision tree / Random Forests

Decision tree imputation builds a predictive model for each variable with missing data by constructing a decision tree where branches represent decisions based on predictor variables, and leaves represent the predicted values. This method can handle both categorical and numerical data while capturing complex interactions and patterns in the data.

```{r}
# Evaluate imputation method using tree-based (Random Forest) imputation
tree_results <- evaluate_imputation_method(tree_based_imputation, data_mcar, data_mar, train_index, synthetic_data)
```


## Ensemble methods

# Results and Discussion

## Performance metrics

- **Mean Absolute Error (MAE)**

    MAE measures the average difference between imputed values and true values defined as:
    
    $$
    MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
    $$

- **Mean Squared Error (MSE)**
    
    While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
    
    $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
    $$

- **Root Mean Square Error (RMSE)**

    RMSE computes the difference in imputed values and actual values as follows:
  
    $$
    RMSE = \sqrt{MSE}
    $$
    
- **Area under the curve (AUC)**

    AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
    
    $$
    TPR = \frac{TP}{TP + FN} \tag{21}
    $$
    
    $$
    FPR = \frac{FP}{FP + TN} \tag{22}
    $$
    
#### Function implementation for different performance metrics
> They are defined inside metrics
    
```{r}
# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```

### Comparison of Results

Create a summary table and plots to compare RMSE, mean absolute differences across all methods 

```{r, echo = T}
# Create results dataframe with RMSE
results <- data.frame(
  Method = c("Listwise", "Pairwise", "Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree"),
  RMSE_Prediction = c(
    listwise_results$rmse_mcar, 
    pairwise_results$rmse_mcar, 
    mean_results$rmse_mcar, 
    median_results$rmse_mcar, 
    regression_results$rmse_mcar, 
    hotdeck_results$rmse_mcar, 
    em_results$rmse_mcar, 
    gam_results$rmse_mcar, 
    tree_results$rmse_mcar
  )
)

# Create comparison results dataframe with safe extraction for MAE, RMSE, and Correlation
comparison_methods <- c("Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree")

# Ensure you include the corresponding differences for Tree-based methods as well
comparison_diffs <- list(mean_results$diff_mcar, 
                         median_results$diff_mcar, 
                         regression_results$diff_mcar, 
                         hotdeck_results$diff_mcar, 
                         em_results$diff_mcar, 
                         gam_results$diff_mcar, 
                         tree_results$diff_mcar)

# Custom function to safely extract values from the list of metrics
safe_extract <- function(metric_list, metric_name) {
  if (!is.null(metric_list) && metric_name %in% names(metric_list)) {
    if (length(metric_list[[metric_name]]) > 0) {
      return(metric_list[[metric_name]][1])  # Extract the first value
    } else {
      return(NA)  # If the metric is an empty vector, return NA
    }
  } else {
    message(paste("Warning: Metric", metric_name, "not found in metric_list. Returning NA."))
    return(NA)
  }
}

# Extract metrics for comparison
comparison_results <- data.frame(
  Method = comparison_methods,
  MAE = sapply(comparison_diffs, safe_extract, "mae"),
  RMSE = sapply(comparison_diffs, safe_extract, "rmse"),
  Correlation = sapply(comparison_diffs, safe_extract, "correlation")
)

# Remove any rows where all metrics are NA
comparison_results <- comparison_results[rowSums(is.na(comparison_results[,-1])) < ncol(comparison_results)-1, ]

# First, create the distribution metrics dataframe
comparison_methods <- c("Listwise", "Pairwise", "Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree")
distribution_results <- data.frame(
  Method = rep(comparison_methods, 2),
  Metric_Type = c(rep("Wasserstein", length(comparison_methods)), 
                 rep("JSD", length(comparison_methods))),
  Value = c(
    # Wasserstein distances
    sapply(list(
      listwise_results$distribution_metrics$mcar,
      pairwise_results$distribution_metrics$mcar,
      mean_results$distribution_metrics$mcar,
      median_results$distribution_metrics$mcar,
      regression_results$distribution_metrics$mcar,
      hotdeck_results$distribution_metrics$mcar,
      em_results$distribution_metrics$mcar,
      gam_results$distribution_metrics$mcar,
      tree_results$distribution_metrics$mcar
    ), function(x) safe_extract(x, "wasserstein")),
    # JSD values
    sapply(list(
      listwise_results$distribution_metrics$mcar,
      pairwise_results$distribution_metrics$mcar,
      mean_results$distribution_metrics$mcar,
      median_results$distribution_metrics$mcar,
      regression_results$distribution_metrics$mcar,
      hotdeck_results$distribution_metrics$mcar,
      em_results$distribution_metrics$mcar,
      gam_results$distribution_metrics$mcar,
      tree_results$distribution_metrics$mcar
    ), function(x) safe_extract(x, "jsd"))
  )
)
```

### Plot the difference metrics between datasets

```{r}
# Plot 1: RMSE of Prediction Models
p1 <- create_bar_plot(results, "reorder(Method, RMSE_Prediction)", "RMSE_Prediction", "#88C0D0", 
                      "Prediction Model Performance")
# Plot 2: MAE
p2 <- create_bar_plot(comparison_results, "reorder(Method, MAE)", "MAE", "#A3BE8C", 
                      "Mean Absolute Error")
# Plot 3: RMSE Comparison
p3 <- create_bar_plot(comparison_results, "reorder(Method, RMSE)", "RMSE", "#EBCB8B", 
                      "Root Mean Square Error")
# Plot 4: Correlation
p4 <- create_bar_plot(comparison_results, "reorder(Method, Correlation)", "Correlation", "#B48EAD", 
                      "Correlation with Original Data")

# Remove rows with NA values
distribution_results_filtered <- distribution_results[!is.na(distribution_results$Value), ]

# Create the faceted distribution metrics plot with filtered data
p5 <- ggplot(distribution_results_filtered, 
             aes(x = reorder(Method, Value), 
                 y = Value)) +
  geom_bar(stat = "identity", 
           fill = "#5E81AC") +
  geom_text(aes(label = round(Value, 3)), 
            vjust = -0.5,
            size = 3) +
  facet_wrap(~Metric_Type, 
             scales = "free_y",  # Allow different y-axis scales for each metric
             ncol = 2) +        # Place plots side by side
  labs(title = "Distribution Metrics Comparison",
       x = "Method",
       y = "Metric Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 12, face = "bold"),
        strip.background = element_rect(fill = "#E5E9F0", color = NA),
        panel.spacing = unit(2, "lines"))  # Add some space between the facets

# Print all plots
print(p1)
print(p2)
print(p3)
print(p4)
print(p5)
```

## Imputation Performance Comparison

## Computational Performance

# Conclusions and Recommendations

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References

```{r}
citation("pROC")
```

- ...

