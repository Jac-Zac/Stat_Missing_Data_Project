---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: cosmo
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
    wrap: 72
---

```{r, echo=F, class.source = 'fold-show'}
# Load utilities
source("setup.R") 

# Load the initial analysis file
child = here::here("dataset_analysis", "synthetic_data_analysis.Rmd")
```

```{r, child = "dataset_analysis/synthetic_data_analysis.Rmd", cache = TRUE}
```

# Imputation Methods

#### NOTES:

For each of the inputation methods, we can test the performances of different approaches.
Perhaps doing a function to test them all, and we can test this with different simulated datasets.
Then we can test the best performing one on a real dataset and discuss.

Perhaps we can also showcase how missing data can lead to inaccurate conclusions when handled poorly.

**BTW I would ignore timeseries data**

**Test also datasets with outliers perhaps to see if this is problematic**

Should we do some testing also on the fact that the type of missing data mechanism might not be known in a real-world scenario ? Sensitivity analyses ? I think this is a bit beyond the scope of what we are doing.

*Inputation for model enanchment is also outside the scope I belive*

#### Testing the different performance metric:

Explain a bit the difference between the metric for imputation. And perhaps make plot to show the

**Pareto front ?**

> I would suggest providing a summary for each of the imputation methods.

## Imputation Methods

The following imputation methods are tested on both the MCAR and MAR datasets. A regression model is fit on the imputed dataset, and the results are summarized.

### 1. Listâ€‘wise (Case) Deletion

List-wise deletion removes any case with missing values from analysis and is the default method in many statistical software tools. It works well when data is large and missing completely at random (MCAR). However, if these conditions are not met, it can lead to bias and loss of important information, making it a less suitable approach.

```{r}
# Evaluate imputation method using listwise deletion
listwise_results <- evaluate_imputation_method(listwise_deletion, data_mcar, data_mar, train_index, synthetic_data)
```


### 2. Pairwise Deletion

Pairwise deletion minimizes information loss compared to list-wise deletion by only removing data points when necessary to check if missing values are truly missing. It produces lower bias for data missing completely at random (MCAR) or missing at random (MAR).

```{r}
# Evaluate imputation method using pairwise deletion
pairwise_results <- evaluate_imputation_method(pairwise_deletion, data_mcar, data_mar, train_index, synthetic_data)
```

### 3. Simple Imputation

Simple imputation replaces missing values using the mean, median, or mode of non-missing data, offering simplicity and ease of use. However, it can introduce bias or unrealistic results, especially in high-dimensional or large-scale data sets, making it unsuitable for handling big data effectively.

##### Mean Imputation

```{r}
# Evaluate imputation method using mean imputation
mean_results <- evaluate_imputation_method(simple_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

##### Median Imputation

```{r}
# Evaluate imputation method using median imputation
median_results <- evaluate_imputation_method(simple_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

### 4. Regression Imputation

Regression imputation replaces missing values using predictions from a regression model built on complete data, preserving sample size and assuming data is missing at random (MAR). It includes single and multivariate regression methods, depending on the number of missing variables. 

```{r}
# Evaluate imputation method using regression imputation
regression_results <- evaluate_imputation_method(regression_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

### 5. Hot-deck Imputation

Hot-deck imputation replaces missing values by selecting a donor from similar cases with complete data, either randomly or based on the closest match. It is widely used because it preserves data structure, reduces bias, and avoids model dependency. However, it lacks a robust theoretical foundation compared to other imputation techniques. 

```{r}
# Evaluate imputation method using hot-deck imputation
hotdeck_results <- evaluate_imputation_method(hot_deck_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

### 6. Expectation-Maximization (EM)

The expectation-maximization (EM) algorithm iteratively handles missing data using an "impute, estimate, and iterate" process until convergence. It alternates between the expectation step, estimating missing values based on observed data, and the maximization step, optimizing the likelihood of the complete data. It requires costly matrix computations.

```{r}
# Evaluate imputation method using EM imputation
em_results <- evaluate_imputation_method(em_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

### 7. Multiple Imputation

Multiple imputation addresses missing data by generating multiple complete datasets using observed data distributions to estimate missing values, reflecting uncertainty. Analysis is performed on each dataset, and results are combined into a single estimate. This technique overcomes single imputation limitations, reducing bias and providing more reliable results. 

### 8. GAM Imputation

GAM-based imputation leverages Generalized Additive Models to handle missing data, allowing for flexible modeling of relationships between variables. This technique is particularly effective when the relationship between predictors and the target variable is non-linear.

```{r}
# Evaluate imputation method using GAM-based imputation
gam_results <- evaluate_imputation_method(gam_based_imputation, data_mcar, data_mar, train_index, synthetic_data)
```

# Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

## Decision tree / Random Forests

Decision tree imputation builds a predictive model for each variable with missing data by constructing a decision tree where branches represent decisions based on predictor variables, and leaves represent the predicted values. This method can handle both categorical and numerical data while capturing complex interactions and patterns in the data.

```{r}
# Evaluate imputation method using tree-based (Random Forest) imputation
tree_results <- evaluate_imputation_method(tree_based_imputation, data_mcar, data_mar, train_index, synthetic_data)
```


## Ensemble methods

# Results and Discussion

## Performance metrics

- **Mean Absolute Error (MAE)**

    MAE measures the average difference between imputed values and true values defined as:
    
    $$
    MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
    $$

- **Mean Squared Error (MSE)**
    
    While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
    
    $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
    $$

- **Root Mean Square Error (RMSE)**

    RMSE computes the difference in imputed values and actual values as follows:
  
    $$
    RMSE = \sqrt{MSE}
    $$
    
- **Area under the curve (AUC)**

    AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
    
    $$
    TPR = \frac{TP}{TP + FN} \tag{21}
    $$
    
    $$
    FPR = \frac{FP}{FP + TN} \tag{22}
    $$
    
#### Function implementation for different performance metrics
> They are defined inside metrics
    
```{r}
# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```


### Comparison of Results

Create a summary table and plots to compare RMSE, mean absolute differences across all methods 

```{r, echo = T}
# Create results dataframe with RMSE
results <- data.frame(
  Method = c("Listwise", "Pairwise", "Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree"),
  RMSE_Prediction = c(
    listwise_results$rmse_mcar, 
    pairwise_results$rmse_mcar, 
    mean_results$rmse_mcar, 
    median_results$rmse_mcar, 
    regression_results$rmse_mcar, 
    hotdeck_results$rmse_mcar, 
    em_results$rmse_mcar, 
    gam_results$rmse_mcar, 
    tree_results$rmse_mcar
  )
)

# Create comparison results dataframe with safe extraction for MAE, RMSE, and Correlation
comparison_methods <- c("Mean", "Median", "Regression", "Hot-deck", "EM", "GAM", "Tree")

# Ensure you include the corresponding differences for Tree-based methods as well
comparison_diffs <- list(mean_results$diff_mcar, 
                         median_results$diff_mcar, 
                         regression_results$diff_mcar, 
                         hotdeck_results$diff_mcar, 
                         em_results$diff_mcar, 
                         gam_results$diff_mcar, 
                         tree_results$diff_mcar)

# Custom function to safely extract values from the list of metrics
safe_extract <- function(metric_list, metric_name) {
  if (!is.null(metric_list) && metric_name %in% names(metric_list)) {
    if (length(metric_list[[metric_name]]) > 0) {
      return(metric_list[[metric_name]][1])  # Extract the first value
    } else {
      return(NA)  # If the metric is an empty vector, return NA
    }
  } else {
    message(paste("Warning: Metric", metric_name, "not found in metric_list. Returning NA."))
    return(NA)
  }
}

# Extract metrics for comparison
comparison_results <- data.frame(
  Method = comparison_methods,
  MAE = sapply(comparison_diffs, safe_extract, "mae"),
  RMSE = sapply(comparison_diffs, safe_extract, "rmse"),
  Correlation = sapply(comparison_diffs, safe_extract, "correlation")
)

# Remove any rows where all metrics are NA
comparison_results <- comparison_results[rowSums(is.na(comparison_results[,-1])) < ncol(comparison_results)-1, ]
```

### Plot the difference metrics between datasets

```{r}
# Plot 1: RMSE of Prediction Models
p1 <- create_bar_plot(results, "reorder(Method, RMSE_Prediction)", "RMSE_Prediction", "#88C0D0", 
                      "Prediction Model Performance")
print(p1)

# Plot 2: MAE
p2 <- create_bar_plot(comparison_results, "reorder(Method, MAE)", "MAE", "#A3BE8C", 
                      "Mean Absolute Error")
print(p2)

# Plot 3: RMSE Comparison
p3 <- create_bar_plot(comparison_results, "reorder(Method, RMSE)", "RMSE", "#EBCB8B", 
                      "Root Mean Square Error")
print(p3)

# Plot 4: Correlation
p4 <- create_bar_plot(comparison_results, "reorder(Method, Correlation)", "Correlation", "#B48EAD", 
                      "Correlation with Original Data")
print(p4)
```

## Imputation Performance Comparison

## Computational Performance

# Conclusions and Recommendations

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References

```{r}
citation("pROC")
```

- ...

