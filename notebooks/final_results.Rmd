---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
editor_options:
  markdown:
    wrap: 72
---

# Missing data study

### Environment Setup
> Everything is defined inside `setup.R`

```{r, echo = T}
# Load utilities
suppressMessages(library(here))

# Load utilities
source(here("src", "setup.R"))
```

## Missing value patterns

In statistical analysis, understanding the patterns of missing data is crucial for selecting appropriate handling methods. The primary patterns are:

**Univariate Missing Data**: Occurs when only a single variable has missing values. This pattern is uncommon across most disciplines and typically arises in experimental studies. 

**Monotone Missing Data**: Characterized by a situation where the missingness of one variable implies the missingness of all subsequent variables. This pattern is often observed in longitudinal studies where participants drop out and do not return. The monotone pattern is generally easier to manage, as the missingness follows a clear, observable sequence. 

**Non-Monotone Missing Data**: Occurs when the missingness of one variable does not predict the missingness of other variables. This pattern is more complex and requires careful analysis to handle appropriately. 

Recognizing these patterns helps in choosing suitable methods for handling missing data, thereby improving the robustness and validity of statistical analyses.


##  Missing value generation mechanisms
Another way to classify missing values is based on their underlying mechanism.

Rubin's missing data theory introduces a dataset $Y$ that is partitioned into observed values ($Y_o$) and missing values ($Y_m$). The presence or absence of data is tracked by an indicator matrix $R$ defined for each element of $Y$ as:

$$
R = \begin{cases}
0 & \text{if } Y \text{ is observed} \\
1 & \text{if } Y \text{ is missing}
\end{cases}
$$

**Missing Completely at Random (MCAR)** defines a mechanism where the probability of missingness $P(R|q)$ has no relationship with any values in the dataset. Implementation can be univariate, where missing values are inserted through random selection or Bernoulli trials with probability $p$, or multivariate, where missing values are distributed either uniformly or randomly across the entire dataset.

**Missing At Random (MAR/CAR)** occurs when missingness probability $P(R|Y_o, q)$ depends on observed data patterns but not on missing values. For univariate cases, missing values are determined by the patterns of observed data, such as rank-based probabilities or percentile thresholds. In multivariate cases, missingness arises due to relationships or correlations between feature pairs. 

**Missing Not at Random (MNAR)**: Missingness depends on both the observed and unobserved data $p(R|Y_0,Y_m,q$, making it difficult to model and handle. Identifying and managing MNAR data requires complex assumptions about the relationship between missing and observed values.

This study focuses on Missing at Random (MAR) data due to its practical relevance and analytical tractability compared to Missing Completely at Random (MCAR) and Missing Not at Random (MNAR).

We exclude Missing Completely at Random (MCAR) data because its randomness preserves the dataset’s underlying distribution, making missingness equivalent to reduced sample size without introducing bias. Handling MNAR (Missing Not at Random) data is beyond this study’s scope because identifying and modeling its dependence on unobserved variables is often ambiguous and requires unverifiable assumptions.

### Syntetich MAR mechanism

> cite paper

Let's create a simple dataset to demonstrate the different mechanisms of missing values and explore various ways to visualize them.

```{r, echo=T, class.source = 'fold-show'}
synthetic_data <- synthetic_dataset_gen(n_samples = 500, 
                                        n_covariates = 5, 
                                        correlation = "none", 
                                        target_type = "linear", 
                                        noise_level = 0.5)
```

#### Censoring method

The `delete_MAR_censoring` function introduces missing values in a dataset based on a MAR mechanism. Missingness depends on values in a **control column** (`cols_ctrl`). Censoring mechanism:

- Sorting: Rows are sorted by the control column, and missing values are added to the smallest, largest, or both extremes.
- Quantile: Missing values are based on calculated quantiles of the control column, without sorting.


```{r}
data.MAR.uni <- delete_MAR_censoring(synthetic_data, p = 0.1, cols_mis = c('X1'), cols_ctrl = c('X2'))
aggr(data.MAR.uni, sortVars=TRUE, plot = TRUE, numbers = TRUE , labels=names(synthetic_data), col = nord_contrast)

# Create a new dataset indicating missing status
intersection_dataset <- synthetic_data
intersection_dataset$missing <- ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")

plot_missing_data(data.MAR.uni, "MAR uni", c("white", red_nord))
```

```{r, fig.width=10, fig.height=10}
# Calculate global ranges for X1 and all Y variables (X2-X5)
x_range <- range(synthetic_data$X1, na.rm = TRUE)
y_range <- range(synthetic_data[,paste0("X", 2:5)], na.rm = TRUE)

# Add padding to ranges
x_pad <- diff(x_range) * 0.05
y_pad <- diff(y_range) * 0.05

# Plot for X1 vs X2
plot_data2 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X2,
  missing = ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")
)

scatter_plot2 <- ggplot(plot_data2, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X2") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot2 <- ggMarginal(scatter_plot2, margins = "y", 
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Plot for X1 vs X3
plot_data3 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X3,
  missing = ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")
)

scatter_plot3 <- ggplot(plot_data3, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X3") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot3 <- ggMarginal(scatter_plot3, margins = "y",
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Plot for X1 vs X4
plot_data4 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X4,
  missing = ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")
)

scatter_plot4 <- ggplot(plot_data4, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X4") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot4 <- ggMarginal(scatter_plot4, margins = "y",
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Plot for X1 vs X5
plot_data5 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X5,
  missing = ifelse(is.na(data.MAR.uni$X1), "Missing", "Present")
)

scatter_plot5 <- ggplot(plot_data5, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X5") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot5 <- ggMarginal(scatter_plot5, margins = "y",
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Arrange the plots in a 2x2 grid
grid.arrange(final_plot2, final_plot3, final_plot4, final_plot5, ncol = 2)

```


#### Likelihood method

The `delete_MAR_1_to_x()` function in R creates MAR values by controlling missingness in one column (`cols_mis`) based on the values of another column (`cols_ctrl`). It splits the data into two groups using a cutoff value (e.g., median) in `cols_ctrl`. Group 1 contains rows below the cutoff, and group 2 contains rows above. The `x` parameter sets the odds ratio of missing data: for `x = 3`, group 2 is **3 times** more likely to have missing values than group 1.

```{r}
data.MAR.multi <- delete_MAR_1_to_x(synthetic_data, 
                                    p = 0.2, 
                                    cols_mis = c('X1'), 
                                    cols_ctrl = c('X3'), 
                                    x = 50)

data.MAR.multi <- delete_MAR_1_to_x(data.MAR.multi, 
                                    p = 0.2, 
                                    cols_mis = c('X1'), 
                                    cols_ctrl = c('X4'), 
                                    x = 50)

data.MAR.multi <- delete_MAR_1_to_x(data.MAR.multi, 
                                    p = 0.2, 
                                    cols_mis = c('X2'), 
                                    cols_ctrl = c('X5'), 
                                    x = 50)

par(mfcol=c(2, 2))
aggr(data.MAR.multi, 
     sortVars=TRUE, 
     plot = TRUE, 
     numbers = TRUE , 
     labels=names(synthetic_data), 
     col = nord_contrast)

plot_missing_data(data.MAR.multi, "MAR multi", c("white", red_nord))
```


```{r, fig.width=10, fig.height=10}
# Calculate global ranges for X1 and all Y variables (X2-X5)
x_range <- range(synthetic_data$X1, na.rm = TRUE)
y_range <- range(synthetic_data[,paste0("X", 2:5)], na.rm = TRUE)

# Add padding to ranges
x_pad <- diff(x_range) * 0.05
y_pad <- diff(y_range) * 0.05

# Plot for X1 vs X2
plot_data2 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X2,
  missing = ifelse(is.na(data.MAR.multi$X1), "Missing", "Present")
)

scatter_plot2 <- ggplot(plot_data2, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X2") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot2 <- ggMarginal(scatter_plot2, margins = "y", 
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Plot for X1 vs X3
plot_data3 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X3,
  missing = ifelse(is.na(data.MAR.multi$X1), "Missing", "Present")
)

scatter_plot3 <- ggplot(plot_data3, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X3") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot3 <- ggMarginal(scatter_plot3, margins = "y",
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Plot for X1 vs X4
plot_data4 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X4,
  missing = ifelse(is.na(data.MAR.multi$X1), "Missing", "Present")
)

scatter_plot4 <- ggplot(plot_data4, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X4") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot4 <- ggMarginal(scatter_plot4, margins = "y",
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Plot for X1 vs X5
plot_data5 <- data.frame(
  X1 = synthetic_data$X1,
  Y = synthetic_data$X5,
  missing = ifelse(is.na(data.MAR.multi$X1), "Missing", "Present")
)

scatter_plot5 <- ggplot(plot_data5, aes(x = X1, y = Y, color = missing)) +
  geom_point() +
  scale_color_manual(values = c("Missing" = red_nord, "Present" = "black")) +
  scale_x_continuous(limits = c(x_range[1] - x_pad, x_range[2] + x_pad)) +
  scale_y_continuous(limits = c(y_range[1] - y_pad, y_range[2] + y_pad)) +
  theme_minimal() +
  labs(color = "Data Status", x = "X1", y = "X5") +
  theme(legend.position = "none") +
  coord_fixed()

final_plot5 <- ggMarginal(scatter_plot5, margins = "y",
                         groupColour = TRUE, groupFill = TRUE,
                         type = "boxplot", size = 10, width = 0.15, outlier.size = 1)

# Arrange the plots in a 2x2 grid
grid.arrange(final_plot2, final_plot3, final_plot4, final_plot5, ncol = 2)

```


## Performance metrics between Datasets
> Since we possess the **original dataset prior to the introduction of missing values**, we will introduce _two metrics_ to compare _the distributions of two datasets_: the **original dataset** without missing values and the dataset in which missing values have been **imputed** using various techniques

We indeed care about comparing the distribution of the two datasets more than comparing pointwise the actual reconstruction of the dataset.

### The Kantorovich Problem

The Kantorovich problem calculates the distance between two probability measures.

**Definition:**
let \((X, \mathcal{X})\) and \((Y, \mathcal{Y})\) be two Polish spaces, that is, complete and separable metric spaces. 
Consider two probability measures \(P\) and \(Q\) belonging to the set of probability measures on \(X\) and \(Y\), 
respectively denoted by \(\mathcal{P}(X)\) and \(\mathcal{P}(Y)\). 

We define the set

$$
\Pi(P,Q) := \bigl\{\pi \in \mathcal{P}(X \times Y) : 
\pi(X \times \cdot) = P(\cdot),\ \pi(\cdot \times Y) = Q(\cdot)\bigr\},
$$

which is the class of probability measures on \((X \times Y,\, \mathcal{X} \times \mathcal{Y})\) 
whose marginals are \(P\) and \(Q\). The elements \(\pi\) of this class are called transport plans.

All elements of \(\Pi\) are probability measures that can be interpreted as ways to transfer the "mass" from \(P\) to \(Q\). 

However, moving this "mass" has a cost, defined by the cost function

$$
c(\cdot,\cdot) : X \times Y \to \mathbb{R}.
$$

Then, once a transport plan \(\pi \in \Pi\) is fixed, we define

$$
I_c(\pi) := \int_{X \times Y} c(x,y)\,\pi(dx\,dy),
$$

which represents the average cost of transferring \(P\) to \(Q\), associated with that particular transport plan. 

We can therefore formulate the Kantorovich problem as follows:

$$
K_c(P,Q) := \inf_{\pi \in \Pi} \int_{X \times Y} c(x,y)\,\pi(dx\,dy).
$$

Having defined the Kantorovich problem, we now assume that \(X = Y\) and let \(d\) be a distance on \(X\).  
We can then define the Wasserstein distance of order \(p\), denoted \(W_p\), as follows:

$$
W_p(P,Q) 
:= K_{d^p}(P,Q)^{1/p} 
= \biggl(\inf_{\pi \in \Pi(P,Q)} \int_{X^2} d^p(x,y)\,\pi(dx\,dy)\biggr)^{\!\!1/p}.
$$

The difference between the Kantorovich definition and the Wasserstein definition lies in the cost function, 
which is given by \(d(x,y)^p\), where \(d(\cdot,\cdot)\) is a distance on \(X\). It is precisely this definition 
of the cost function that guarantees, via the following proposition, that \(W_p\) is indeed a distance.

**Proposition**  
Assume \(X = Y\) and that for \(p > 1\), \(c(x,y) = d(x,y)^p\), where \(d\) is a distance on \(X\), i.e.:

1. \(d(x,y) = d(y,x) \geq 0\);
2. \(d(x,y) = 0\) if and only if \(x = y\);
3. \(\forall (x,y,z) \in X^3,\ d(x,z) \le d(x,y) + d(y,z)\).

Then \(W_p\) is a distance, namely it is symmetric, positive, satisfies \(W_p(P,Q) = 0\) 
if and only if \(P = Q\), and fulfills the triangle inequality:

$$
\forall (P,Q,Z) \in \mathcal{P}(X)^3 \quad 
W_p(P,Z) \le W_p(P,Q) + W_p(Q,Z).
$$

#### The Discrete Case

The Kantorovich problem can also be expressed in discrete form.

**Definition:** suppose \(X = \{ x_1, \dots, x_n \}\) and \(Y = \{ y_1, \dots, y_m \}\). 
In this case, \(P\) and \(Q\) can be identified with two probability vectors 
\(a \in \Sigma_n\) and \(b \in \Sigma_m\), where

$$
\Sigma_n 
= \bigl\{ a \in \mathbb{R}_+^n : \sum_{i=1}^n a_i = 1 \bigr\}.
$$

These probability vectors can be thought of as histograms, in which 
each \(a_i\) represents the probability associated with \(x_i\), and each \(b_j\) 
represents the probability associated with \(y_j\). 

We can define the discrete analogue of the set of probability measures 
\(\Pi(P,Q)\) as follows:

$$
U(a, b) := \bigl\{\, P \in \mathbb{R}_+^{n \times m} : 
P\,\mathbf{1}_m = a,\; P^\top \mathbf{1}_n = b \bigr\},
$$

where \(P\) is the transport matrix, the discrete analogue of 
the transport plan \(\pi\). In other words, 
\(P_{i,j}\) tells us how much mass to move from the point \(x_i\) to the point \(y_j\).

From bin \(i\) of \(a\) to bin \(j\) of \(b\). The constraints related to the set \(U(a,b)\) 
are similar to those of \(\Pi\), meaning that the sum of the rows or columns 
returns \(a\) or \(b\), so that \(P\,\mathbf{1}_m = a = \sum_{j=1}^m P_{i,j}\) 
for each \(i \in \{1,\dots,n\}\) and \(P^\top \mathbf{1}_n = \sum_{i=1}^n P_{i,j} = b_j\) 
for each \(j \in \{1,\dots,m\}\).

The cost function \(\mathbf{c}\) is represented by a matrix \(\mathbf{C} \in \mathbb{R}^{n \times m}\), 
where each \(C_{i,j} = c(x_i, y_j)\) explains the cost of going from \(a_i\) to \(b_j\). 
The average cost analogous to \(I_c\) in the discrete case is defined as

$$
\bar{I}_c 
= \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

We can then formulate the discrete Kantorovich problem as follows:

$$
L_C(a,b) 
:= \min_{P \in U(a,b)} \sum_{i,j} C_{i,j} \, P_{i,j}.
$$

As in the continuous case, assuming \(X, Y \subset E\), with \(E\) a metric space 
equipped with a distance \(d\), we can also define the Wasserstein distance 
of order \(p\) in the discrete setting:

$$
W_p(a,b) 
:= \min_{P \in U(a,b)} 
\biggl(\sum_{i,j} D_{i,j}^p \, P_{i,j}\biggr)^{\!\!1/p}.
$$

where \(D_{i,j}^p = d^p(x_i, y_j)\) is the matrix of distances between the points \(x_i\) and \(y_j\).  
If \(X,Y \subset \mathbb{R}^n\), a standard example of distance between two points in the discrete case 
is \(D_{i,j} = \|x_i - y_j\|\), where \(\|\cdot\|\) is the Euclidean norm.

In the Wasserstein case, we consider the cost matrix \(\mathbf{C} = \mathbf{D}\), 
with \(\mathbf{D}\) being the matrix of Euclidean distances. 

### Jensen–Shannon Divergence

The Jensen–Shannon divergence (JSD) is a measure of similarity (or dissimilarity) between two probability distributions. 
**Definition:** let \(P\) and \(Q\) be two discrete probability distributions over the same domain \(\Omega\). Define

\[
M = \tfrac{1}{2}\,(P + Q),
\]

which is the average (or midpoint) distribution of \(P\) and \(Q\). Then the Jensen–Shannon divergence between \(P\) and \(Q\) is given by

\[
\mathrm{JSD}(P \,\|\, Q) 
= \tfrac{1}{2}\,\mathrm{KL}(P \,\|\, M)
+ \tfrac{1}{2}\,\mathrm{KL}(Q \,\|\, M),
\]

where \(\mathrm{KL}(\cdot \,\|\, \cdot)\) denotes the KL divergence, defined by:

\[
\mathrm{KL}(P \,\|\, Q) 
= \sum_{x \in \Omega} P(x) \,\log\!\Bigl(\tfrac{P(x)}{Q(x)}\Bigr).
\]

**Properties:**

1. **Symmetry**  
   \[
   \mathrm{JSD}(P \,\|\, Q) = \mathrm{JSD}(Q \,\|\, P).
   \]
   This follows because \(M\) is symmetric in \(P\) and \(Q\).

2. **Non-negativity**  
   \[
   \mathrm{JSD}(P \,\|\, Q) \ge 0,
   \]
   with equality if and only if \(P = Q\).

3. **Boundedness**  
   \[
   0 \,\le\, \mathrm{JSD}(P \,\|\, Q) \,\le\, \log(2).
   \]
   The maximum value \(\log(2)\) occurs when \(P\) and \(Q\) have disjoint supports.

4. **Metric Property**  
   Taking the square root gives a metric:
   \[
   d_{\mathrm{JSD}}(P, Q) := \sqrt{\mathrm{JSD}(P \,\|\, Q)}.
   \]
   This metric satisfies the triangle inequality.


```{r}
# Define the child RMarkdown file path
part_1 <- here("notebooks", "partial_analyses", "part_1.Rmd")
```

```{r, child = here("notebooks", "partial_analyses", "part_1.Rmd"), cache = TRUE}
# Run the file which does data analyses even if not knitted by runned manually
rmarkdown::render(part_1)
```

# Temporary stop the knit process here
# The second part will be integrated here

```{r}
knitr::knit_exit()
```

```{r}
# Define the child RMarkdown file path
part_2 <- here("notebooks", "partial_analyses", "part_2.Rmd")
```

```{r, child = here("notebooks", "partial_analyses", "part_2.Rmd"), cache = TRUE}
rmarkdown::render(part_2)
```

# Conclusions and Recommendations

> Perhaps some final plots and small discussions

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References
> Need to add references

```{r}
citation("pROC")
```

- ...
