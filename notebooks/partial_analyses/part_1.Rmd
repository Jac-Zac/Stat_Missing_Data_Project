---
title: "Synthetic Data Analysis"
author: "Jacopo Zacchigna, Devid Rosa, Cristiano Baldassi, Ludovica Bianchi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true 
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: flatly
    number_sections: true
    toc_collapsible: trueÂ  # Enable collapsible TOC
---

```{r, echo = F}
# Load utilities
suppressMessages(library(here))

# Load utilities
source(here("src", "setup.R"))
```


# Imputation techiques

This section reviews imputation techniques for handling missing data, from simple deletion methods to advanced machine learning approaches, highlighting their uses and impact on data reliability.

| **Imputation Technique**         | **Description**                                 | **Strengths**                               | **Limitations**                                |
|-----------------------------------|-------------------------------------------------|---------------------------------------------|------------------------------------------------|
| **List-wise Deletion**            | Removes cases with missing values.              | Simple; works for MCAR data.                | Causes data loss and bias if MCAR is violated. |
| **Pairwise Deletion**             | Removes data points only when necessary.        | Retains more data; less bias for MCAR/MAR.  | Inconsistent sample sizes; potential bias.     |
| **Simple Imputation**             | Replaces missing values with mean, median, or mode. | Easy and efficient.                         | Can distort data and introduce bias.           |
| **Regression Imputation**         | Predicts missing values using regression.      | Preserves sample size; good for MAR.        | Assumes linearity; less effective for non-linear data. |
| **Hot-deck Imputation**           | Uses similar cases to fill missing values.      | Avoids model assumptions.                  | Less reliable for small datasets.              |
| **Expectation-Maximization (EM)** | Iteratively estimates missing values.          | Accurate imputations; handles MAR well.     | Computationally intensive.                    |
| **Multiple Imputation**           | Generates multiple datasets to reflect uncertainty. | Reduces bias; more robust results.          | Time-consuming; complex to combine results.    |
| **GAM Imputation**                | Uses Generalized Additive Models for non-linear data. | Flexible for non-linear data.              | More complex and computationally expensive.    |
| **Decision Tree / Random Forest** | Builds models using decision trees.            | Captures complex patterns; handles various data types. | Computationally heavy; prone to overfitting.   |
| **K-Nearest Neighbors (KNN)**     | Imputes based on nearest neighbors.            | Flexible and captures patterns.             | High computational cost for large datasets.    |                                                                                                                   |

## Visualization of the different techiques

The following paragraph presents visualizations of various imputation techniques applied to different types of datasets to observe their impact. The reconstructed datasets are then evaluated and compared using two distribution-based metrics.

```{r,  class.source = 'fold-show'}
# Define methods with their display names and corresponding functions
imputation_methods <- list(
  "Mean Imputation" = function(data) { simple_imputation(data, "mean") },
  # Return only necessary collums
  "Hot Deck" = function(data) { hot_deck_imputation(data) },
  # "EM" = function(data) { impute_EM(data) },
  "KNN Imputation" = function(data) { imputed_data <- kNN(data, imp_var = FALSE)},
  "Regression" = function(data) { regression_imputation(data) },
  "Regression with Noise" = function(data) { regression_imputation(data, noise = TRUE) },
  "Regression with Noise Emp" = function(data) { regression_imputation_emp(data, noise = TRUE) },
  "GAM" = function(data) { gam_based_imputation(data) },
  "GAM with Noise" = function(data) { gam_based_imputation(data, noise = TRUE) },
  "Random Forest" = function(data) { tree_based_imputation(data) },
  "Random Forest with Noise" = function(data) { tree_based_imputation(data, noise=TRUE) }
)
```


### Linear Relationship

We begin by testing the imputation techniques on data characterized by a linear relationship and heteroscedasticity. Methods such as regression imputation are expected to perform well in this scenario, as they are designed to capture and model linear patterns within the data.

#### Analyses of noise in regression imputation

We begin by demonstrating how various noise strategies can be incorporated into the imputation methods, as this technique will be applicable in future scenarios as well.

```{r, fig.width=18, fig.height=10}
# Generate and create missing data
n <- 200
p <- 0.3
linear_data <- generate_data(n, "linear",x_range = c(0, 1),  noise_sd = 0.5, 
                             homoscedasticity = F, min_sd_noise = 0.1)

# linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 100)
# linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)
linear_missing <- delete_MAR_1_to_x(linear_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

noise_imputation_methods <- list(
  # "Regression" = function(data) { regression_imputation(data) },
  "Regression with Noise" = function(data) { regression_imputation(data, noise = TRUE) },
  "Regression with Noise Emp" = function(data) { regression_imputation_emp(data, noise = TRUE) }
)

# Use the new combined analysis function
noise_result <- plot_imputations_and_metrics(original_data = linear_data, missing_data = linear_missing, imputation_methods = noise_imputation_methods)

# Access the list of individual plots
noise_plots <- noise_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = noise_plots, ncol = 2)
```

```{r}
# Custom method registration
# Ensure the custom function is correctly named
mice.impute.custom_regression <- custom_regression_impute
# Register the custom imputation function
mice.impute.gam <- mice_impute_gam

# norm.predict: Deterministic regression imputation
# norm: Deterministic regression imputation

# Perform multiple imputation
# imputed_data <- mice(linear_missing, method ="custom_regression", m = 5)
imputed_data <- mice(linear_missing, method ="gam", m = 5)

# Extract completed data (first imputed dataset)
completed_data <- complete(imputed_data, action = 1)

# Identify observed and imputed values for x2
is_imputed <- is.na(linear_missing$x2)
observed_values <- completed_data[!is_imputed, ]
imputed_values <- completed_data[is_imputed, ]

# Custom plot
plot(completed_data$x1, completed_data$x2, col = ifelse(is_imputed, red_nord, "black"),
     pch = 19, xlab = "x1", ylab = "x2",
     main = "Imputed Values (Red) vs Observed Values (Black)")
legend("topright", legend = c("Observed", "Imputed"), col = c("black", red_nord), pch = 19)

```

#### Comparison of the different techniques for the simple dataset

```{r, fig.width=18, fig.height=20}
# Use the new combined analysis function
linear_result <- plot_imputations_and_metrics(original_data = linear_data, missing_data = linear_missing, imputation_methods = imputation_methods)

# Access the list of individual plots
imputation_plots <- linear_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- linear_result$metrics_plot
print(metrics_plot)
```


### Quadratic Relationship

The second dataset is constructed with a quadratic relationship and an unbalanced number of data points. In cases involving quadratic relationships, we anticipate that more flexible imputation methods, such as Generalized Additive Models (GAM) and Random Forest, will outperform simple linear regression due to their ability to capture complex, non-linear patterns in the data.

```{r, fig.width=18, fig.height=20}
# Generate and create missing data
quad_data <- generate_data(n, "quadratic",x_range = c(-2, 2), noise_sd = 1, balanced = F, alpha = 1, beta = 2)
quad_missing <- delete_MAR_1_to_x(quad_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

quad_result <- plot_imputations_and_metrics(quad_data, quad_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- quad_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- quad_result$metrics_plot
print(metrics_plot)
```

### Piecewise

The third dataset is designed with a piecewise relationship, where distinct segments of the data follow different patterns. For this type of distribution, we expect imputation methods capable of handling discontinuities and local variations, such as Random Forest and k-Nearest Neighbors, to perform better than global modeling approaches like linear regression, which may struggle to accurately capture the segmented structure of the data.

```{r, fig.width=18, fig.height=20}
# Generate and create missing data
piecewise_data <- generate_data(n, "piecewise", noise_sd = 2)
piecewise_missing <- delete_MAR_1_to_x(piecewise_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

piecewise_result <- plot_imputations_and_metrics(piecewise_data, piecewise_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- piecewise_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- piecewise_result$metrics_plot
print(metrics_plot)
```

### Logarithmic Relationship

The final dataset follows a logarithmic relationship, providing insight into how different imputation methods handle non-linear but monotonic patterns.

```{r, fig.width=18, fig.height=20}
# Generate and create missing data
log_data <- generate_data(n, "log", x_range = c(1, 100), noise_sd = 1)
log_missing <- delete_MAR_1_to_x(log_data, p, cols_mis = "x2", cols_ctrl = "x1", x = 10)

log_result <- plot_imputations_and_metrics(log_data, log_missing, imputation_methods)

# Access the list of individual plots
imputation_plots <- log_result$imputation_plots

# Arrange and display them as needed
grid.arrange(grobs = imputation_plots, ncol = 2)
```

```{r, fig.width=18, fig.height=12}
# Access the metrics plot
metrics_plot <- log_result$metrics_plot
print(metrics_plot)
```

### Conclusions

The visualizations demonstrate that:

1. Simple mean imputation tends to perform poorly as it ignores the relationship between variables
2. kNN and hot deck methods can work well when there are similar cases nearby
3. Regression imputation works best for linear relationships
4. GAM and Random Forest methods are more flexible and can handle non-linear relationships better
5. The choice of imputation method should depend on the underlying relationship between variables
