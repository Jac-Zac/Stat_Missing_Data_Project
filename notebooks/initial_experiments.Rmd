---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: cosmo
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
  word_document:
    toc: true
    toc_depth: '3'
  pdf_document:
    toc: true
    toc_depth: 3
editor_options:
  markdown:
    wrap: 72
---

## Environment Setup 

```{r, echo=F}
## Install requirements
# install.packages("ggplot2")
# install.packages("GGally")
# install.packages("reshape2")
# install.packages("corrplot")
# install ...
# install.packages("pROC")

# Inputation methods
# install.packages("mice")
# Other alternative
# install.packages("VIM") # (Visualization and Imputation of Missing Values) 
```

```{r, echo=T, class.source = 'fold-show'}
## Load requirements 
library(ggplot2)
suppressMessages(library(GGally))
suppressMessages(library(pROC))
suppressMessages(library(corrplot))
suppressMessages(library(reshape2))
suppressMessages(library(mice))
suppressMessages(library(VIM))
suppressMessages(library(RColorBrewer))

# Load utilities
source("../src/synthetic_data.R") 
source("../src/missing_data.R") 
source("../src/inputation_methods.R") 
source("../src/metrics.R") 
source("../src/utils.R") 

# Set seed (also essential for the syntethic dataset generation)
set.seed(42)
```

### Missing data functions

Showcase Introduce Mar function (example to show how to use print_function)

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(introduce_mcar)
```

# Data Generation and Exploration

## Synthetic Dataset Creation

```{r, echo=F, class.source = 'fold-show'}
# This will create:
# - 5 continuous variables
# - 3 categorical variables with 3, 4, and 5 levels respectively
data <- generate_random_data(n_samples = 1000, 
                           n_continuous = 5, 
                           n_categorical = 3,
                           n_categories = c(3, 4, 5))

# Visualize dataset properties
# summary(data)
```

```{r, echo=TRUE, class.source = 'fold-show'}
synthetic_data <- synthetic_dataset_gen(n_samples = 1000, n_covariates = 5, correlation = "linear", target_type = "linear", noise_level = 0.5)

cor_matrix <- cor(synthetic_data)

# Plot the correlation matrix with different upper and lower
corrplot.mixed(cor_matrix, 
    lower = "number", 
    upper = "ellipse",
    addgrid.col = "gray",
    tl.col = "black",
    tl.cex = 0.7,
    # Using color brewer colormap
    # lower.col = brewer.pal(n = 8, name = "RdBu"),  # Change 'RdBu' to your desired palette
    # upper.col = brewer.pal(n = 8, name = "RdBu")   # Change 'RdBu' to your desired palette
)
```

## Data Visualization
> This section should be expanded to explore the dataset more in depth

```{r}
# Plot histogram for each continuous variable
data_continuous <- synthetic_data[, sapply(data, is.numeric)]
continuous_melted <- reshape2::melt(data_continuous)

ggplot(continuous_melted, aes(x = value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Distribution of Continuous Variables", x = "Value", y = "Frequency")
```

```{rm, class.source = 'fold-show'}
# Plot pairwise relationships between continuous variables
# ggpairs(synthetic_data)
```

# Missing Data Analysis

## Introducing Missing Data
> Using our function to artificially create missing data

### Visualization of Missing Patterns

##### Missing data in the original dataset
> Obviously, no missing data is present in our dataset; it is synthetically generated.

```{r}
# Plot missing data patterns in the original dataset
original_missing <- as.data.frame(sapply(data, is.na))
original_missing$index <- 1:nrow(original_missing)
original_missing_melted <- melt(original_missing, id.vars = "index")

ggplot(original_missing_melted, aes(x = index, y = variable, fill = value)) +
  geom_tile() +
  scale_fill_manual(values = c("white", "red")) +
  labs(title = "Missing Data Pattern - Original", x = "Index", y = "Variable")
```

#### Testing different Missing Data Mechanisms

```{r, echo =T, class.source = 'fold-show'}
# Artificially create missing data for all mechanisms (MCAR, MAR, MNAR)
# data_mcar <- introduce_mcar(data, prop_missing = 0.1, missing_cols = c("cont_2", "cont_1"))
# data_mar <- introduce_mar(data, prop_missing = 0.1, predictor_cols = c("cont_1", "cont_3"), target_cols = c("cont_2", "cont_4"))

data_mcar <- introduce_mcar(synthetic_data, prop_missing = 0.1, missing_cols = "X3")
data_mar <- introduce_mar(synthetic_data, prop_missing = 0.1, predictor_cols = c("X1", "X2"), target_cols = c("X3"))

sum(is.na(data_mcar)) # Should be approximately 10%
sum(is.na(data_mar)) # Should be approximately 10%

# Analyze missing patterns and summarize
# print(summarize_missing(data_mcar))
# print(summary(data_mcar))
# print(summarize_missing(data_mar))
# print(summary(data_mar))
```

##### Plot missing data for the different mechanisms

```{r, class.source = 'fold-show'}
## Plot for MCAR
plot_missing_data(data_mcar, "MCAR")

# Plot for MAR
plot_missing_data(data_mar, "MAR")
```

```{r, class.source = 'fold-show'}
md.pattern(data)
# Rotate variable names for better readability
md.pattern(data_mar,  rotate.names = TRUE)
summary(data_mcar)
```

```{r, class.source = 'fold-show'}
# Create a custom color palette (we can choose the color pallet for the entire project if we want)
my_palette <- brewer.pal(3, "Set2")

# For the original data
aggr(data, plot = TRUE, numbers = TRUE, prop = FALSE, col = my_palette)

# For data mcar
aggr(data_mcar, plot = TRUE, numbers = TRUE, prop = FALSE, col = my_palette)

# For data_mar
aggr(data_mar, plot = TRUE, numbers = TRUE, prop = FALSE, col = my_palette)
```

# Imputation Methods

## Performance metrics

- **Mean Absolute Error (MAE)**

    MAE measures the average difference between imputed values and true values defined as:
    
    $$
    MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
    $$

- **Mean Squared Error (MSE)**
    
    While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
    
    $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
    $$

- **Root Mean Square Error (RMSE)**

    RMSE computes the difference in imputed values and actual values as follows:
  
    $$
    RMSE = \sqrt{MSE}
    $$
    
- **Area under the curve (AUC)**

    AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
    
    $$
    TPR = \frac{TP}{TP + FN} \tag{21}
    $$
    
    $$
    FPR = \frac{FP}{FP + TN} \tag{22}
    $$
    
#### Function implementation for different performance metrics
    
```{r}
mse <- function(actual, predicted) {
  if (length(actual) != length(predicted)) {
    stop("The lengths of actual and predicted vectors must be the same.")
  }
  
  mse <- mean((actual - predicted)^2)
  
  return(mse)
}


rmse <- function(actual, predicted) {
  if (length(actual) != length(predicted)) {
    stop("The lengths of actual and predicted vectors must be the same.")
  }
  
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  
  return(rmse)
}

# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```


#### NOTES:

For each of the inputation methods, we can test the performances of different approaches.
Perhaps doing a function to test them all, and we can test this with different simulated datasets.
Then we can test the best performing one on a real dataset and discuss.

Perhaps we can also showcase how missing data can lead to inaccurate conclusions when handled poorly.

**BTW I would ignore timeseries data**

**Test also datasets with outliers perhaps to see if this is problematic**

Should we do some testing also on the fact that the type of missing data mechanism might not be known in a real-world scenario ? Sensitivity analyses ? I think this is a bit beyond the scope of what we are doing.

*Inputation for model enanchment is also outside the scope I belive*

#### Testing the different performance metric:

Explain a bit the difference between the metric for inputation. And perhaps make plot to show the

**Pareto front ?**

> I would suggest providing a summary for each of the inputation methods.

### Regression Model on Original Dataset (Baseline)

Fit a regression model using the complete dataset without any missing values for baseline comparison.

```{r}

# Calculate number of samples for training set
num_samples <- nrow(synthetic_data) * 0.8

# Randomly sample indices for training set
train_index <- sample(seq_len(nrow(synthetic_data)), size = num_samples)

# Split the dataset in training and testing
train_data <- synthetic_data[train_index, ]
test_data <- synthetic_data[-train_index, ]

# Fit baseline model
baseline_model <- lm(target ~ ., data = train_data)
baseline_predictions <- predict(baseline_model, test_data)
baseline_rmse <- sqrt(mean((baseline_predictions - test_data$target)^2))

summary(baseline_model)
baseline_rmse
```

## Imputation Methods

The following imputation methods are tested on both the MCAR and MAR datasets. A regression model is fit on the imputed dataset, and the results are summarized.

### 1. Listâ€‘wise (Case) Deletion

```{r}
# Imputation
listwise_result <- listwise_deletion(data_mcar)
listwise_mar_result <- listwise_deletion(data_mar)

# Split into training and testing
train_listwise <- listwise_result[train_index, ]
test_listwise <- listwise_result[-train_index, ]

train_listwise_mar <- listwise_mar_result[train_index, ]
test_listwise_mar <- listwise_mar_result[-train_index, ]

# Regression Models
listwise_model_mcar <- lm(target ~ ., data = train_listwise)
listwise_predictions <- predict(listwise_model_mcar, test_listwise)
listwise_rmse <- rmse(test_listwise$target, listwise_predictions)

listwise_model_mar <- lm(target ~ ., data = train_listwise_mar)
listwise_mar_predictions <- predict(listwise_model_mar, test_listwise_mar)
listwise_mar_rmse <- rmse(test_listwise_mar$target, listwise_mar_predictions)

listwise_rmse
```

#### 2. Pairwise Deletion

```{r}
# Imputation
pairwise_result <- pairwise_deletion(data_mcar)
pairwise_mar_result <- pairwise_deletion(data_mar)

# Split into training and testing
train_pairwise <- pairwise_result[train_index, ]
test_pairwise <- pairwise_result[-train_index, ]

train_pairwise_mar <- pairwise_mar_result[train_index, ]
test_pairwise_mar <- pairwise_mar_result[-train_index, ]

# Regression Models
pairwise_model_mcar <- lm(target ~ ., data = train_pairwise)
pairwise_predictions <- predict(pairwise_model_mcar, test_pairwise)
pairwise_rmse <- rmse(test_pairwise$target, pairwise_predictions)

pairwise_model_mar <- lm(target ~ ., data = train_pairwise_mar)
pairwise_mar_predictions <- predict(pairwise_model_mar, test_pairwise_mar)
pairwise_mar_rmse <- rmse(test_pairwise_mar$target, pairwise_mar_predictions)

pairwise_rmse
```

#### 3. Simple Imputation

##### Mean Imputation

```{r}
# Imputation
mean_result <- simple_imputation(data_mcar, method = "mean")
mean_mar_result <- simple_imputation(data_mar, method = "mean")

# Split into training and testing
train_mean <- mean_result[train_index, ]
test_mean <- mean_result[-train_index, ]

train_mean_mar <- mean_mar_result[train_index, ]
test_mean_mar <- mean_mar_result[-train_index, ]

# Regression Models
mean_model_mcar <- lm(target ~ ., data = train_mean)
mean_predictions <- predict(mean_model_mcar, test_mean)
mean_rmse <- rmse(test_mean$target, mean_predictions)

mean_model_mar <- lm(target ~ ., data = train_mean_mar)
mean_mar_predictions <- predict(mean_model_mar, test_mean_mar)
mean_mar_rmse <- rmse(test_mean_mar$target, mean_mar_predictions)

# Metrics
mean_diff <- compare_imputed_to_original(synthetic_data, mean_result)

mean_rmse
mean_diff
```

##### Median Imputation

```{r}
# Imputation
median_result <- simple_imputation(data_mcar, method = "median")
median_mar_result <- simple_imputation(data_mar, method = "median")

# Split into training and testing
train_median <- median_result[train_index, ]
test_median <- median_result[-train_index, ]

train_median_mar <- median_mar_result[train_index, ]
test_median_mar <- median_mar_result[-train_index, ]

# Regression Models
median_model_mcar <- lm(target ~ ., data = train_median)
median_predictions <- predict(median_model_mcar, test_median)
median_rmse <- rmse(test_median$target, median_predictions)

median_model_mar <- lm(target ~ ., data = train_median_mar)
median_mar_predictions <- predict(median_model_mar, test_median_mar)
median_mar_rmse <- rmse(test_median_mar$target, median_mar_predictions)

# Metrics
median_diff <- compare_imputed_to_original(synthetic_data, median_result)

median_rmse
median_diff
```

#### 4. Regression Imputation

```{r}
# Imputation
regression_result <- regression_imputation(data_mcar)
regression_mar_result <- regression_imputation(data_mar)

# Split into training and testing
train_regression <- regression_result[train_index, ]
test_regression <- regression_result[-train_index, ]

train_regression_mar <- regression_mar_result[train_index, ]
test_regression_mar <- regression_mar_result[-train_index, ]

# Regression Models
regression_model_mcar <- lm(target ~ ., data = train_regression)
regression_predictions <- predict(regression_model_mcar, test_regression)
regression_rmse <- rmse(test_regression$target, regression_predictions)

regression_model_mar <- lm(target ~ ., data = train_regression_mar)
regression_mar_predictions <- predict(regression_model_mar, test_regression_mar)
regression_mar_rmse <- rmse(test_regression_mar$target, regression_mar_predictions)

# Metrics
regression_diff <- compare_imputed_to_original(synthetic_data, regression_result)

regression_rmse
regression_diff
```

#### 5. Hot-deck Imputation

```{r}
# Imputation
hotdeck_result <- hot_deck_imputation(data_mcar)
hotdeck_mar_result <- hot_deck_imputation(data_mar)

# Split into training and testing
train_hotdeck <- hotdeck_result[train_index, ]
test_hotdeck <- hotdeck_result[-train_index, ]

train_hotdeck_mar <- hotdeck_mar_result[train_index, ]
test_hotdeck_mar <- hotdeck_mar_result[-train_index, ]

# Regression Models
hotdeck_model_mcar <- lm(target ~ ., data = train_hotdeck)
hotdeck_predictions <- predict(hotdeck_model_mcar, test_hotdeck)
hotdeck_rmse <- rmse(test_hotdeck$target, hotdeck_predictions)

hotdeck_model_mar <- lm(target ~ ., data = train_hotdeck_mar)
hotdeck_mar_predictions <- predict(hotdeck_model_mar, test_hotdeck_mar)
hotdeck_mar_rmse <- rmse(test_hotdeck_mar$target, hotdeck_mar_predictions)

# Metrics
hotdeck_diff <- compare_imputed_to_original(synthetic_data, hotdeck_result)

hotdeck_rmse
hotdeck_diff
```

#### 6. Expectation-Maximization (EM)

```{r}
# Imputation
em_result <- em_imputation(data_mcar)
em_mar_result <- em_imputation(data_mar)

# Split into training and testing
train_em <- em_result[train_index, ]
test_em <- em_result[-train_index, ]

train_em_mar <- em_mar_result[train_index, ]
test_em_mar <- em_mar_result[-train_index, ]

# Regression Models
em_model_mcar <- lm(target ~ ., data = train_em)
em_predictions <- predict(em_model_mcar, test_em)
em_rmse <- rmse(test_em$target, em_predictions)

em_model_mar <- lm(target ~ ., data = train_em_mar)
em_mar_predictions <- predict(em_model_mar, test_em_mar)
em_mar_rmse <- rmse(test_em_mar$target, em_mar_predictions)

# Metrics
em_diff <- compare_imputed_to_original(synthetic_data, em_result)

em_rmse
em_diff
```

### 7. Multiple Imputation

...

# Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

## Decision tree / Random Forests

## Ensemble methods

# Results and Discussion

### Comparison of Results

Create a summary table and plots to compare RMSE, mean absolute differences across all methods 

```{r, echo = T}
# Create results dataframe with prediction RMSE
results <- data.frame(
  Method = c("Baseline", "Listwise", "Pairwise", "Mean", "Median", "Regression", "Hot-deck", "EM"),
  RMSE_Prediction = c(baseline_rmse, listwise_rmse, pairwise_rmse, mean_rmse, 
                     median_rmse, regression_rmse, hotdeck_rmse, em_rmse)
)

# Create comparison results dataframe with safe extraction
comparison_methods <- c("Mean", "Median", "Regression", "Hot-deck", "EM")
comparison_diffs <- list(mean_diff, median_diff, regression_diff, hotdeck_diff, em_diff)

# Custom function to safely extract values from the list of metrics
safe_extract <- function(metric_list, metric_name) {
  if (!is.null(metric_list[[metric_name]])) {
    return(metric_list[[metric_name]][1])  # Extract the first value (assuming it's a vector of length 1)
  } else {
    return(NA)  # Return NA if the metric is not found
  }
}

comparison_results <- data.frame(
  Method = comparison_methods,
  MAE = sapply(comparison_diffs, safe_extract, "mae"),
  RMSE = sapply(comparison_diffs, safe_extract, "rmse"),
  Correlation = sapply(comparison_diffs, safe_extract, "correlation")
)

# Remove any rows where all metrics are NA
comparison_results <- comparison_results[rowSums(is.na(comparison_results[,-1])) < ncol(comparison_results)-1, ]

# Plot 1: RMSE of Prediction Models
p1 <- ggplot(results, aes(x = reorder(Method, RMSE_Prediction), y = RMSE_Prediction)) +
  geom_bar(stat = "identity", fill = "#88C0D0") +
  geom_text(aes(label = round(RMSE_Prediction, 3)), 
            vjust = -0.5, size = 3) +
  labs(title = "Prediction Model Performance",
       x = "Method",
       y = "RMSE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5))

# Display the first plot
print(p1)
```

### Plot the difference metrics between datasets

```{r}
# Plot 2: MAE
p2 <- ggplot(comparison_results, aes(x = reorder(Method, MAE), y = MAE)) +
  geom_bar(stat = "identity", fill = "#A3BE8C") +
  geom_text(aes(label = round(MAE, 3)), 
            vjust = -0.5, size = 3) +
  labs(title = "Mean Absolute Error",
       x = "Method",
       y = "MAE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5))

# Display the second plot
print(p2)

# Plot 3: RMSE
p3 <- ggplot(comparison_results, aes(x = reorder(Method, RMSE), y = RMSE)) +
  geom_bar(stat = "identity", fill = "#EBCB8B") +
  geom_text(aes(label = round(RMSE, 3)), 
            vjust = -0.5, size = 3) +
  labs(title = "Root Mean Square Error",
       x = "Method",
       y = "RMSE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5))

# Display the third plot
print(p3)

# Plot 4: Correlation
p4 <- ggplot(comparison_results, aes(x = reorder(Method, Correlation), y = Correlation)) +
  geom_bar(stat = "identity", fill = "#B48EAD") +
  geom_text(aes(label = round(Correlation, 3)), 
            vjust = -0.5, size = 3) +
  labs(title = "Correlation with Original Data",
       x = "Method",
       y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5))

# Display the fourth plot
print(p4)
```

## Imputation Performance Comparison

## Computational Performance

# Conclusions and Recommendations

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data (not really interesting to deal with)
> Can pretty much always be reconducted to MAR

#### References

```{r}
citation("pROC")
```

- ...

