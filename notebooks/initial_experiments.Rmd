---
title: "Missing Data Analysis: Methods and Experiments"
author: "Jacopo Zacchigna"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    # toc_float: false
    code_fold: hide # Hidden by default unless specified otherwise
    df_print: paged
    highlight: tango
    theme: cosmo
    number_sections: true
    toc_collapsible: true  # Enable collapsible TOC
  word_document:
    toc: true
    toc_depth: '3'
  pdf_document:
    toc: true
    toc_depth: 3
editor_options:
  markdown:
    wrap: 72
---

## Environment Setup 

```{r, echo=F}
## Install requirements
# install.packages("ggplot2")
# install.packages("GGally")
# install.packages("reshape2")
# install.packages("corrplot")
# install ...
# install.packages("pROC")

# Inputation methods
# install.packages("mice")
# Other alternative
# install.packages("VIM") # (Visualization and Imputation of Missing Values) 
```

```{r, echo=T, class.source = 'fold-show'}
## Load requirements 
library(ggplot2)
suppressMessages(library(GGally))
suppressMessages(library(pROC))
suppressMessages(library(corrplot))
library(reshape2)
library(mice)
library(VIM)
library(RColorBrewer)

# Load utilities
source("../src/synthetic_data.R") 
source("../src/missing_data.R") 
source("../src/utils.R") 

# Set seed 
set.seed(42)
```

### Missing data functions

Showcase Introduce Mar function (example to show how to use print_function)

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(introduce_mcar)
```

```{r eval=TRUE, echo=FALSE, results='asis'}
print_function_code(plot_missing_data)
```

# Data Generation and Exploration

## Synthetic Dataset Creation

```{r, echo=T, class.source = 'fold-show'}
# This will create:
# - 5 continuous variables
# - 3 categorical variables with 3, 4, and 5 levels respectively
data <- generate_random_data(n_samples = 1000, 
                           n_continuous = 5, 
                           n_categorical = 3,
                           n_categories = c(3, 4, 5))

# Visualize dataset properties
summary(data)
```

## Data Visualization
> This section should be expanded to explore the dataset more in depth

```{r}
# Plot histogram for each continuous variable
data_continuous <- data[, sapply(data, is.numeric)]
continuous_melted <- reshape2::melt(data_continuous)

ggplot(continuous_melted, aes(x = value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Distribution of Continuous Variables", x = "Value", y = "Frequency")
```

```{rm, class.source = 'fold-show'}
# Plot pairwise relationships between continuous variables
ggpairs(data_continuous)
```

# Missing Data Analysis

## Introducing Missing Data
> Using our function to artificially create missing data

### Visualization of Missing Patterns

##### Missing data in the original dataset
> Obviously, no missing data is present in our dataset; it is synthetically generated.

```{r}
# Plot missing data patterns in the original dataset
original_missing <- as.data.frame(sapply(data, is.na))
original_missing$index <- 1:nrow(original_missing)
original_missing_melted <- melt(original_missing, id.vars = "index")

ggplot(original_missing_melted, aes(x = index, y = variable, fill = value)) +
  geom_tile() +
  scale_fill_manual(values = c("white", "red")) +
  labs(title = "Missing Data Pattern - Original", x = "Index", y = "Variable")
```

#### Testing different Missing Data Mechanisms

```{r, echo =T, class.source = 'fold-show'}
# Artificially create missing data for all mechanisms (MCAR, MAR, MNAR)
data_mcar <- introduce_mcar(data, prop_missing = 0.1)
data_mar <- introduce_mar(data, prop_missing = 0.1)
data_mnar <- introduce_mnar(data, prop_missing = 0.1)

# Analyze missing patterns and summarize
print(summarize_missing(data_mcar))
print(summary(data_mcar))
print(summarize_missing(data_mar))
print(summary(data_mar))
print(summarize_missing(data_mnar))
print(summary(data_mnar))
```

##### Plot missing data for the different mechanisms

```{r, class.source = 'fold-show'}
## Plot for MCAR
plot_missing_data(data_mcar, "MCAR")

# Plot for MAR
plot_missing_data(data_mar, "MAR")

# Plot for MNAR
plot_missing_data(data_mnar, "MNAR")
```

```{r, class.source = 'fold-show'}
md.pattern(data)
# Rotate variable names for better readability
md.pattern( data_mar,  rotate.names = TRUE)
md.pattern(data_mar)
md.pattern(data_mnar)
```

```{r, class.source = 'fold-show'}
# Create a custom color palette (we can choose the color pallet for the entire project if we want)
my_palette <- brewer.pal(3, "Set2")

# For the original data
aggr(data, plot = TRUE, numbers = TRUE, prop = FALSE, col = my_palette)

# For data_mar
aggr(data_mar, plot = TRUE, numbers = TRUE, prop = FALSE, col = my_palette)

# For data_mnar
aggr(data_mnar, plot = TRUE, numbers = TRUE, prop = FALSE, col = my_palette)
```

# Imputation Methods

## Performance metrics

- **Mean Absolute Error (MAE)**

    MAE measures the average difference between imputed values and true values defined as:
    
    $$
    MAE = \frac{1}{m}\sum_{i=1}^m|y_i - \hat{y}_i|
    $$

- **Mean Squared Error (MSE)**
    
    While MSE is equal to the sum of variance and squared predicted missing value as in the following equation:
    
    $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2
    $$

- **Root Mean Square Error (RMSE)**

    RMSE computes the difference in imputed values and actual values as follows:
  
    $$
    RMSE = \sqrt{MSE}
    $$
    
- **Area under the curve (AUC)**

    AUC is the representation of the degree or measure of separability and is used as a summary of the Root Receiver Operator Characteristic (ROC) curve, which is curve is a visualisation graph representing imputation performance [143]. The AUC is represented by the true positive rate (TPR) and the false positive rate (FPR). Where the TPR is the proportion of correctly imputed positives of all positives and the TPR is the proportion of all negatives that are wrongly imputed as positives [144]. The true positive rate and the false positive rate are defined as:
    
    $$
    TPR = \frac{TP}{TP + FN} \tag{21}
    $$
    
    $$
    FPR = \frac{FP}{FP + TN} \tag{22}
    $$
    
#### Function implementation for different performance metrics
    
```{r}
mse <- function(actual, predicted) {
  if (length(actual) != length(predicted)) {
    stop("The lengths of actual and predicted vectors must be the same.")
  }
  
  mse <- mean((actual - predicted)^2)
  
  return(mse)
}


rmse <- function(actual, predicted) {
  if (length(actual) != length(predicted)) {
    stop("The lengths of actual and predicted vectors must be the same.")
  }
  
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  
  return(rmse)
}

# library(pROC)
# roc_obj <- roc(actual, predicted)
# auc_value <- auc(roc_obj)
```


#### NOTES:

For each of the inputation methods, we can test the performances of different approaches.
Perhaps doing a function to test them all, and we can test this with different simulated datasets.
Then we can test the best performing one on a real dataset and discuss.

Perhaps we can also showcase how missing data can lead to inaccurate conclusions when handled poorly.

**BTW I would ignore timeseries data**

**Test also datasets with outliers perhaps to see if this is problematic**

Should we do some testing also on the fact that the type of missing data mechanism might not be known in a real-world scenario ? Sensitivity analyses ? I think this is a bit beyond the scope of what we are doing.

*Inputation for model enanchment is also outside the scope I belive*

#### Testing the different performance metric:

Explain a bit the difference between the metric for inputation. And perhaps make plot to show the

**Pareto front ?**

> I would suggest providing a summary for each of the inputation methods.

## List‑wise or case deletion

### Regression model ...
### Spline ...

## Pairwise deletion

...

## Simple imputation

Test the different ones

...

## Regression imputation

Usually needs a decent amount of data (thus it is interesting to try if that is not the case what happens)

...

## Hot‑deck imputation

...

## Expectation–maximization

....

## Multiple imputation

....

# Imputation methods inspired by ML

> Evaluate which of those one makes sense to actually test

I think this can be explore more later

## K nearest neighbour classification (KNN)

*Computationally expensive. So perhaps we might want to keep track of the computational cost for different methods*

Test with different distances maybe. Iterative KNN method ... + other approaches
High dimensional data problem

## Support vector machine (SVM)

Suited for high dimensional data

## Decision tree / Random Forests

## Clustering imputation

## Ensemble methods

# Results and Discussion

## Imputation Performance Comparison

## Computational Performance

# Conclusions and Recommendations

(Add your conclusions and recommendations based on the analysis results)

## Appendix

#### Notes on Missing Data Mechanisms

- MCAR: Missingness is unrelated to data values.
- MAR: Missingness depends on observed values.
- MNAR: Missingness depends on unobserved data.

#### References

```{r}
citation("pROC")
```

- ...

